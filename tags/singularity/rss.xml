













    
        
    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="https://hockenworks.com/tags/singularity/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="https://hockenworks.com/tags/singularity/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-02-01 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>https://hockenworks.com/tags/singularity/</link>

        

        <title>Singularity · Tags · Home</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have long been of the mind LLMs and their evolutions are truly thinking, and that they are on their way to solving all of the intellectual tasks that humans can solve today. It is just too uncanny that the technology that seems to have made the final jump to actually thinking, after a long string of attempts and architectures, is a type of neural net. It would be much easier to argue away transformer models as non-thinking stochastic parrots if we had happened to have had success with any other architecture than the one that was designed to mimic our own brains and the neurons firing off to one another within them. It&rsquo;s just too weird. They are shaped like us, they sound like us in a lot of ways, and it&rsquo;s obvious they are thinking something like us too.</p>
<p>That&rsquo;s not to say they are AGI in the modern definition. They can&rsquo;t do every task humans can do intellectually (IE without a body, which I will get to) for several reasons:</p>
<p>The Limitations</p>
<ol>
<li>
<p>Looping reasoning.
This was a huge problem for early transformers that had to output in one shot, and the examples were obvious. This one has been essentially solved via thinking models like o1. That was a huge unlock and a huge bone for things like programming where there is lots of nested recursion of logic that has to occur to get a reasonable answer.</p>
</li>
<li>
<p>Memory and context.
Context windows get larger all the time but this one still is not solved. Just adding a bunch of tokens into a context window doesn&rsquo;t get you much when 2 million token models lose coherence after about the first 40,000 - which they do, and which every programmer working with anything but a tiny codebase intuitively understands. But this one too will largely be solved soon, if not through architectures that actually update their weights, it&rsquo;ll be solved through nuanced memory systems that people are actively developing on top of thinking models.</p>
</li>
<li>
<p>Vision</p>
</li>
</ol>
<p>And this one might sound funny to someone that is paying attention to AI in particular, because GPT-4 with vision launched something like 2 years ago now. And it has been impressive for a long time, able to do things like identify what objects are in an image, where an image is from, even things that payments can&rsquo;t do glancing at an image.</p>
<p>But the vision itself is not “good” vision. It cannot really pick out small important details, and it still behaves in many ways like vision recognition models have for years now. Now that we have a model that has both thinking and image input and editing at every step, the 03 and 04 mini series just released, we can really start to see the limitations in vision. Let me take you through 2 examples that represent the 2 types of failure modes that result from these not having true image understanding, yet.</p>
<p>Each release from the major providers steadily knocks away my intelligence tests, which I admit are mostly programming oriented, but the ones that they can never really dent are the spatial reasoning ones where a model really has to think about images in its head or have to use an image provided for detailed work.</p>
<p>Example 1:</p>
<p>Every major model release, I test what models can do with OpenSCAD. I won’t get technical about it here, but OpenSCAD is a CAD program (Computer Aided Design - think 3D modeling for engineers, not the artistic kind) that is defined entirely through a programming language vs the typical mousestrokes and key presses that software like Solidworks or AutoCAD depend on.</p>
<p>This makes OpenSCAD the perfect test platform for a model that inputs and output text primarily. I can describe a 3D model I want, and the model can output text that renders into a 3D model.</p>
<p>But for as amazing as LLMs are at scripting in normal programming languages, they have never been good at OpenScad (link to GPT4 chair article)</p>
<p>Here is O3’s attempt to make a</p>
<p>Map example:</p>
<p>I recently gave this question to the latest thinking image model, &lsquo;03: &ldquo;Here&rsquo;s an image from Google maps of the block I live on between the avenues of Burbank, hazeltine, Oxnard, and Van nuys. What is the longest continuous loop I can walk within the neighborhood without crossing my path or touching one of the avenues? This square is 1/2 mi on each side&rdquo;</p>
<p>O3 thinks for 4 minutes about this question, zooming in to various parts of the map countless times to form the route. And then it fails on the first step, suggesting starting at tiara and stansbury, which do not intersect on the map. Any person looking at this image could tell that is true in just a few seconds.</p>
<p>What I think is going on in these examples is that we have a limitation in training data (duh) but it isn&rsquo;t because there aren&rsquo;t a lot of images and videos on the internet, it&rsquo;s because there is so much more information in the average image then there is in the average chunk of text, and a lot more of that information is irrelevant to any given question.</p>
<p>When I say that we have a limitation on training data, I&rsquo;m not in the typical camp of &ldquo;well, then transformer neural nets are obviously stupid because I was able to understand this thing without training on terabytes of data from the internet&rdquo;. This is always been a bad take because the average human trains on petabytes, not terabytes of data, and that data is streamed into their brains mostly in the form of images. I am also not in the camp of thinking that this means that the data &ldquo;just doesn&rsquo;t exist&rdquo; to get these models to AGI in this dimension. It&rsquo;s so clearly does exist, and it exists so abundantly that a unique mage stream can be sent to each of the billions of human brains, and they all learn the same principles that let them immediately identify the mistake that the cutting edge thinking vision model made after 4 minutes of rigor.</p>
<p>The data exists, and we never actually had a data problem in AI. We have an instruction problem. That doesn&rsquo;t mean model architecture or data massaging really, it means that we need to plug our models into the real world where all the data streams exist. I&rsquo;m guessing that this comes in the form of robots with cameras on them, the first of which is happening on mass via Tesla full self-driving, and I&rsquo;m sure those vision neural nets are quite insane compared to what we see in the consumer transform r models, but the real leap probably comes when we get to humanoids walking around collecting and learning from vision data everyday.</p>
<p>The Robots</p>
<p>If you’ve ever tried to get concrete actions to take based on a vision-transformer’s outputs, you will know it’s hard.</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2025-02-01:/vision-is-the-last-hurdle-before-agi/</guid>

                
                    <link>https://hockenworks.com/vision-is-the-last-hurdle-before-agi/</link>
                

                
                    <pubDate>Sat, 01 Feb 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Vision is the last hurdle before AGI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I came across this tweet today that was surprisingly contentious. Maybe not so surprising given the state of Twitter, but still:</p>
<p><a href="https://twitter.com/AdamRackis/status/1762321041899012307">https://twitter.com/AdamRackis/status/1762321041899012307</a></p>
<p>On the face of it, “people get twisted in their relationship with work” seems like a reasonable take. Stop complaining—you are making 16x the median salary in this country. Just do the boring job with the toxic team.</p>
<p>Let’s stop and think about this for a second. The original poster (OP from now on) is making $800K annually due to the appreciation of Spotify stock. An obscene amount of money? Maybe, maybe not. I do not have the typical hang-ups about the wealthy or ultra-wealthy. I don’t see this world as a zero-sum game, and I think the richest people out there have usually done some amazing things, especially in countries like the US where most wealth is not old wealth.</p>
<p>But there is a difference between founding a company you’re passionate about, which goes on to become incredibly valuable, and working for someone else doing a job you think is boring in order to join the 1%. In this article, I’m talking about the latter.</p>
<p>I propose this core question for this type of person: Is this large income worth doing a job you find boring, or working with people you feel are toxic? I would say no. Absolutely not. OP is clearly regretting how he’s spending his time at work. He should look for something to do that he values, even if it pays a quarter as much as he gets now. Or less!</p>
<h2 id="the-hedonic-treadmill">The Hedonic Treadmill</h2>
<p>There’s an often misquoted study from ~2010 that personal income beyond about $75K (presumed to cover basic necessities with a comfortable overhead) does not equate to further happiness. This number is actually about right when it comes to simple reported happiness, which has more to do with the hedonic treadmill than anything else. But the study in question was also measuring reported “life satisfaction” as surveyed, which did not stop increasing with income. Of course, “life satisfaction” is a fraught survey metric as well, as it might actually measure a perceived comparative number with one’s neighbors (we’ll get to this later).</p>
<p>But whatever the proper measure of happiness or satisfaction is, there is a great deal of logic to thinking its relationship with income looks like this:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="happiness income curve"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/treadmill-graph.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>As in: there are diminishing returns to incremental income. Not a revolutionary idea. It’s obvious. But it also means that somewhere along this curve is the often-quoted $75K, and somewhere far to the right is OP’s $800K. And the curve between those points is probably pretty flat.</p>
<p>What could OP get in return for moving to the left along this flat section of the curve? Eight hours of time per workday, at least, that he finds incrementally more valuable. How much do you think those 8 hours could improve his life satisfaction on their own happiness/hours curve?</p>
<p>This is where the real paradox comes in. Gaining wealth is supposed to make your time more valuable. You choose to get more services, have a housekeeper, maybe even a private chef. But if those things actually indicate a person values their time more highly than someone poorer, how come they would even consider sacrificing most of that time doing something they find boring, or with people they find annoying? Just to maintain their position along the flat section of the curve?</p>
<p>Maybe they aren’t prioritizing happiness at all.</p>
<h2 id="keeping-up-with-the-joneses">Keeping up with the Joneses</h2>
<p>My argument so far would be absolute blasphemy to most of the $500K+ salary people who don’t have much fun at work. Let’s talk about their two most common counterarguments:</p>







<div class="paige-quote">
<blockquote class="blockquote"><ol>
<li><em>&ldquo;Work should be a sacrifice, and more money means more security for my family.&rdquo;</em></li>
</ol>
</blockquote>


</div>

<p>This is noble. It’s also way too self-sacrificial for the extremely wealthy first-world people we’re talking about. You’re making $800K. There is not a salary sacrifice in the world that will make your family “insecure.”</p>
<p>And what pattern does this even establish? You’re going to choose to be unhappy for most of your waking hours so that you can guarantee your children will be able to do the same? What are you actually working for if not you or anyone else being able to actually enjoy themselves?</p>







<div class="paige-quote">
<blockquote class="blockquote"><ol start="2">
<li><em>&ldquo;But I could make this crazy money and then retire in 5 years.&rdquo;</em></li>
</ol>
</blockquote>


</div>

<p>Now this is a good argument! You could indeed retire in 5 years after making $800K per year and then spend your time with family or pursue passion projects! The problems here are twofold:</p>
<p><strong>First</strong>, nobody does this. Instead, they spend more money. Of course, some of it will be saved, but the main thing that happens when people find themselves with far more money than needed for their family’s security is that they spend it. Bigger houses, more trips, elite schools for the kids. These make you feel well-off compared to your neighbors but don’t push you much higher on that flat curve.</p>
<p><strong>Second</strong>, people find value in being useful. There’s a reason why so many struggle in retirement. Even with a rigid FIRE plan, you’ll still want to work afterward on something interesting. So why not find that now?</p>
<p>What’s actually happening? Lifestyle creep. I think this is 90% of why people feel they could never work on something more fun or meaningful for less money. They’ve already started to spend their new money, and now losing those things would hurt more than gaining them felt good. That damn hedonic treadmill.</p>
<h2 id="what-am-i-saying">What am I saying?</h2>
<p>I’m asking you to think deeply about what you value. Money isn’t a core value. Value your own time as much as your spending habits suggest you do. This is the right logic for your well-being, even if the world stays as it is. Now for my last thought:</p>
<p>I’m obsessed with AI and the technological singularity. It’s a healthy time to step back, reflect on the fundamental values driving our behavior, and make changes. Wouldn’t it be silly to spend most of our waking hours working miserably on meaningless things, only to arrive in a second half of life in a world of abundance?</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2024-02-01:/the-treadmill/</guid>

                
                    <link>https://hockenworks.com/the-treadmill/</link>
                

                
                    <pubDate>Thu, 01 Feb 2024 00:00:00 UTC</pubDate>
                

                
                    <title>The Treadmill</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="this is a robot"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/ai-software-dev.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>Lots of chatter right now about AI replacing software developers.</p>
<p>I agree - AI will take over software development. The question is: what work will be left when this happens?</p>
<p>Some considerations:</p>
<ul>
<li>Benchmarks for the best LLMs still put them solidly in the &ldquo;bad at programming&rdquo; category, scoring in the 5th percentile of human programmers on common tests. Meanwhile, LLMs score in the 80th-95th percentile for law exams and 85th–100th for psychology, statistics, and many other less technical fields. More scores available in the &ldquo;simulated exams&rdquo; section of <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>.</li>
<li>Engineers have been using language models like tabnine and copilot as &ldquo;super-stackoverflow&rdquo; style code assistance years before chatGPT released. This means much of the velocity increase we might expect from current LLMs&rsquo; ability to write code has already been &ldquo;priced in&rdquo; to the market.</li>
<li>Many of the trends making software development more costly are growing, not shrinking: Systems are becoming more distributed. The cloud lowered infrastructure costs but made applications more complex. We&rsquo;re making more and deeper integrations among disparate systems. Auth is becoming more secure and thus complex (managed identity, MFA, etc).</li>
</ul>
<p>Github copilot chat and other LLM dev tools are speeding up the rote stuff. I’ve seen it in my own work.</p>
<p>And I really do believe new AI models will do more than just the basics, maybe in the next couple of years. Even precluding &ldquo;AGI&rdquo;, the trend we are on is that more and more work is automatable, and engineers, especially more junior ones - are going to have to shift focus away from algorithmic work that AI can do.</p>
<p>But by the time our neural nets are &ldquo;good enough&rdquo; at building software to make it significantly cheaper to build, I doubt this trend will make the news. Everything else gets automated too.</p>
<p>These are my thoughts at what seems to be the beginning of the next AI revolution in early 2024. I plan to revisit this topic and see if I&rsquo;m right in future posts.</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2024-01-24:/on-ai-software-development/</guid>

                
                    <link>https://hockenworks.com/on-ai-software-development/</link>
                

                
                    <pubDate>Wed, 24 Jan 2024 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development</title>
                
            </item>
        
    </channel>
</rss>
