













    
        
    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/categories/writing/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/categories/writing/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-07-28 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>http://localhost:1313/categories/writing/</link>

        

        <title>Writing · Categories · hockenworks</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Recently I read the <a href="https://ai-2027.com/scenario.pdf">AI 2027 paper</a>. I was surprised to see Scott Alexander&rsquo;s name on this paper and I was doubly surprised to see him do his <a href="https://www.dwarkesh.com/p/scott-daniel">first face reveal podcast about it with Dwarkesh</a></p>
<p>On its face this is one of the most aggressive predictions for when we will have AGI (at least the new definition of AGI which is something that is comparable or better than humans at all non-bodily tasks) that I have read. Even as someone who has been a long believer in <a href="https://en.wikipedia.org/wiki/The_Singularity_Is_Near">Ray Kurzweil&rsquo;s Singularity predictions</a>, 2027 strikes me as very early. I realize that Kurzweil&rsquo;s AGI date was also late 2020&rsquo;s, which puts his prediction inline with AI 2027, while 2045 was his singularity prediction. But 2027 still feels early to me.</p>
<div class="paige-row-wide">
  <div style="display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:15px;">
      































    













    



    























<img  alt="AI 2027"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-2027.png"    >


      































    













    



    























<img  alt="Singularity Is Near"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/The-Singularity-Is-Near.jpg"    >

</div>
</div>

<p>I won&rsquo;t get into my full take on AI 2027 here, but the core argument comes down to the same one I was making in my <a href="/on-ai-software-development">original post on AI software development</a> - which is that, once AI agents are able to replace software engineers, instead of just assisting them, it doesn&rsquo;t matter their current performance in non-software domains, because they will simply be able to improve on their own software and training procedures at such a rate that the time difference between the moment of automating software engineering tasks and the moment of automating all other tasks is negligible.</p>
<p>In light of AI 2027, I figured it was a good time to update where I think AI is actually in terms of software engineering. I&rsquo;ve had the chance to test many of the latest AI software development tools and models, and they have come a long way since my original post. This post is primarily a story of my own personal experience with the latest AI coding and &ldquo;vibe coding&rdquo; tools on a game and web developement project.</p>
<h1 id="what-i-built">What I Built</h1>
<p>I&rsquo;ve been developing with AI since 2021, but I got a special chance to really put the latest models and tools to the last few months while on parental leave. And I think I have a more valuable type of project with which to accurately evaluate these models on their creative capacity for programming, which is really the hard stuff.</p>
<p>This project is absolutely not in the training data, not even a little bit. That&rsquo;s because it&rsquo;s a game, which AI is already much worse than other types of developments (mobile and web dev in particular are particular), but also because it&rsquo;s a game combined with a CMS website. <a href="/this-website/#ball-machine---the-game">This website!</a>.</p>
<p>It&rsquo;s a very strange project. I won&rsquo;t get too much into the weeds here, but essentially we have a <a href="https://gohugo.io/">hugo CMS</a> underneath everything, and that Hugo CMS has a bunch of short code that interacts with a bunch of other custom JavaScript that loads dynamically when the player decides to start playing any given page. Just try it on the top right corner of this page and you&rsquo; might start to imagine how weird of an application this would be to work on.</p>
<h1 id="three-ways-for-ai-to-build-your-project">Three Ways for AI to Build Your Project</h1>
<p>Over the course of 5 months, I worked on this game via 3 main methods, which happen to be the main 3 ways that anyone is writing code with AI these days.</p>
<ol>
<li>Chat Assistants - particularly the O4-mini and O3 models</li>
<li>Automated IDEs - particularly Cursor</li>
<li>Autonomous Coding Agents - particularly OpenAI&rsquo;s &ldquo;Codex&rdquo;, which launched about 2 months before I finished the project.</li>
</ol>
<p>I&rsquo;m going to cover what they are and how they performed on my game.</p>
<h2 id="chat-assistants">Chat Assistants</h2>
<blockquote>
<p>What Are They?</p>
<p>Chat Assistants are models running in a client such as <a href="https://chatgpt.com/">ChatGPT</a>, <a href="https://claude.ai/login?returnTo=%2F%3F">Claude</a>, or <a href="https://gemini.google.com/app">Gemini</a>. This often happens on a website, but can happen in other software too. The key is that the human is doing all of the input into the AI model and output from the AI response to the codebase. Getting these assistants to write code for you entails passing in instructions and sometimes code context from your project.</p>
</blockquote>
<p>Chat assistants are great and have been helping developers for a long time! These are the closest most people get to running LLMs &ldquo;raw&rdquo; rather than through specialized tools designed to limit their errors or make them more agentic. So, you can think of the performance of Chat Assistants for coding as sort of the &ldquo;default performance&rdquo; of AI models to today at coding tasks. The other methods of having AI writing code still have the general performance dynamics I&rsquo;m going to talk about here at their cores, even if they mitigate some types of failures by running their own tests and dealing with console errors.</p>
<p>I&rsquo;ll start with what might be an obvious statement: LLMs are great at producing simple, low context code. Like - so great that most humans won&rsquo;t need to do write much algorithmic logic from here on out. I certainly didn&rsquo;t when making this game, or really anything else I&rsquo;ve made in the last two years.</p>
<p>But basic coding is not all of software engineering, and LLMS still have major issues:</p>
<p><strong>1. They can&rsquo;t deal well with contexts over 30K tokens or so (even the best models with supposed millions of token context windows).</strong></p>
<p>This means the actual developer (me and you) are typically the ones picking specific files and functions to send into the context window, lest we confuse the model. This is arduous unless the codebase is small enough to fit entirely into the context window. That&rsquo;s exactly what I think is going on with most of the new &ldquo;vibe coded&rdquo; projects we see showing impressive results - these are just tiny POC apps that haven&rsquo;t hit more than a few thousand lines of code yet. For context, most serious enterprise apps containing the detailed logic and edge cases real use cases require are in the millions or hundreds of millions of lines of code.</p>
<p>This issue is compounded by the fact that the largest labs seem to recognize it, and have modified their chat clients in secret ways to the user to save themselves money. OpenAI in particular implements and invisible, silently failing ~40K token limit on text pasted into the chat window. I have run into many strange issues caused by this, and it’s an insane policy on OpenAI’s part because they have non-invisible text limits for some models like 4o that tell you when your message is too long. My theory on why is pretty insidious if true: I think they want paying customers to think their very large thinking models can accept the full token limit of the models, and don’t think many customers will find out they are truncating them for cost reasons anyway, because these models become stupid after 30K tokens or so anyway.</p>
<p><strong>2. They are biased to be &ldquo;advisors&rdquo; rather than &ldquo;doers&rdquo;</strong></p>
<p>This is just annoying, and I hope it gets trained out soon, though the second two categories of AI coding tools might obviate fixing this is chat assistants themselves. When they are not specifically trained out of this mindset, most AI models just really <em>want</em> you, the human, to be doing everything, and to act themselves as an advisor. This makes sense with one of the main sources of code training data being from Stackoverflow and other blogs, where developers can never seem to rid themselves of a pseudo-condescending &ldquo;you should have been able to read the docs and learn this yourself&rdquo; tone. It&rsquo;s also just a pattern exhibited by people in general - more often than not, especially in the corporate world, people are trained to be the &ldquo;coaches&rdquo; rather than the &ldquo;worker bees&rdquo;. One reason why things get done so slowly in big political companies sometimes.</p>
<p><strong>3. They are still wrong sometimes, even on the basics, but they&rsquo;re wrong MUCH more confidently than human developers.</strong></p>
<p>This is probably the biggest issue for all code produced by AI, regardless of interface. But I&rsquo;m including it in the &ldquo;Chat Assistants&rdquo; section because there are no guardrails on chat assistants other than the human user: they won&rsquo;t run tests and they won&rsquo;t encounter errors. Here&rsquo;s a recent example of a true algorithmic mistake that cost me days of confusion developing my game. This is with O3, potentially the best model for coding, via a chat client.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl"><span class="n">setInterval</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="o">/</span><span class="n">_</span> <span class="o">---</span> <span class="n">recompute</span> <span class="n">every</span> <span class="mi">30</span> <span class="n">engine</span> <span class="n">ticks</span> <span class="p">(</span><span class="o">~</span><span class="mf">0.5</span> <span class="n">s</span><span class="p">)</span> <span class="o">----------------------</span> <span class="n">_</span><span class="o">/</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">timing</span><span class="o">.</span><span class="n">timestamp</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">===</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="n">let</span> <span class="n">maxV</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">Matter</span><span class="o">.</span><span class="n">Composite</span><span class="o">.</span><span class="n">allBodies</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">world</span><span class="p">)</span><span class="o">.</span><span class="n">forEach</span><span class="p">((</span><span class="n">b</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">label</span> <span class="o">===</span> <span class="s2">&#34;BallFallBall&#34;</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">const</span> <span class="n">v</span> <span class="o">=</span> <span class="n">Math</span><span class="o">.</span><span class="n">hypot</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">velocity</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">velocity</span><span class="o">.</span><span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="n">maxV</span><span class="p">)</span> <span class="n">maxV</span> <span class="o">=</span> <span class="n">v</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="o">/*</span> <span class="n">map</span> <span class="n">speed</span> <span class="err">→</span> <span class="n">substeps</span> <span class="p">(</span><span class="mi">1</span><span class="err">‒</span><span class="mi">5</span><span class="p">)</span> <span class="o">*/</span>
</span></span><span class="line"><span class="cl">        <span class="n">let</span> <span class="n">target</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">maxV</span> <span class="o">&gt;</span> <span class="mi">40</span> <span class="err">?</span> <span class="mi">8</span> <span class="p">:</span> <span class="n">maxV</span> <span class="o">&gt;</span> <span class="mi">25</span> <span class="err">?</span> <span class="mi">8</span> <span class="p">:</span> <span class="n">maxV</span> <span class="o">&gt;</span> <span class="mi">12</span> <span class="err">?</span> <span class="mi">8</span> <span class="p">:</span> <span class="n">maxV</span> <span class="o">&gt;</span> <span class="mi">3</span> <span class="err">?</span> <span class="mi">8</span> <span class="p">:</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="o">/*</span> <span class="n">mobile</span> <span class="n">caps</span> <span class="n">at</span> <span class="mi">2</span> <span class="o">*/</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">isMobileLike</span> <span class="o">&amp;&amp;</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="n">target</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">substeps</span> <span class="o">=</span> <span class="n">target</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&#34;New Substeps: &#34;</span> <span class="o">+</span> <span class="n">substeps</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">const</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">baseDt</span> <span class="o">/</span> <span class="n">substeps</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="p">(</span><span class="n">let</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">substeps</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Matter</span><span class="o">.</span><span class="n">Engine</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">engine</span><span class="p">,</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">engine</span><span class="o">.</span><span class="n">timing</span><span class="o">.</span><span class="n">timeScale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span> <span class="n">baseDt</span><span class="p">);</span>
</span></span></code></pre></div><p>This code was confidently written by o3 after lots of correct, thoughtful discussion about how often we should run physics simulation on my game in times/s. 60 hz is a common simulation time, which is Matter.js’s default, but when you have small objects moving around quickly, you need to increase it so that they don’t tunnel through each other.</p>
<blockquote>
<p>Tunneling in physics simulations is when one of two objects is moving fast enough that there is no single frame where they are colliding, and so they simply pass through each other</p>
</blockquote>
<p>This is all good theory, and o3 came up with a really thorough chart for this little line:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">maxV &gt; 40 ? 8 : maxV &gt; 25 ? 8 : maxV &gt; 12 ? 8 : maxV &gt; 3 ? 8 : 1;
</span></span></code></pre></div><p>Which tried to determine dynamically how many physics steps to run based on how fast objects are moving on the screen. It&rsquo;s a great idea, it is simple algorithmic work, and I was happy not to have to write or think about it.</p>
<p>There was just one problem. The condition at the start of this snippet, <em>if (engine.timing.timestamp % 20 === 0)</em>, would never be true, because we are always dividing time up into little chunks in a way that they wouldn’t even be whole numbers. The entire code snippet above never runs.</p>
<p>Here&rsquo;s where it gets really hairy, and the confidence issue really compounds in a negative way. I asked o3 and many other AI chatbots and agents about this code, and all “assumed” it was correct and would fire when I told them it wasn’t working. They would dig into all the little minutia of this function, the thresholds, how we were getting the max velocity, etc, and none figured out that why it wasn’t working was something more basic like this.. that it was never firing at all because of the faulty condition that O3 wrote at the top.</p>
<p>To be fair, I, the human, should have caught this before asking yet more AI agents why it wasn’t working. But the AI has taught me to be a little bit lazy, and what does it say about the state of AI independently-written software if multiple passes by state-of-the-art agents didn’t see the issue with a snippet of less than 20 lines of code?</p>
<h2 id="agentic-ides">Agentic IDEs</h2>
<blockquote>
<p>Automated IDEs are &ldquo;development environments&rdquo;, the software that humans use to write code, with LLM integrations added in to auto-complete or produce new code somewhat agentically. They can do things like read terminal output and attempt to fix their mistakes before you see them. The main examples right now are <a href="https://cursor.com/en">Cursor</a> and <a href="https://windsurf.com/editor">Windsurf</a>. Amazon just launched <a href="https://kiro.dev/blog/introducing-kiro/">Kiro</a> in this category.</p>
</blockquote>
<p>After I became a parent and then started having a few hours a day to come up for air, one of the first things I wanted to do was REALLY crack into one of these vibe-coding tools to see what it was all about. I picked Cursor.</p>
<p>Simple download, login, install. All going well. The very first feature I decided to through at it (using Claude 3.7 which was at the time the top recommended model) was something relatively simple: I wanted to modify the behaviour of the &ldquo;Compactor&rdquo; item in the Ball Machine so that it would not only combine the value of balls, but also remember how many balls had been combined and combine their value growth per second. This would need to change about 3 files and read from maybe 5 more.</p>
<p>The first real disappointment here happened after I entered my detailed prompt for how I wanted this to work: Cursor not only wanted an instruction file telling the AI how my codebase worked, it also wanted me to <em>manually pick every file</em> to include in the context. I laughed out loud when I saw this. I had been under some impression that Cursor was more than essentially copying and pasting a prompt and some code context into ChatGPT.</p>
<p>Nevertheless, I picked all the files I thought were relevant - about 10 of them - and let the agent run. I did this at least 3 times, testing each one. Some of these runs were many minutes of Claude 3.7 iteratively coding.</p>
<p>Abject failure. Nonsense written into the files. Total disregard for the context I had provided.</p>
<p>After the final attempt with prompt tweaks and file context tweaks, I just gave up on Cursor doing this. I used Context Caddy and O3 and got code (Code I had to tweak and find the right place for) that worked. Apparently this was too complicated for Cursor and its top recommended AI model. I was able to achieve simple, mostly one file edits with Cursor using simple natural language prompts. It never seemed to respond well to my multi-pager prompts that I have grown accustomed to giving big thinking models like O3.</p>
<p>In the end I cannot argue that using a raw LLM via a chat assistant is better than using an automated IDE. There is theoretically nothing that Cursor could not do that a web chat-based LLM could, provided you are willing to pay for an expensive model like O3 behind it, prompt it carefully, and correct its mistakes. But I was not willing to do that - the benefits of Cursor didn&rsquo;t seem worth this cost to me. And, there is a lot that it can do, like reading and correcting basic terminal errors.</p>
<p>Perhaps I had been spoiled by my own creation, context caddy, which is a vs code extension I made that essentially let me use any typical chat assistant with the same kind of context file picker.</p>
<p>If you were used to simply plugging in prompts to ChatGPT, being able to pick files for context and then having the model able to respond to terminal errors could seem like a big upgrade.</p>
<p>After all this, here&rsquo;s my hottest take of the article: Automated IDEs like Cursor and Kiro are a fad that will pass us by soon.</p>
<p>They exist today because IDEs are where all code editing takes place, and putting LLMs there was obvious. The next obvious thing was to let them run code and read output from the terminal in a loop. The next most obvious thing is to give them &ldquo;specs&rdquo; instead of prompts, so they have more patterns to follow rather than duplicating code everywhere as LLMs always want to do. This is where we&rsquo;re at with these Agentic IDEs, and they are useful.</p>
<p>But the core issue with this obvious progression of features is that it ignores this fundamental reality: Once AI models are actually good enough to do a software engineering task, there is no reason for that task to happen in an IDE at all, a tool made for humans to write code.</p>
<h2 id="autonomous-coding-agents">Autonomous Coding Agents</h2>
<blockquote>
<p>Autonomous Coding Agents, unlike Agentic IDEs, approach the problem of agentic software development without the lens of the toolspace that humans rely on. Using these systems does not depend on the actual code ever living on your own computer. Instead, you give them access to the code in your remote repository such as Github, and they do all the work to clone that code, look through it, make changes, test what they can, and ultimately contribute directly back to that codebase in the form of a <a href="https://en.wikipedia.org/wiki/Distributed_version_control#Pull_requests">Pull Request</a>. The main two right now are OpenAI&rsquo;s <a href="https://openai.com/codex/">Codex</a> and <a href="https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/">Github Copilot Agents</a></p>
</blockquote>
<p>On the face of it, Autonomous Coding Agents aim to do the same thing which has come to be the top feature of Agentic IDEs: The let you specify a feature, change, or fix by simply speaking or typing English, and then they proceed through a loop of coding, evaluating, and refining until they &ldquo;think&rdquo; the feature is done. And both types of system can do that job. But in my experience with Codex, the Autonomous Coding Agent I used, the difference was night and day.</p>
<p>I think Codex benefits most from its limitations. It does not sit in an IDE, so it could not possibly ask you, the human, to pick the files to add to system context. It doesn&rsquo;t run on your computer with you watching it, so it needs to handle all of its own looping, eval, and stop logic on its own. It needed to &ldquo;get good&rdquo; at these things. And that it did.</p>
<p>I will just skip straight to the punchline: OpenAI&rsquo;s Codex was writing full new features for my game via my voice-texted prompts alone (most of which I spoke while walking the baby) within days of me setting it up. In fact, the <a href="/this-website/#achievements">entire achievements feature of my game</a>, including the UI components, the logic to detect them, and save them - everything but the images I used for the achivement icon - was created in two Codex PRs generated without me touching the codebase.</p>
<p>I had built up to this point a little bit. I started Codex on simple tasks reminiscent of the one I started Cursor on, but I found that I was willing to lend more tasks to Codex, because I knew it wouldn&rsquo;t interupt my work at all. I would just end up with a PR I could review 20 minutes later or so, and if it worked I would merge it. Compare that to the experience with Agentic IDEs, where most of the time you need to open your computer, type up a prompt, select all the context you think is necessary, and then &ldquo;help it along&rdquo; when it gets stuck. This is not to say Codex didn&rsquo;t get stuck - there were things it couldn&rsquo;t do, even the occasional simple thing, but for those I would pull the PR, see that it didn&rsquo;t work, and then just reject it. No longer was I in the world of trying to &ldquo;help along&rdquo; the AI ending up in strange diff states and git stashing to clean up my own workspace. It was just &ldquo;Hey this is a thing the robot could do&rdquo;, then quick voice text, and 20 minutes later 80% of the time a working PR.</p>
<p><strong>But that thought, &ldquo;this is a thing the robot could do&rdquo; is very important</strong></p>
<p>For all the advantages of Codex, it’s still just as bad and generally uncreative as any other AI. And of course that is highly dependent on how standard and boilerplate your code is. Codex is better than any agent I&rsquo;ve used at following a super detailed prompt to completion (The first prompt for that achievements feature is almost 2 pages long) - but you still need to know your project well enough to write that super detailed prompt. For my game, I just know there are things I could never do with AI. Those involve messing with the fundamental architecture of the code, rethinking gameplay look and feel, or anything with too many moving parts. AI has a limit, and in 2025 that limit is still quite low. It&rsquo;s a junior developer that writes super fast. But I found that, when shifting from thinking of AI as an assistant I would proactively try to use, vs something I chould completely offload half of my tasks to &ldquo;set and forget&rdquo; - the latter is a much more meaningful time savings. It really forced me to draw the line of what could be automated and what the AI could do for me.</p>
<p>There is another nice little benefit to a system that introduces a neat little PR with comments instead of pouring a bunch of code into your active workspace: it immediately puts you into the mindset of &ldquo;I am reviewing this potentially bad code&rdquo;. It forces you to eaither accept or not. Typically, in &ldquo;assistant land&rdquo; whether that is using chat assistants or things like Cursor, you are going to PR the code yourself. You won&rsquo;t be distinctly &ldquo;reviewing it&rdquo; like a Codex PR, and to other humans it will appear as your work.</p>
<p>I like Codex. A lot. It&rsquo;s more fun to use than something like Cursor and lets me focus on the things I need to. I think this type of system is where automated software development is going, long term.</p>
<h1 id="2026-and-beyond">2026 and Beyond</h1>
<p>With the advent of Autonomous Coding Agents (keep in mind, these only started to be a thing a few months ago), software engineers have a clearer view into how much of their work can be truly automated. That&rsquo;s becuase systems like Codex are &ldquo;all or nothing&rdquo; in some sense. They PR working code or they don&rsquo;t - and there is much less space to get caught up in &ldquo;helping&rdquo; the AI.</p>
<p>Right now, I think about half of the things I typically set out to do on a project like this game are achievable via Codex. That still means there&rsquo;s a long way to go, because there is a long tail of low complexity work in software engineering and Codex is already picking all of that up. But now that AI is working &ldquo;independently&rdquo;, instead of humans always subtly correcting it, that threshold of &ldquo;AI achievable&rdquo; is much clearer. If it&rsquo;s at 50% now we&rsquo;ll see it tick up to 60%, 70%, and so on.</p>
<p>When something like Codex is at 100%, I think the world has entered the <a href="https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion">intelligence explosion</a>.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-07-28:/on-ai-software-development-2/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development-2/</link>
                

                
                    <pubDate>Mon, 28 Jul 2025 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development, 2025 Edition</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Early last year, I came across this tweet:</p>
<p><a href="https://twitter.com/AdamRackis/status/1762321041899012307">https://twitter.com/AdamRackis/status/1762321041899012307</a></p>
<p>The discussion was surprisingly contentious. Maybe I should stop being surprised given the state of Twitter and all social media, but still sometimes shocking how dug-in people are in their beliefs about work. On the face of it, “people get twisted in their relationship with work” seems like a reasonable take. &ldquo;Stop complaining—you are making 16x the median salary in this country. Just do the boring job with the toxic team.&rdquo;</p>
<p>Let’s stop and think about this for a second. The original poster (OP) is making $800K annually due to the appreciation of Spotify stock. An obscene amount of money? Maybe, maybe not. I do not have the typical hang-ups about the wealthy or ultra-wealthy. I don’t see this world as a zero-sum game, and I think the richest people out there have usually done some amazing things, especially in countries like the US where most wealth is not old wealth.</p>
<p>But there is a huge difference in the two main routes to creating becoming very wealthy:</p>
<ol>
<li>Founding a company or practice, or joining one early, out of passion - which then goes on to become incredibly valuable</li>
<li>Working for someone else doing a job you think is boring in order to join the 1%.</li>
</ol>
<p>In this article, I’m talking about the latter.</p>
<p>I propose this core question for this type of person engaging in number 2: Is this large income worth doing a job you find boring, or working with people you feel are toxic? I would say no. Absolutely not. OP is clearly regretting how he’s spending his time at work. He should look for something to do that he values, even if it pays a quarter as much as he gets now. Or less!</p>
<h2 id="the-hedonic-treadmill">The Hedonic Treadmill</h2>
<p>There’s an often misquoted study from ~2010 that personal income beyond about $75K (presumed to cover basic necessities with a comfortable overhead) does not equate to further happiness. This number is actually about right when it comes to simple reported happiness, which has more to do with the hedonic treadmill than anything else. But the study in question was also measuring reported “life satisfaction” as surveyed, which did not stop increasing with income. Of course, “life satisfaction” is a fraught survey metric as well, as it might actually measure a perceived comparative number with one’s neighbors (we’ll get to this later).</p>
<p>But whatever the proper measure of happiness or satisfaction is, there is a great deal of logic to thinking its relationship with income looks like this:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="happiness income curve"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/treadmill-graph.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>As in: there are diminishing returns to incremental income. Not a revolutionary idea. It’s obvious. But it also means that somewhere along this curve is the often-quoted $75K, and somewhere far to the right is OP’s $800K. And the curve between those points is probably pretty flat.</p>
<p>What could OP get in return for moving to the left along this flat section of the curve? Eight hours of time per workday, at least, that he finds incrementally more valuable. How much do you think those 8 hours could improve his life satisfaction on their own happiness/hours curve?</p>
<p>This is where the real paradox comes in. Gaining wealth is supposed to make your time more valuable. You choose to get more services, have a housekeeper, maybe even a private chef. But if those things actually indicate a person values their time more highly than someone poorer, how come they would even consider sacrificing most of that time doing something they find boring, or with people they find annoying? Just to maintain their position along the flat section of the curve?</p>
<p>Maybe they aren’t prioritizing happiness at all.</p>
<h2 id="keeping-up-with-the-joneses">Keeping up with the Joneses</h2>
<p>My argument so far would be absolute blasphemy to most of the $500K+ salary people who don’t have much fun at work. Let’s talk about their two most common counterarguments:</p>







<div class="paige-quote">
<blockquote class="blockquote"><ol>
<li><em>&ldquo;Work should be a sacrifice, and more money means more security for my family.&rdquo;</em></li>
</ol>
</blockquote>


</div>

<p>This is noble. It’s also way too self-sacrificial for the extremely wealthy first-world people we’re talking about. You’re making $800K. There is not a salary sacrifice in the world that will make your family “insecure.”</p>
<p>And what pattern does this even establish? You’re going to choose to be unhappy for most of your waking hours so that you can guarantee your children will be able to do the same? What are you actually working for if not you or anyone else being able to actually enjoy themselves?</p>







<div class="paige-quote">
<blockquote class="blockquote"><ol start="2">
<li><em>&ldquo;But I could make this crazy money and then retire in 5 years.&rdquo;</em></li>
</ol>
</blockquote>


</div>

<p>Now this is a good argument! You could indeed retire in 5 years after making $800K per year and then spend your time with family or pursue passion projects! The problems here are twofold:</p>
<p><strong>First</strong>, nobody does this. Instead, they spend more money. Of course, some of it will be saved, but the main thing that happens when people find themselves with far more money than needed for their family’s security is that they spend it. Bigger houses, more trips, elite schools for the kids. These make you feel well-off compared to your neighbors but don’t push you much higher on that flat curve.</p>
<p><strong>Second</strong>, people find value in being useful. There’s a reason why so many struggle in retirement. Even with a rigid FIRE plan, you’ll still want to work afterward on something interesting. So why not find that now?</p>
<p>What’s actually happening? Lifestyle creep. I think this is 90% of why people feel they could never work on something more fun or meaningful for less money. They’ve already started to spend their new money, and now losing those things would hurt more than gaining them felt good. That damn hedonic treadmill.</p>
<h2 id="what-is-enough">What is &ldquo;Enough&rdquo;?</h2>
<p>While writing this essay I came across another recent one from Adam Singer, who is more blunt about this:</p>
<p><a href="https://www.hottakes.space/p/250k-per-year-is-plenty-of-income">https://www.hottakes.space/p/250k-per-year-is-plenty-of-income</a></p>
<p>Of course I agree, but what I found really funny while reading it is that I see two reactions to this 250K threshold: That it&rsquo;s extremely high, and that it&rsquo;s extremely low. These diverse reactions are only possible in a society where, for many, money has become largely about comparison to others vs something needed to live a satisfied life. I think those who see this and think its an extremely low to be &ldquo;plenty&rdquo; is living in comparison land.</p>
<p>So, think deeply about what you value. Money itself is a means to an end. It&rsquo;s the way you spend your time, and that includes working too, that has true value. Value your own time as much as your spending habits would suggest you do. This is the right logic for your long-term well-being.</p>
<p>We might be approaching a technological singularity. It’s a healthy time to step back, reflect on the fundamental values driving our behavior, and make changes. Wouldn’t it feel silly to spend most of your waking hours working miserably on meaningless things, only to arrive in the second half of your life in a world of true abundance?</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-07-21:/the-treadmill/</guid>

                
                    <link>http://localhost:1313/the-treadmill/</link>
                

                
                    <pubDate>Mon, 21 Jul 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Treadmill</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>During my parental leave, which ends tomorrow, I played through quite a few video games - something I love and one of the easiest ways to spend time while rocking my baby daughter to sleep. And it doesn&rsquo;t hurt that my amazing wife loves watching games about as much as TV or movies with me, as long as they are beautiful or cooperative in some way. All of these are.</p>
<p>They include, in order:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/The_Legend_of_Zelda:_The_Minish_Cap">The Legend of Zelda: The Minish Cap</a> <sup>Steam Deck</sup></li>
<li><a href="https://store.steampowered.com/app/1172450/Carto/">Carto</a> <sup>Steam on Windows</sup> – also where my favorite Daddy-daughter song so far, <a href="https://open.spotify.com/track/669Y0Jq6EBlsvToJ6AeUNz?si=1b321f505f4e4f8c">Bond</a> comes from.</li>
<li><a href="https://store.steampowered.com/app/553420/TUNIC/">Tunic</a> <sup>Steam on Windows</sup></li>
<li><a href="https://store.steampowered.com/app/384190/ABZU/">Abzu</a> <sup>Steam on Windows</sup></li>
<li><a href="https://store.steampowered.com/app/2134770/SteamWorld_Build/">SteamWorld: Build</a> <sup>Steam on Windows</sup></li>
<li><a href="https://www.nintendo.com/us/store/products/world-of-goo-2-switch/">World of Goo 2</a> <sup>Nintendo Switch</sup></li>
<li><a href="https://store.steampowered.com/app/2623190/The_Elder_Scrolls_IV_Oblivion_Remastered/">Oblivion Remastered</a> <sup>Steam on Windows</sup></li>
<li><a href="https://store.steampowered.com/app/2379780/Balatro/">Balatro</a> <sup>Steam on Windows</sup></li>
<li><a href="https://www.nintendo.com/us/store/products/mario-kart-world-switch-2/">Mario Kart World</a> <sup>Nintendo Switch 2</sup></li>
<li><a href="https://www.nintendo.com/us/store/products/nintendo-switch-2-welcome-tour-switch-2/">Nintendo Switch 2 Welcome Tour</a> <sup>Nintendo Switch 2</sup></li>
<li><a href="https://en.wikipedia.org/wiki/Yoshi%27s_Island">Yoshi&rsquo;s Island SNES</a> <sup>Nintendo Switch 2</sup></li>
<li><a href="https://store.steampowered.com/app/1601570/The_Alters/">The Alters</a> <sup>Steam on Windows</sup></li>
</ul>
<p>And yes, I did pick the game from my favorite series that happens to be about miniature people as the first game my miniature daughter Alice would hear.</p>
<p>Most of these are indie titles, but the one I spent the most time on was the Oblivion Remaster - one which surprised me both with how good it looks and with how well it played. Oblivion is a 19 year old game, and a purely graphics-related overhaul should not have made it as good or better than modern AAA games releasing today, but in my opinion (and many others I’m reading) it absolutely did. How could this be?</p>
<h2 id="graphics">Graphics</h2>
<p>Well - I think that’s pretty clear. The only thing that has really improved about mainstream gaming in the last 20 years is graphics.</p>
<p>And boy have the graphics improved. Oblivion not only uses new techniques like ray-tracing and revamped 4K textures and normal maps etc, it uses the full suite of global illumination provided by <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/lumen-global-illumination-and-reflections-in-unreal-engine">Unreal Engine</a>, which means when you turn the settings up, it depends on hardly any of the typical tricks games need to use, like baked light maps, instead lighting almost everything in the game dynamically or “procedurally”, including things like reflections of reflections and lit up dust and fog.</p>
<p>
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/dbd00XaK1so?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<em>This is from my playthrough, some outdoor torches in Bruma and mountain lighting in the background make for a pretty good use of full Lumen</em></p>
<p>Yes, Oblivion still has some simplistic design in terms of how landscapes are laid out, but that simplicity might also be why people can run it with Unreal&rsquo;s Lumen set up to run at ultra. Lighting is what’s really differentiating in video game graphics now, and fully simulated lighting beats or meets nearly every AAA game releasing lately that all cost tens or hundreds of millions of dollars to make.</p>
<h2 id="not-graphics">Not Graphics</h2>
<p>Left unstated is why nothing else has improved. Walking around Oblivion, meeting characters that have some scripted voice lines, responding to them with 1 of 4 options, holding down the left stick to sprint, and hitting a single button on your controller to watch your character animate a full scripted sword swing are all the standard gameplay of action-RPGs both 19 years ago and today. The same can be said of other genres - mechanics are largely untouched for the last 20 years outside of some common quality of life changes in how menus and inventory and HUDs work.</p>
<p>Here’s my theory for why: The games industry achieved the threshold of what was comfortable and possible with the current human-computer input and output mechanisms only a few years after they were technically possible. And those inputs and outputs have not changed much since the 90s. Looking at a flat screen in front of you and pressing buttons on a controller or keyboard/mouse is incredibly limiting for gaming. Increasing the amount of pixels on the screen and making that controller wireless don&rsquo;t change the core gameplay. This is not really true for any other media than games. Reading, watching films, and all kinds of media in between essentially max out on one screen and limited input, but games immediately ran into forms of input and output as a barrier.</p>
<p>I wrote a little about this topic in <a href="/social-media-is-antisocial">my first post about VR and why I think it’s the future</a>. The only true innovation in gameplay that is happening in two places:</p>
<ul>
<li>Indie, where one-man teams can come up with strange mashups and mechanics can execute on a vision spending very little. These are almost never totally genre defining, but they are inventive.</li>
<li>VR, where we have only scratched the surface of mechanics that work and what is possible when the player’s entire hands and 3D field of view are in the game.</li>
</ul>
<p>AAA game developers are excluded from the first by definition and are excluded from the second by their own financial decision makers.</p>
<h2 id="so-what-my-predictions">So What? My predictions</h2>
<p>Maybe people want to keep buying the same games with better graphics forever. I don’t think they do. I think those decision makers that aren’t investing in truly new AAA gameplay are short sighted. As long as we’ve been hearing that VR is the future and not seeing it totally come to fruition, at some point these companies that are milking the same game franchises for years will face the reality that people will only buy the same games reskinned with better graphics for so long. They’ll either be replaced by companies that innovate or individuals using AI that will be plenty good at recreating the same game over and over again.</p>
<p>I am hoping that the current hypestorm around AI re-kindles the ideas in the hearts of AAA game studio CFOs that they might need to invest in innovation and new ideas again. Some of the things that make Oblivion just like any RPG of today (Think about the four-option scripted discourse and stuffy voice lines as two) would be meaningfully different if AI was applied in the right way. And I think they’d be far more powerful experiences, just like I think VR games can be - so much higher fidelity and responsive to player input.</p>
<p>Here are my predictions of things we’ll see in the next decade or so, whether or not AAA studios are the ones to pioneer them:</p>
<p>A decade or two from now, we will look at the period of 2005 to 2025 and see that it was a period of a great stagnation in game innovation. This will be driven by a few key technological advances and the downstream game mechanics that will flow from those, stemming mainly from the areas of AI and what is now called mixed reality.</p>
<p>I’m going to make a few specific predictions of mechanics we will see in the future that will make the stagnation of the last 20 years totally transparent:</p>
<h4 id="ai">AI:</h4>
<ul>
<li>
<p>In open world games, it will become standard for non-player characters to have fully generated dialogues based on motivations and incentives rather than scripts</p>
</li>
<li>
<p>In open world games, players will speak directly or engage directly in some way with their own words that NPCs will understand and react to intelligently</p>
</li>
<li>
<p>NPCs won&rsquo;t have recorded voice lines, but will instead have voices generated in real time, and AAA games will start to make contracts with celebrity talent in order to generate their voice in games</p>
</li>
<li>
<p>We will see a transition from the current minorly procedural elements of gameplay to full procedural generated worlds, especially where player actions and environmental events change landscape, buildings, etc dynamically</p>
</li>
<li>
<p>Game art will go through an incredible revolution, and we will stop having massive teams of people creating 3D meshes and textures to drop around the world, with many of these meshes and textures being generated from prompts in development, but also being generated on the fly in games</p>
</li>
</ul>
<h4 id="mixed-reality">Mixed Reality:</h4>
<p>The first reasonable augmented reality glasses that consumers are willing to wear en masse will generate entirely new genres:</p>
<ul>
<li>
<p>A new genre of video board games, where gameplay happens in view of a group of people in the same physical space, on a table, on the ground, in a park, etc, that will incorporate video game elements such as computations that are too complicated for typical board games, with the advantages that board games have today where players can interact with the same physical pieces, point and gesture, and socialize in person</p>
</li>
<li>
<p>Another new genre of exercise programs combined with games. This will go far beyond current treadmills that have built-in virtual run routes or virtual exercise classes, this will be exercise incorporated as a leveling or other mechanic that will incentivize players to get some exercise. I&rsquo;ve had a lot of hope for this category for a long time, and the one major threat to it is GLP-1 drugs that may cause a serious decline in the need and desire for people to exercise daily for calorie loss</p>
</li>
</ul>
<p>When will these happen? My specific prediction is that the next 3 years will be seen as the end of the period of stagnation we are in right now, a period that will be much more apparent looking backwards than it is from within. The combo of a decline in AAA game spending and AI hype feels like ripe conditions for innovation to return to gameplay - and my hope is that all of the other potential gameplay innovations ride the same AI wave.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-07-07:/the-purgatory-of-aaa-gaming/</guid>

                
                    <link>http://localhost:1313/the-purgatory-of-aaa-gaming/</link>
                

                
                    <pubDate>Mon, 07 Jul 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Purgatory of AAA Gaming</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have long been of the mind that LLMs and their evolutions are truly thinking, and that they are on their way to solving all of the intellectual tasks that humans can solve today.</p>
<p>To me, it is just too uncanny that the technology that seems to have made the final jump to some degree of competence in tasks that require what is commonly understood as &ldquo;thinking&rdquo; or &ldquo;understanding&rdquo;, after a long string of attempts and architectures that fail these tasks, is a type of neural network. It would be much easier to argue away transformer models as non-thinking stochastic parrots if we had happened to have had success with any other architecture than the one that was designed to mimic our own brains and the neurons firing off to one another within them. It&rsquo;s just too weird. They are shaped like us, they sound like us in a lot of ways, and it&rsquo;s obvious they are thinking something like us too.</p>
<p>I am not saying they are as good as us yet, though, for a few small reasons and one big one.</p>
<h2 id="the-limitations">The Limitations</h2>
<p>Current frontier &ldquo;thinking&rdquo; models are not AGI in the modern definition. They can&rsquo;t do every task humans can do intellectually (IE without a body, which I will get to) for several reasons:</p>
<ol>
<li>
<p>Looping/Recursive reasoning:
This was a huge problem for early transformers that had to output in one shot, and the examples were obvious. This one has been essentially solved via thinking models like o1, and now o3, gemini and grok thinking models, and many more. That was a huge unlock and a huge boon for applications like programming where there is lots of nested recursion of logic that has to occur to find a reasonable solution.</p>
</li>
<li>
<p>Memory and context:
Context windows get larger all the time, but this limitation still is not solved. Just adding a bunch of tokens into a context window doesn&rsquo;t get you much when 2 million token models lose coherence after about the first 40,000 - which they do, and which every programmer working with anything but a tiny codebase intuitively understands. But this one too will largely be solved soon, if not through architectures that actually update their weights, it&rsquo;ll be solved through nuanced memory systems that people are actively developing on top of thinking models.</p>
</li>
<li>
<p>Size:
This is basic, but most of the models we can interact with today are still working with an order of magnitude fewer neural synapses than human brains. It could very easily be that, even with the other problems solved, we just need bigger electronic brains to match the size of our meat ones. It certainly <em>feels</em> like some of the ways LLMs fail today sort of come down to &ldquo;not enough horsepower&rdquo; types of issues.</p>
</li>
<li>
<p><strong>Vision</strong>:
And this one might sound funny to someone that is paying attention to AI in particular, because GPT-4 with vision launched something like 2 years ago now. And it has been impressive for a long time, able to do things like identify what objects are in an image, where an image is from, etc: things most humans can&rsquo;t do glancing at an image, that seem super-human.</p>
<p>But the vision itself is not “good” vision. It cannot really pick out small important details, and it still behaves in many ways like vision recognition models have for years now. Now that we have a model that has both thinking and image input and editing at every step, the o3 and o4-mini series that recently released, we can really start to see the limitations in vision. Let me take you through 2 examples that represent the 2 types of failure modes that result from these not having true image understanding, yet.</p>
<p>My thesis today is that this is the key limitation that is not going to be overcome &ldquo;by default,&rdquo; but that it will be overcome.</p>
</li>
</ol>
<h2 id="proof-the-vision-is-not-there-yet">Proof the Vision is Not There Yet</h2>
<p>Each release from the major providers steadily knocks away my intelligence tests, which I admit are mostly programming oriented, but the ones that they can never really dent are the spatial reasoning ones - where a model really has to think about images in its head or use an image provided for detailed work.</p>
<h3 id="simple-3d-modeling-with-frontier-ai-models">Simple 3D Modeling with Frontier AI Models</h3>
<p>Every major model release, <a href="/3d-modeling-with-ai">I test what models can do with OpenSCAD</a>. I won’t get technical about it here, but OpenSCAD is a CAD program (Computer Aided Design - think 3D modeling for engineers, not the artistic kind) that is defined entirely through a programming language vs the typical mouse strokes and key presses that software like SolidWorks or AutoCAD depend on.</p>
<p>This makes OpenSCAD the perfect test platform for a model that inputs and outputs text primarily. I can describe a 3D model I want, and the model can output text that renders into a 3D model.</p>
<p>As amazing as LLMs are at scripting in normal programming languages, they have never been good at OpenSCAD. See my link above for GPT-3.5 and GPT-4 trying to model an incredibly simple object. That acorn was about as complex as GPT-4 could get without really falling down.</p>
<p>Here is OpenAI&rsquo;s o3’s attempt to make a standard 2x4 Lego Brick:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="an OpenSCAD render"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-lego.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">o3&#39;s model left, real Lego brick right</span>
    
</div>

<p>This was the easiest object I tested, and o3 does a decently good job. It grabbed the correct dimensions online and, using its inherent training data of what a 2x4 Lego block is, applied those dimensions into a mostly coherent object. It has one major flaw, which you can see on the underside as I have the image rotated - it drew two lines through two of the cylinders. My guess is that this is its interpretation of the supports in the middle of the actual Lego brick, that connect but don&rsquo;t run through the center cylinder.</p>
<p>Now for a harder test: a simple engineering part that&rsquo;s definitely not in its training data, because it is my own design. I printed this for a robotics project more than a decade ago, and had it sitting around in my 3D printer storage drawer.</p>
<p>It&rsquo;s a bit of a weird part - a pinion with a smooth section and an 11-tooth gear of equal diameter, and a hole in the center with a slightly raised wall. This is the kind of part that an engineer well versed in AutoCAD or SolidWorks can produce in just a few minutes, but which requires attention to detail and a conceptual model of how parts fit together.</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="an OpenSCAD render"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-pinion.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">o3&#39;s model left, real part right</span>
    
</div>

<p>This is where you can see how these models fall apart. o3 immediately gets that it&rsquo;s a pinion, that it has a hole in the middle, and that it has a smooth section on the bottom and a gear on top. But the execution is nowhere close to workable, from most major to least (in my opinion):</p>
<ul>
<li>There are two gears (unknown as to why or if this is intentional, o3 explained one as a &ldquo;grooved ring&rdquo; - whatever that means)</li>
<li>The gear teeth are concave - whereas the rounded sharp tooth shape is clear in the image</li>
<li>There are 10 teeth, not eleven - which seems trivial, but it&rsquo;s indicative of a real flaw that messes up all complex models I throw at AI - where LLMs make an assumption like what number of teeth is &ldquo;likely,&rdquo; rather than looking at the image in detail and counting them.</li>
<li>There is clearly an attempt at the raised wall around the top hole, but it&rsquo;s far too big.</li>
<li>The height of the smooth base section and gear sections are equal in the real part, but o3 makes the gear more than 3x thicker than the base.</li>
</ul>
<h3 id="map-reading">Map Reading</h3>
<p>Here&rsquo;s another great example of what I mean when I say that frontier models have bad vision.</p>
<p>I recently gave this question to the latest thinking image model, o3: &ldquo;Here&rsquo;s an image from Google Maps of the block I live on between the avenues of Burbank, Hazeltine, Oxnard, and Van Nuys. What is the longest continuous loop I can walk within the neighborhood without crossing my path or touching one of the avenues? This square is 1/2 mi on each side&rdquo;</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="the uploaded map"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-struggle-map-new.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">The image uploaded with this query</span>
    
</div>

<p>O3 thinks for 4 minutes about this question, zooming in to various parts of the map countless times to form the route. And then it fails on the first step, suggesting starting at Tiara and Stansbury, which do not intersect on the map. Any person looking at this image could tell that is true in just a few seconds.</p>
<p>This is what I mean when I say these things have bad vision - and this is the best model from the lab I think has the best vision. Vision is not about being able to identify millions of different objects, <a href="https://www.image-net.org/">ImageNet-style</a>. It&rsquo;s about seeing the detail and paying attention to the right thing. Here in this map, that means looking roughly at the lines representing Stansbury and Tiara, looking at where they would intersect, and seeing they do not.</p>
<p> </p>
<p>Though getting AI models to read maps and create 3D models from code may not be on everyone&rsquo;s rubric, any UX developer that has worked with a frontier AI knows what I&rsquo;m saying intuitively. This is likely just as true for any role heavily leaning on visual information. There is a difference between generating some Tailwind code that spits out a standard UI and getting to the level of complexity that the AI starts to need to look at screenshots in detail and know the relative position and orientation of components, or see small details. They just.. don&rsquo;t do that yet.</p>
<h3 id="non-frontier-ai-models">Non-Frontier AI Models</h3>
<p>A common retort to this argument might be &ldquo;Well Brian, you&rsquo;re using the wrong type of model.&rdquo;</p>
<p>But believe me, I try essentially everything I can get my hands on, and like non-LLM AI models in other domains, other than being incredibly limited in application, these models are simply not good at vision, either. Here&rsquo;s an image of a 3D model generated by Zoo CAD, a company doing text and image to 3D, which was, when I printed it back in January at least, state of the art in this domain:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="My Flipper Zero and 3D Printed Clone"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/flipper-3d.jpg"   style="height: auto; max-width: 720px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">My Flipper Zero (left) and its 3d-printed clone (right)</span>
    
</div>

<p>The input to the model was a picture I took of my <a href="https://flipperzero.one/">Flipper Zero hacking toy</a>. The 3D print on the right is of the 3D model it produced, with the only modification on my part being to scale it.</p>
<p>I have long awaited an easy way to &ldquo;3D scan&rdquo; parts from the real world into software. This would make so many of my DIY printing jobs around the house much easier - and it gets us one step closer to the dream of teleportation.</p>
<p>And I was impressed when I first saw this output - it was one of the best of my attempts with this AI tool. Feature-wise, it did a pretty good job of capturing the main facets of my Flipper Zero. But it&rsquo;s just too obviously not good enough: dimensions are all wrong, there is hallucinated symmetry all over the place, and small details are missed everywhere. One of the reasons I printed this model was just to get a real feel for how similar it is to the Flipper when scaled correctly, and it just becomes obvious that this is not a useful technology yet when you hold both of them in your hands.</p>
<p>My prediction is that frontier models will get there before purpose-trained models like the one that cloned my Flipper Zero. This has been the reality across most other domains of AI. Now that we have general-purpose AI models that have started to encapsulate a working, if not complete, conceptual model of the world, they are starting to outperform all of the smaller purpose-built models of the last decade.</p>
<p>And this will be good for the general capability of the AI industry: It&rsquo;s a much better outcome to have a few general-purpose models that can do fine visual work, that can be prompted and orchestrated by people working in different domains, than it is to need an AI lab or startup to train a specific model for every task. It&rsquo;s the same reason I believe humanoid robots are going to win over purpose-built ones in the long run: We can&rsquo;t predict what users will want their robots to do, and our world is built for humans to operate in it - therefore the most capable robots will be shaped generally like humans.</p>
<p>So how will frontier models get good at visual tasks, if they have such bad vision right now? I think the answer to that question also involves humanoids.</p>
<h2 id="the-humanoids">The Humanoids</h2>
<p>What I think is going on in these examples of frontier model vision failures is that we have a limitation in training data (duh!) - but that isn&rsquo;t because there aren&rsquo;t a lot of images and videos on the internet, it&rsquo;s because there is so much more information in the average image than there is in the average chunk of text, and a lot more of that information is irrelevant to any given question.</p>
<p>When I say that we have a limitation on training data, I&rsquo;m not in the typical camp of &ldquo;well, then transformer neural nets are obviously stupid because I was able to understand this thing without training on terabytes of data from the internet&rdquo;. This has always been a bad take because the average human trains on petabytes, not terabytes of data, and that data is streamed into their brains mostly in the form of images. I am also not in the camp of thinking that this means that the data &ldquo;just doesn&rsquo;t exist&rdquo; to get these models to AGI in the visual dimension. It so clearly does exist, and it exists so abundantly that a unique image stream can be sent to each of the billions of human brains, and they all learn the same principles that let them immediately identify the mistake that the cutting-edge thinking vision model made after 4 minutes of rigor.</p>
<p>Not only does the data exist: We never actually had a data problem in AI in the first place. We have an instruction problem. That doesn&rsquo;t mean model architecture or data massaging really, it means that we need to plug our models into the real world where all the data streams exist. I believe this will come first in the form of robots with cameras on them, the first of which is happening en masse via Tesla full self-driving AI, and I&rsquo;m sure those vision neural nets are quite insanely capable compared to what we see in the consumer transformer vision models. But the real leap probably comes when we get to humanoid robots walking around collecting and learning from vision data every day - and learning from actions they take in the real world.</p>
<p>If you’ve ever tried to get concrete actions to take based on a vision-transformer’s outputs, you will know it’s hard. I never posted about it, but I did a small project with a friend a year or two ago, trying to get automated QA working on web software, and we failed in a lot of different ways. I am very impressed that the big labs are starting to crack <a href="https://docs.anthropic.com/en/docs/agents-and-tools/computer-use">computer use</a> - because getting an LLM to give specific coordinates or elements to click on is the same type of challenge I was testing above. But it&rsquo;s no wonder these computer use applications are still very inaccurate.</p>
<p>Letting transformer-based agents control robots will be a much harder problem of a similar type. Not only does it require attention to detail, but now the images are in 3D, they come at you many times per second, and actions need to be produced as quickly. But my prediction is that we will brute force this, and it will work &ldquo;well enough&rdquo; for enthusiasts and niche industrial use cases to benefit from humanoid robots. And that&rsquo;s the takeoff point for true vision, as we all intuitively understand it. Releasing humanoids at scale (and cars, to some extent) are when we really unleash the datastream that&rsquo;s needed to get models that can see as well as you or I can. This is probably one reason why many AI companies and labs are pushing them so hard - they also understand the data they collect will be more valuable than the money paid for the first units.</p>
<p>Once we have a few hundred thousand humanoids roaming around early adopters&rsquo; houses, we will start to see AI models that can use OpenSCAD and Google Maps. And, conveniently, that&rsquo;s also the missing capability that will make the humanoids really useful. We have interesting years ahead of us.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-06-16:/vision-is-the-last-hurdle-before-agi/</guid>

                
                    <link>http://localhost:1313/vision-is-the-last-hurdle-before-agi/</link>
                

                
                    <pubDate>Mon, 16 Jun 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Vision is the last hurdle before AGI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Brian and Alice"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/brian-and-alice-gaming-zoom.jpg"   style="height: auto; max-width: 400px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">↑ me and Alice</span>
    
</div>

<p>I&rsquo;m Brian Hockenmaier, dad, engineer, and fun haver.</p>
<p>I grew up in Ventura, California with two UCLA-engineer parents. Even though neither of them were engineers long-term, I was an engineering child. I was always building something, highlights including a functional submarine and weather balloon with Lego robotics.</p>
<p>I went to school for industrial engineering at <a href="https://www.calpoly.edu/">Cal Poly SLO</a>, nearly double majored in economics, and then promptly moved out of both fields just like my parents had moved out of their education fields of mechanical and aerospace. Since then, after reading Ray Kurzweil&rsquo;s <a href="https://en.m.wikipedia.org/wiki/The_Singularity_Is_Near">The Singularity is Near</a> in 2011, feeling I needed to be closer to software and realizing I liked building systems more than anything in industrial engineering, I&rsquo;ve built a career in software and then AI.</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="A lego crane"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/lego-crane.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">One of the few pictures I have from my time building robots with legos, showing a robotic crane to my Dad</span>
    
</div>

<p>At work, I try to be low time preference, high autonomy, low consensus (even contrarian), high invention, process-light.</p>
<p>I&rsquo;ve been at one company for a long time for a millennial. I&rsquo;ve built a lot of enterprise software and four high-performing software teams. I started on back-office stuff like billing systems, TV scheduling systems, IP rights management systems, and file ingest systems for News. Most recently I’m making some pretty awesome stuff in agentic AI for the mundane work that the people at my company have to do!</p>
<p>My bias is to build rather than talk. I&rsquo;m the one pushing to just try to build the thing, to scrap it early if needed, and to take risks releasing early. I figure that we always know more, and usually throw away our plans, when we start to build. I love building strange things, useful things, fun things. One of the things I built was this website and the game running on top of it. <a href="/this-website">Read more about that here</a></p>
<p>Other Profiles:</p>
<p>Most of my development projects are stored on public or private repos on <a href="https://github.com/hockenmaier">my github</a></p>
<p>A collection of physical projects and 3D designs can be found on <a href="https://www.thingiverse.com/hockenmaier/designs">my thingiverse</a></p>
<p>And my professional persona can be found on <a href="https://www.linkedin.com/in/hockenmaier/">my linkedin</a></p>
<hr>
<h1 id="where-to-start">Where to start</h1>
<p>I post here mostly about things I&rsquo;m making and occasional essays. If you’re interested in this site but don’t know where to start, try reading any of these posts that sound interesting to you. They&rsquo;re either pieces or projects I put a lot of time into.</p>
<h2 id="video-games">Video Games</h2>
<p>I&rsquo;ve been developing video games on and off since 2017 or so. This has entailed more than 6 project starts and 3 finishes - those are below.</p>
<p> </p>







  








  
  

  

  
  
    
    
  








<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/land-war/">
          <img src="images/land_war_e3.png" alt="Land War">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/land-war/">Land War</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>Land War is an 8-player strategy game I developed as a solo project and released to Steam in March of 2019.<br>
This game was intended to have low art requirements and simple interaction rules that result in deep strategic gameplay.</p>
<p>The core concept is that of an ultra-simplified real-time-strategy game. Each player is represented by a color and can grow their territory by moving in any direction. The strategic elements occur when players encounter other players and have to make choices about which side of their land to defend or give up. Players can use the structure of the map and the coordinated action of other players to gain defensible footholds in order to take more area and eventually be the last player on the board.</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      March 1, 2019 · 3 minutes ·
      <a href="/land-war/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>Land War was my first published video game, released using Unity after many unfinished starts in HTML5 and Unreal Engine. I put myself on a tight timeline, built in 6 months in my spare time and released on Steam only, where it got a few hundred downloads and made a bit over $1000</p>
</blockquote>
<p> </p>
<hr>
<p> </p>







  








  
  
  






<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/treekeepers-vr/">
          <img src="http://localhost:1313/images/treekeepers_moonlight.png" alt="Treekeepers VR">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/treekeepers-vr/">Treekeepers VR</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    







    



    


























    
        
    
</div>

<p>Treekeepers VR is a networked VR game where up to 4 players can cooperate to navigate an oversized world and save a giant tree.</p>
<p>Treekeepers is in production on both Quest (standalone VR) and Steam (PC VR) with full cross-play functionality. See the <a href="https://togetheragainstudios.com/treekeepersvr/">Treekeepers VR Website</a> for links to all storefronts and more detail about the game.</p>
<h2 id="heading"></h2>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      October 1, 2022 · 2 minutes ·
      <a href="/treekeepers-vr/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>Treekeepers VR was an ambitious game development project I took on in 2021- my first published VR title and my first multiplayer title. The latter turned out to be the really hard part. It&rsquo;s now free on two platforms. Read more about it:</p>
</blockquote>
<p> </p>
<hr>
<p> </p>







  








  
  

  

  
  
    
    
  








<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/this-website/">
          <img src="images/bubble4.gif" alt="This Website">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/this-website/">This Website</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>I&rsquo;m <a href="/about-me">Brian Hockenmaier</a>, and this site is full of things I build and write about. I love making games and things with VR and AI. And I love DIY projects, especially ones involving programming, engineering and 3D modeling. Some of this has been cross or back-posted from my <a href="https://www.thingiverse.com/hockenmaier/designs">thingiverse</a>, <a href="https://github.com/hockenmaier">github</a>, <a href="https://www.linkedin.com/in/hockenmaier/">linkedin</a>, and other places, but it all lives here permanently.</p>
<p>This is an evolution of <a href="old-site/index.html">my previous site last updated in 2022, which I still keep inside this one</a> for posterity and for the AIs of the future to know more about me. I like it not because of the content as much as because it was a fully custom js and html site with no framework&hellip; and I think it&rsquo;s sort of fun and funny that it was like this.</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      May 28, 2025 · 13 minutes ·
      <a href="/this-website/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>This Website itself was my 3rd released video game</p>
</blockquote>
<p> </p>
<hr>
<h2 id="top-3d-prints">Top 3D Prints</h2>
<p>I&rsquo;ve been 3D Modeling since Cal Poly and printing since I got my first printer - the MakerGearM2 - in 2013. I have dozens of designs on thingiverse, some of which got pretty popular. A lot of my best work is made up of 3D printed games and game paraphernalia.</p>
<p> </p>







  








  
  
  






<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/bloom-or-bust/">
          <img src="http://localhost:1313/images/bloom-or-bust-characters.jpg" alt="Bloom Or Bust!">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/bloom-or-bust/">Bloom Or Bust!</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>Here&rsquo;s a peek at my first major board game creation!</p>
<p>I&rsquo;ve made a few clones or slight enhancements of games I like before like <a href="/nope-game">Nope</a> and <a href="/hocken-pocket-blokus">Hocken-Pocket-Blokus</a>, but Bloom or Bust is my first attempt at something of my own design with very high production quality.</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    


























    
        
    
</div>

<p>This is a risk-taking game where players compete to take over a tree with their specific type of fruit. Trees and bees are becoming a recurring theme in my games!</p>
<p>All parts of this game are hand-designed by me, mostly in VR with a tool called Gravity Sketch, blender, and one of my favorite programmer&rsquo;s 3D tools called OpenSCAD.</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      May 28, 2025 · 2 minutes ·
      <a href="/bloom-or-bust/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>Bloom Or Bust is my latest and best board game. Fully designed from scratch using OpenSCAD and Gravity Sketch (A very fun VR-based 3D Modeling program)</p>
</blockquote>
<p> </p>
<hr>
<p> </p>







  








  
  
  






<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/8-bit-coasters-10-year/">
          <img src="http://localhost:1313/images/2023-8-bit-set-2.jpg" alt="8-Bit Videogame Coasters, 10 Year Anniversary Edition">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/8-bit-coasters-10-year/">8-Bit Videogame Coasters, 10 Year Anniversary Edition</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>If you like retro video games and also drinking things, you&rsquo;re in luck!
Print them for your living room! Print them for your friends!
I hope these characters remind you of some of your favorite series.</p>
<p>I returned to <a href="/8-bit-coasters">this project</a> after 10 years to make more coasters, including some designs for multi-plastic printers and a reinforced 4-coaster holder!</p>
<div class="paige-row-wide">
  <div style="display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:15px;">
      































    













    



    


























      































    













    



    

























</div>
</div>

<div class="paige-row-wide">
  <div style="display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:15px;">
      































    













    



    


























      































    













    



    

























</div>
</div>

<p>It was super fun returning to this project! I was able to fix a few things that I had noticed failing over the years, like the new reinforced sides to the &ldquo;? Block&rdquo; holder. Now that multi-color prints are proliferating more widely, I&rsquo;m hoping people can use the new colored files - they really pop with the Silk+Matte PLA combo I used here. Links to those filaments:</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      August 6, 2023 · 1 minute ·
      <a href="/8-bit-coasters-10-year/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>My most popular 3D design, remade for color in 2023.</p>
</blockquote>
<p> </p>
<hr>
<p> </p>







  








  
  
  






<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        <a href="/nope-game/">
          <img src="http://localhost:1313/images/nope-1.jpg" alt="NOPE - A 3D and 2D Printed Card Game">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/nope-game/">NOPE - A 3D and 2D Printed Card Game</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>This game is based off of a similar card game called &ldquo;No Thanks!&rdquo; but expands the number of players to 8.</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      January 19, 2015 · 2 minutes ·
      <a href="/nope-game/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>My first custom game, NOPE is an expanded version of No Thanks that I released for free on Thingiverse</p>
</blockquote>
<p> </p>
<hr>
<p> </p>







  








  
  
  






<article class="latest-item mb-4" style="display:flex; align-items:flex-start;">
  <div class="media-column">
    <div class="media-container" style="width:300px; height:300px;">

      
        
        <a href="/3d-key/">
          <img src="https://img.youtube.com/vi/_H2W8qXUJtg/hqdefault.jpg" alt="3D Printed Key">
        </a>

      

    </div>
  </div>

  <div class="text-column" style="align-self:flex-start; margin-top:0;">
    <h2 class="post-title" style="margin-top:0 !important; margin-bottom:0 !important;">
      <a class="link-light" href="/3d-key/">3D Printed Key</a>
    </h2>
    <div class="post-summary" style="margin-top:0; margin-bottom:.5rem;">
      <p>I 3D Modeled and printed my apartment building&rsquo;s key with the Makergear M2. I won&rsquo;t be posting the model because it IS in fact a key to my apartment. Thanks for watching!</p>
    </div>
    <p class="read-more text-secondary fst-italic" style="margin:0;">
      May 13, 2013 · 1 minute ·
      <a href="/3d-key/">Read more →</a>
    </p>
  </div>
</article>


<blockquote>
<p>One of my first great 3D printing experiments, I duplicated my apartment key in CAD shortly after getting my first 3D printer, and actually used these keys as spares.</p>
</blockquote>
<p> </p>
<hr>
<p> </p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-04-30:/about-me/</guid>

                
                    <link>http://localhost:1313/about-me/</link>
                

                
                    <pubDate>Tue, 30 Apr 2024 00:00:00 UTC</pubDate>
                

                
                    <title>Hi, I’m Brian</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="this is a robot"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-software-dev.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>Lots of chatter right now about AI replacing software developers.</p>
<p>I agree - AI will take over software development. The question is: what work will be left when this happens?</p>
<p>Some considerations:</p>
<ul>
<li>Benchmarks for the best LLMs still put them solidly in the &ldquo;bad at programming&rdquo; category, scoring in the 5th percentile of human programmers on common tests. Meanwhile, LLMs score in the 80th-95th percentile for law exams and 85th–100th for psychology, statistics, and many other less technical fields. More scores available in the &ldquo;simulated exams&rdquo; section of <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>.</li>
<li>Engineers have been using language models like tabnine and copilot as &ldquo;super-stackoverflow&rdquo; style code assistance years before chatGPT released. This means much of the velocity increase we might expect from current LLMs&rsquo; ability to write code has already been &ldquo;priced in&rdquo; to the market.</li>
<li>Many of the trends making software development more costly are growing, not shrinking: Systems are becoming more distributed. The cloud lowered infrastructure costs but made applications more complex. We&rsquo;re making more and deeper integrations among disparate systems. Auth is becoming more secure and thus complex (managed identity, MFA, etc).</li>
</ul>
<p>Github copilot chat and other LLM dev tools are speeding up the rote stuff. I’ve seen it in my own work.</p>
<p>And I really do believe new AI models will do more than just the basics, maybe in the next couple of years. Even precluding &ldquo;AGI&rdquo;, the trend we are on is that more and more work is automatable, and engineers, especially more junior ones - are going to have to shift focus away from algorithmic work that AI can do.</p>
<p>But by the time our neural nets are &ldquo;good enough&rdquo; at building software to make it significantly cheaper to build, I doubt this trend will make the news. Everything else gets automated too.</p>
<p>These are my thoughts at what seems to be the beginning of the next AI revolution in early 2024. I plan to revisit this topic and see if I&rsquo;m right in future posts.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-01-24:/on-ai-software-development/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development/</link>
                

                
                    <pubDate>Wed, 24 Jan 2024 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have been occasionally challenging GPT to create models using <a href="https://openscad.org/">OpenSCAD</a>, a &ldquo;programming language for 3D models&rdquo;</p>
<p>Both struggle, but GPT-4 has been a massive improvement. Here are both models&rsquo; outputs after asking for an acorn and 3 messages of me giving feedback:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="some weird acorns"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/gpt-acorn.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>For the record, it is impressive that these LLMs can get anything right with no visual input or training on shapes like these. Imagine looking at the programming reference for openSCAD and trying to do this blind. The fact that the 3.5 version has a bunch of strangely intersecting primitives and some union issues has been normal in my experience. It takes quite a bit of spatial logic to get a model not to look like that.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2023-03-19:/3d-modeling-with-ai/</guid>

                
                    <link>http://localhost:1313/3d-modeling-with-ai/</link>
                

                
                    <pubDate>Sun, 19 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>3D Modeling With AI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<blockquote>
<p>Editor&rsquo;s note from 2025:</p>
<p>This article was written as part of the launch of <a href="/treekeepers-vr">Treekeepers VR</a> and the sole proprietorship Together Again Studios, and represents some of my core beliefs of the value of VR and where it&rsquo;s taking us socially. Though I&rsquo;m no longer actively working on Treekeepers, I do hold that VR and AR are truly the &ldquo;endgame&rdquo; of interface and one that could save us from some of the social attitudes caused by social media of today. Enjoy!</p>
</blockquote>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="proofreader GPT"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/together-again-studios.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
</div>

<p>With Together Again Studios and Treekeepers VR, we&rsquo;re setting out to solve an insidious problem we see all around us:</p>
<p><strong>Social Media Is Anti-Social</strong></p>
<p>Though Facebook, Instagram, Twitter, and Tiktok all let us share more with each other than ever before, what we are sharing is surprisingly hostile and dismissive of opinions other than our own.</p>
<p>Though Zoom, Hangouts and Teams let us finally see each other from a distance, we still can&rsquo;t speak naturally. We depend on tools like &ldquo;mute&rdquo;. We create meeting upon meeting with different sets of the same group of people. And we don&rsquo;t form the depth of relationships we could in-person.</p>
<p>We as humans are all-too-capable of forming us-versus-them &ldquo;tribes&rdquo; and dehumanizing those who appear too different, and this problem is becoming ever more apparent behind the curtain of the graphical user interface.</p>
<p><strong>Virtual and augmented reality are a way out</strong></p>
<p>In 2016, thanks to pioneers like Palmer Luckey, Michael Abrash, and John Carmack, we suddenly gained access to a technology that removes the curtain and forces us to see eachother. And in 2020, an event that has permanently limited our in-person interaction arose and gave new meaning to this technology.</p>
<p>In VR/AR, voices are no longer text on a screen, taken out of context by our social media bubbles. They&rsquo;re voices again.</p>
<p>In VR/AR, people are no longer user profiles with one image and a tag-line. They&rsquo;re really people, with bodies, faces, and hands that can point and gesture.</p>
<p>In VR/AR, messages are not just &ldquo;public&rdquo;, or &ldquo;direct&rdquo;. Conversations are dynamic, with people physically approaching one another to talk, with people moving in and out of physical groups, and with people attending public conversations together again while still able to have &ldquo;sidebar&rdquo; conversations.</p>
<p>All these abilities we used to have in-person, we have gained again in virtual reality.</p>
<p>Soon, we&rsquo;ll go even further with this technology. We&rsquo;ll be able to make real eye contact with eachother in VR. We&rsquo;ll use AR to invite distant friends and family over to our homes.</p>
<p>And at Together Again, we plan on using these new tools to let people like eachother again.</p>
<p><strong>Treekeepers: Only Possible in VR</strong></p>
<p>Why is Treekeepers a VR Game?</p>
<p><strong>Multiplayer of this depth only works in VR</strong></p>
<p>The challenges in Treekeepers VR hinge on player coordination and quick group decisions - Which weapon should we upgrade? Who&rsquo;s doing which job? Where are we going?</p>
<p>In VR, you gain the ability to gesture and point to enemies and obstacles naturally.</p>
<p>No more &ldquo;Who is the green player?&rdquo; - spend zero mental energy figuring out who you are talking to. Just turn towards them and speak.</p>
<p><strong>Scale is the most fun when you&rsquo;re in the world</strong></p>
<p>We&rsquo;re going to be exploring oversized objects around a gigantic tree. Only VR can get the full benefit of this experience.</p>
<p><strong>VR can be uncomfortable</strong></p>
<p>BUT, experiencing it via a static vehicle which acts as a persistent frame of reference reduces motion sickness.</p>
<p>No need to rotate - gameplay is based on the hot air balloon always facing one direction, and players navigating within it.</p>
<hr>
<p>Check out Treekeepers VR <a href="/treekeepers-vr">here</a></p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2022-08-01:/social-media-is-antisocial/</guid>

                
                    <link>http://localhost:1313/social-media-is-antisocial/</link>
                

                
                    <pubDate>Mon, 01 Aug 2022 00:00:00 UTC</pubDate>
                

                
                    <title>Social Media Is Anti-Social</title>
                
            </item>
        
    </channel>
</rss>
