<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Brian Hockenmaier&#39;s Homepage</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Brian Hockenmaier&#39;s Homepage</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Human Joystick VR</title>
      <link>http://localhost:1313/human-joystick/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/human-joystick/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/human_joystick_centered.jpg&#34; alt=&#34;Human Joystick centered&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Human Joystick VR&lt;/strong&gt; explores a hybrid locomotion system where players move through a virtual environment by physically changing their position within their VR playspace.&lt;/p&gt;&#xA;&lt;p&gt;Link to the project: &lt;a href=&#34;https://github.com/hockenmaier/humanjoystick&#34;&gt;Human Joystick on Github&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;concept&#34;&gt;Concept&lt;/h2&gt;&#xA;&lt;p&gt;The &amp;ldquo;Human Joystick&amp;rdquo; solves a common VR problem: providing natural movement in large virtual environments. Players can physically move within a playspace, but once they approach the edge, the system detects the movement vector and shifts the playspace accordingly.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Raspberry Pi Control Panel</title>
      <link>http://localhost:1313/raspberry-pi-panel/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/raspberry-pi-panel/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Raspberry Pi Control Panel&lt;/strong&gt; is a hardware project designed to manage home automation systems. The project involved designing a custom 3D-printed case for a Raspberry Pi microcomputer with a touchscreen interface.&lt;/p&gt;&#xA;&lt;p&gt;Links:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/hockenmaier/RaspberryPiControlPanel&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.thingiverse.com/thing:2524560&#34;&gt;Thingiverse&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;This project was primarily a hardware initiative. The panel was mounted to a wall and connected to SaaS home automation systems. A custom HTML wrapper was created to control the orientation and settings of the interface.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Land War</title>
      <link>http://localhost:1313/land-war/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/land-war/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/land_war_mvp.gif&#34; alt=&#34;Land War 4-day MVP&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Land War&lt;/strong&gt; is an 8-player real-time-strategy game that combines minimalistic graphics with deep strategic gameplay. Players grow their territories and face off against each other in a battle for dominance.&lt;/p&gt;&#xA;&lt;p&gt;Link to the project: &lt;a href=&#34;https://store.steampowered.com/app/1030960/Land_War/&#34;&gt;Land War on Steam&lt;/a&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;gameplay&#34;&gt;Gameplay&lt;/h2&gt;&#xA;&lt;p&gt;The core concept of Land War is an ultra-simplified RTS. Players expand their territories and strategically decide which side of their land to defend or sacrifice when encountering opponents. By leveraging the map structure and coordinating with others, players aim to outlast their opponents.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Treekeepers VR</title>
      <link>http://localhost:1313/treekeepers-vr/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/treekeepers-vr/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/treekeepers_moonlight.png&#34; alt=&#34;The Treekeepers Puddle Jumper&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;Treekeepers VR is a networked VR game where up to 4 players can cooperate to navigate an oversized world and save a giant tree.&lt;/p&gt;&#xA;&lt;p&gt;Treekeepers is in production on both Quest (standalone VR) and Steam (PC VR) with full cross-play functionality. See the &lt;a href=&#34;https://togetheragainstudios.com/treekeepersvr/&#34;&gt;Treekeepers VR Website&lt;/a&gt; for links to all storefronts and more detail about the game.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;development&#34;&gt;Development&lt;/h2&gt;&#xA;&lt;p&gt;I began working on Treekeepers in June 2021, and my primary goal was to go significantly deeper into Unity and make a fully networked game. Very few co-op games existed in VR at the time (the area is still lacking), and my intention was to answer this need and create a game that 4 players could cooperate in within a static frame of reference (players move within a ship, and the ship moves through the world) while having to solve coordination challenges together.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Answering Machine</title>
      <link>http://localhost:1313/answering-machine/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/answering-machine/</guid>
      <description>&lt;p&gt;&lt;strong&gt;The Answering Machine&lt;/strong&gt; is a proof-of-concept system that uses natural language processing (NLP) to produce answers to questions asked about data in plain English.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href=&#34;http://voicequery-dev.s3-website-us-west-2.amazonaws.com/&#34;&gt;http://voicequery-dev.s3-website-us-west-2.amazonaws.com/&lt;/a&gt;&#xA;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/hockenmaier/voicequery&#34;&gt;https://github.com/hockenmaier/voicequery&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/answering_machine_uploads.png&#34; alt=&#34;Answering Machine homepage&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;It is designed with simplicity in mind — upload any columnar dataset and start asking questions. Advanced NLP algorithms interpret your queries and assumptions, providing answers. You can even correct these assumptions for better follow-up queries.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Upload any dataset&lt;/strong&gt;: Users can upload columnar datasets (CSV format) and start querying.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Real-time responses&lt;/strong&gt;: Questions are answered instantly, with NLP driving the interpretation and response generation.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Serverless architecture&lt;/strong&gt;: The system incurs no hosting costs apart from traffic-based expenses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;how-to&#34;&gt;How-to&lt;/h2&gt;&#xA;&lt;p&gt;On a desktop or tablet, click the link above to navigate to &lt;strong&gt;The Answering Machine&lt;/strong&gt;. Currently, it is not optimized for smartphones.&lt;/p&gt;</description>
    </item>
    <item>
      <title>First Ten</title>
      <link>http://localhost:1313/first-ten/</link>
      <pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/first-ten/</guid>
      <description>&lt;p&gt;&lt;strong&gt;First Ten&lt;/strong&gt; is an educational app that provides information about the U.S. Bill of Rights. It’s a voice-only experience, available through Google devices and smart speakers.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Try it here:&lt;/strong&gt; &lt;a href=&#34;https://assistant.google.com/services/a/uid/00000036f6a580ed&#34;&gt;https://assistant.google.com/services/a/uid/00000036f6a580ed&lt;/a&gt;&#xA;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href=&#34;https://github.com/hockenmaier/billofrights&#34;&gt;https://github.com/hockenmaier/billofrights&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://localhost:1313/images/first_ten_architecture.png&#34; alt=&#34;Serverless Architecture of the First Ten app&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;To try it, simply ask your Google Home or Android device, “Can I speak to First Ten?”&lt;/p&gt;&#xA;&lt;p&gt;Try it out on&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;development&#34;&gt;Development&lt;/h2&gt;&#xA;&lt;p&gt;First Ten is powered by a serverless backend built on AWS Lambda and DynamoDB. The voice input is processed by Google&amp;rsquo;s Dialogflow, which extracts user intents and parameters to guide interactions.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
