













    
        
    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/predictions/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/tags/predictions/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-06-09 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>http://localhost:1313/tags/predictions/</link>

        

        <title>Predictions · Tags · hockenworks</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have long been of the mind LLMs and their evolutions are truly thinking, and that they are on their way to solving all of the intellectual tasks that humans can solve today. It is just too uncanny that the technology that seems to have made the final jump to actually thinking, after a long string of attempts and architectures, is a type of neural net. It would be much easier to argue away transformer models as non-thinking stochastic parrots if we had happened to have had success with any other architecture than the one that was designed to mimic our own brains and the neurons firing off to one another within them. It&rsquo;s just too weird. They are shaped like us, they sound like us in a lot of ways, and it&rsquo;s obvious they are thinking something like us too.</p>
<h2 id="the-limitations">The Limitations</h2>
<p>That&rsquo;s not to say they are AGI in the modern definition. They can&rsquo;t do every task humans can do intellectually (IE without a body, which I will get to) for several reasons:</p>
<ol>
<li>
<p>Looping reasoning:
This was a huge problem for early transformers that had to output in one shot, and the examples were obvious. This one has been essentially solved via thinking models like o1. That was a huge unlock and a huge boon for applications like programming where there is lots of nested recursion of logic that has to occur to get a reasonable answer.</p>
</li>
<li>
<p>Memory and context:
Context windows get larger all the time but this one still is not solved. Just adding a bunch of tokens into a context window doesn&rsquo;t get you much when 2 million token models lose coherence after about the first 40,000 - which they do, and which every programmer working with anything but a tiny codebase intuitively understands. But this one too will largely be solved soon, if not through architectures that actually update their weights, it&rsquo;ll be solved through nuanced memory systems that people are actively developing on top of thinking models.</p>
</li>
<li>
<p><strong>Vision</strong>:
And this one might sound funny to someone that is paying attention to AI in particular, because GPT-4 with vision launched something like 2 years ago now. And it has been impressive for a long time, able to do things like identify what objects are in an image, where an image is from: things most humans can&rsquo;t do glancing at an image.</p>
<p>But the vision itself is not “good” vision. It cannot really pick out small important details, and it still behaves in many ways like vision recognition models have for years now. Now that we have a model that has both thinking and image input and editing at every step, the o3 and o4-mini series just released, we can really start to see the limitations in vision. Let me take you through 2 examples that represent the 2 types of failure modes that result from these not having true image understanding, yet.</p>
</li>
</ol>
<h2 id="proof-the-vision-is-not-there-yet">Proof the Vision is Not There Yet</h2>
<p>Each release from the major providers steadily knocks away my intelligence tests, which I admit are mostly programming oriented, but the ones that they can never really dent are the spatial reasoning ones - where a model really has to think about images in its head or use an image provided for detailed work.</p>
<h3 id="simple-3d-modeling">Simple 3D Modeling</h3>
<p>Every major model release, <a href="/3d-modeling-with-ai">I test what models can do with OpenSCAD</a>. I won’t get technical about it here, but OpenSCAD is a CAD program (Computer Aided Design - think 3D modeling for engineers, not the artistic kind) that is defined entirely through a programming language vs the typical mouse strokes and key presses that software like SolidWorks or AutoCAD depend on.</p>
<p>This makes OpenSCAD the perfect test platform for a model that inputs and outputs text primarily. I can describe a 3D model I want, and the model can output text that renders into a 3D model.</p>
<p>But for as amazing as LLMs are at scripting in normal programming languages, they have never been good at OpenSCAD. See my link above for GPT-3.5 and GPT-4 trying to model an incredibly simple object. That acorn was about as complex as GPT-4 could get without really falling down.</p>
<p>Here is OpenAI&rsquo;s o3’s attempt to make a standard 2x4 Lego Brick:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="an OpenSCAD render"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-lego.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">o3&#39;s model left, real Lego brick right</span>
    
</div>

<p>This was the easiest object I tested, and o3 does a decently good job. It grabbed the correct dimensions online and, using its inherent training data of what a 2x4 Lego block is, applied those dimensions into a mostly coherent object. It has one major flaw, which you can see on the underside as I have the image rotated - it drew two lines through two of the cylinders. My guess is that this is its interpretation of the supports in the middle of the actual Lego brick, that connect but don&rsquo;t run through the center cylinder.</p>
<p>Now for a harder test, a simple engineering part that&rsquo;s definitely not in its training data, because it is my own design, printed and sitting around in my 3D printer storage from long ago.</p>
<p>It&rsquo;s a bit of a weird part - a pinion with a smooth section and an 11-tooth gear of equal diameter, and a hole in the center with a slightly raised wall. This is the kind of part that an engineer well versed in AutoCAD or SolidWorks can produce in just a few minutes, but which requires attention to detail and a model of how parts fit together.</p>
<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="an OpenSCAD render"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-pinion.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">o3&#39;s model left, real part right</span>
    
</div>

<p>This is where you can see how these models fall apart. o3 immediately gets that it&rsquo;s a pinion, that it has a hole in the middle, and that it has a smooth section on the bottom and a gear on top. But the execution is nowhere close to workable, from most major to least (in my opinion):</p>
<ul>
<li>There are two gears (unknown as to why or if this is intentional, o3 explained one as a &ldquo;grooved ring&rdquo; - whatever that means)</li>
<li>The gear teeth are concave - whereas the rounded sharp tooth shape is clear in the image</li>
<li>There are 10 teeth, not eleven - which seems trivial, but it&rsquo;s indicative of a real flaw that messes up all complex models I throw at AI - where LLMs make an assumption like what number of teeth is &ldquo;likely,&rdquo; rather than looking at the image in detail and counting them.</li>
<li>There is clearly an attempt at the raised wall around the top hole, but it&rsquo;s far too big.</li>
<li>The height of the smooth base section and gear sections are equal in the real part, but o3 makes the gear more than 3x thicker than the base.</li>
</ul>
<h3 id="map-reading">Map Reading</h3>
<p>Here&rsquo;s another great example of what I mean when I say that models have bad vision.</p>
<p>I recently gave this question to the latest thinking image model, o3: &ldquo;Here&rsquo;s an image from Google Maps of the block I live on between the avenues of Burbank, Hazeltine, Oxnard, and Van Nuys. What is the longest continuous loop I can walk within the neighborhood without crossing my path or touching one of the avenues? This square is 1/2 mi on each side&rdquo;</p>
<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="the uploaded map"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/o3-struggle-map.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">The image uploaded with this query</span>
    
</div>

<p>O3 thinks for 4 minutes about this question, zooming in to various parts of the map countless times to form the route. And then it fails on the first step, suggesting starting at Tiara and Stansbury, which do not intersect on the map. Any person looking at this image could tell that is true in just a few seconds.</p>
<p>This is what I mean when I say these things have bad vision - and this is the best model from the lab I think has the best vision. Vision is not about being able to identify millions of different objects, <a href="https://www.image-net.org/">ImageNet-style</a>. It&rsquo;s about seeing the detail and paying attention to the right thing. Here in this map, that means looking roughly at the lines representing Stansbury and Tiara, looking at where they would intersect, and seeing they do not.</p>
<p> </p>
<p>Any UX developer that has worked with AI knows what I&rsquo;m saying intuitively. There is a difference between generating some Tailwind code that spits out a standard UI and getting to the level of complexity that the AI starts to need to look at screenshots in detail and know the relative position and orientation of components, or see small details. They just.. don&rsquo;t do that yet.</p>
<h2 id="the-humanoids">The Humanoids</h2>
<p>What I think is going on in these examples is that we have a limitation in training data (duh) but it isn&rsquo;t because there aren&rsquo;t a lot of images and videos on the internet, it&rsquo;s because there is so much more information in the average image than there is in the average chunk of text, and a lot more of that information is irrelevant to any given question.</p>
<p>When I say that we have a limitation on training data, I&rsquo;m not in the typical camp of &ldquo;well, then transformer neural nets are obviously stupid because I was able to understand this thing without training on terabytes of data from the internet&rdquo;. This has always been a bad take because the average human trains on petabytes, not terabytes of data, and that data is streamed into their brains mostly in the form of images. I am also not in the camp of thinking that this means that the data &ldquo;just doesn&rsquo;t exist&rdquo; to get these models to AGI in this dimension. It so clearly does exist, and it exists so abundantly that a unique image stream can be sent to each of the billions of human brains, and they all learn the same principles that let them immediately identify the mistake that the cutting-edge thinking vision model made after 4 minutes of rigor.</p>
<p>The data exists, and we never actually had a data problem in AI. We have an instruction problem. That doesn&rsquo;t mean model architecture or data massaging really, it means that we need to plug our models into the real world where all the data streams exist. I&rsquo;m guessing that this comes in the form of robots with cameras on them, the first of which is happening en masse via Tesla full self-driving, and I&rsquo;m sure those vision neural nets are quite insane compared to what we see in the consumer transformer models, but the real leap probably comes when we get to humanoids walking around collecting and learning from vision data every day - and learning from actions they take in the real world.</p>
<p>If you’ve ever tried to get concrete actions to take based on a vision-transformer’s outputs, you will know it’s hard. I am very impressed that the big labs are starting to crack <a href="https://docs.anthropic.com/en/docs/agents-and-tools/computer-use">computer use</a> - because getting an LLM to give specific coordinates or elements to click on is the same type of challenge I was testing above. But it&rsquo;s no wonder these models are still very inaccurate.</p>
<p>Letting transformer-based agents control Robots will be a much harder problem of a similar type. Not only does it require attention to detail, but now the images are in 3D, they come at you many times per second, and actions need to be produced as quickly. But my prediction is that we will brute force this, and it will work &ldquo;well enough&rdquo; for enthusiasts and niche industrial use cases to benefit from humanoid robots. And that&rsquo;s the takeoff point for true vision, as we all intuitively understand it. Releasing humanoids at scale (and cars, to some extent) are when we really unleash the datastream that&rsquo;s needed to get models that can see as well as you or I can. This is probably one reason why many AI companies and labs are pushing them so hard - they also understand the data they collect will be more valuable than the money paid for the first units.</p>
<p>Once we have a few hundred thousand humanoids roaming around early adopters&rsquo; houses, we will start to see AI models that can use OpenSCAD and Google Maps.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-06-09:/vision-is-the-last-hurdle-before-agi/</guid>

                
                    <link>http://localhost:1313/vision-is-the-last-hurdle-before-agi/</link>
                

                
                    <pubDate>Mon, 09 Jun 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Vision is the last hurdle before AGI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Recently I read the <a href="https://ai-2027.com/scenario.pdf">AI 2027 paper</a>. I was surprised to see Scott Alexander&rsquo;s name on this paper and I was doubly surprised to see him do his <a href="https://www.dwarkesh.com/p/scott-daniel">first face reveal podcast about it with Dwarkesh</a></p>
<p>On its face this is one of the most aggressive predictions for when we will have AGI (at least the new definition of AGI which is something that is comparable or better than humans at all non-bodily tasks) that I have read. Even as someone who has been a long believer in <a href="https://en.wikipedia.org/wiki/The_Singularity_Is_Near">Ray Kurzweil&rsquo;s Singularity predictions</a>, 2027 strikes me as very early. I realize that Kurzweil&rsquo;s AGI date was also late 2020&rsquo;s and 2045 was his singulartiy prediction - 2027 still feels early to me.</p>
<div class="paige-row-wide">
  <div style="display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:15px;">
      






























    













    



    






















<img  alt="AI 2027"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-2027.png"    >


      






























    













    



    






















<img  alt="Singularity Is Near"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/The-Singularity-Is-Near.jpg"    >

</div>
</div>

<p>I won&rsquo;t get into my full take on AI 2027 here, but the core argument comes down to the same one I was making in my <a href="/on-ai-software-development">original post on AI software development</a></p>
<ul>
<li>which is that, once AI agents are able to replace software engineers, instead of just assisting them, it doesn&rsquo;t matter how they are doing another realms, because they will simply be able to improve on themselves and their own software at such a rate that the difference between the time of automating software engineering jobs and the time of automating all other jobs is negligible.</li>
</ul>
<p>So I figured it was a good time to update on where I think AI is actually at at software engineering tasks. I&rsquo;ve had the chance to test many of the latest AI software development tools and models, and we have come a long way since my original post.</p>
<p>I have been doing a lot of development with AI for the last few years, especially the last couple of months on leave building the game in this site. And they are good. But the core problems with these models for software engineering are:</p>
<ol>
<li>They can&rsquo;t deal well with contexts over 30K tokens or so (even the best models with supposed millions of token windows).</li>
</ol>
<p>This means the actual developer (me and you) are the ones picking specific files and functions to send into the context window, lest we confuse the model. This is arduous unless the codebase is small enough to fit entirely into the context window. That&rsquo;s exactly what I think is going on with most of the new &ldquo;vibe coded&rdquo; projects we see showing impressive results - these are just tiny POC apps that haven&rsquo;t hit more than a few thousand lines of code yet. For context, most serious enterprise apps containing the detailed logic and edge cases real use cases require are in the millions or hundreds of millions of lines of code.</p>
<ol start="2">
<li>They are biased to be &ldquo;advisors&rdquo; as well as &ldquo;doers&rdquo;</li>
</ol>
<p>This is just annoying, and I hope it gets trained out soon. Models just really <em>want</em> you to be doing everything, and to act themselves as an advisor. It makes sense with one of the main sources of code training data being from Stackoverflow and other blogs, where developers can never seem to rid themselves of a pseudo-condescending &ldquo;you should have been able to read the docs and learn this yourself&rdquo; tone. It&rsquo;s also just a pattern exhibited by people in general - more often than not, especially in the corporate world, people are trained to be the &ldquo;coaches&rdquo; rather than the &ldquo;worker bees&rdquo;. One reason why things get done so slowly in big political companies sometimes.</p>
<p>&mdash;Notes&mdash;</p>
<p>Add links to my chat with 03 about the refund feature,</p>
<p>Add links to videos about co-pilot agents as well as codex as the state of the art actual software developers</p>
<p>Talk a little bit about my experience using cursor and what a disappointment it was for a code base as unique as mine</p>
<p>Add notes about cursor not handling context any more easily than context caddy - and that might be a bit of a stumbling block</p>
<p>First feature that sonnet 3.7 consistently failed at was simply having the compactors add ball value accumulation of compacted balls</p>
<p>Your request has been blocked as our system has detected suspicious activity from your account.If you believe this is a mistake, please contact us at <a href="mailto:hi@cursor.com">hi@cursor.com</a>.(Request ID: 997e94bb-34ee-44c0-b0aa-15bb6822add6)</p>
<p>LOL</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-05-24:/on-ai-software-development-2/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development-2/</link>
                

                
                    <pubDate>Sat, 24 May 2025 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development, 2025 Edition</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>During my parental leave, I played through quite a few video games - something I love and one of the easiest ways to spend time while rocking my daughter to sleep.</p>
<p>They include:</p>
<ul>
<li>The Legend of Zelda: The Minish Cap <sup>Steam Deck</sup></li>
<li>Carto <sup>Steam on Windows</sup></li>
<li>Tunic <sup>Steam on Windows</sup></li>
<li>Abzu <sup>Steam on Windows</sup></li>
<li>SteamWorld: Build <sup>Steam on Windows</sup></li>
<li>World of Goo 2 <sup>Nintendo Switch</sup></li>
<li>Oblivion Remastered <sup>Steam on Windows</sup></li>
<li>Mario Kart World <sup>Nintendo Switch 2</sup></li>
<li>Donkey Kong Bananza <sup>Nintendo Switch 2</sup></li>
</ul>
<p>Most of these are indie titles, but the one I spent the most time on was the Oblivion Remaster - one which surprised me both with how good it looks and with how well it played. Oblivion is a 19 year old game, and a purely graphics-related overhaul should not have made it as good or better than modern AAA games releasing today, but in my opinion (and many others I’m reading) it absolutely did. How could this be?</p>
<h2 id="graphics">Graphics</h2>
<p>Well - I think that’s pretty clear. The only thing that has really improved about mainstream gaming in the last 20 years is graphics.</p>
<p>And boy have the graphics improved. Oblivion not only uses new techniques like ray-tracing and revamped 4K textures and normal maps etc, it uses the full suite of global illumination provided by <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/lumen-global-illumination-and-reflections-in-unreal-engine">Unreal Engine</a>, which means when you turn the settings up, it depends on hardly any of the typical tricks games need to use, like baked light maps, instead lighting almost everything in the game dynamically or “procedurally”, including things like reflections of reflections and lit up dust and fog.</p>

<div style="text-align: center; margin-bottom: 1rem;">
  <video controls loop muted autoplay style="max-width:100%;">
    <source src="/videos/bruma-clip.mp4" type="video/mp4">
    Your browser doesn’t support HTML5 video.
  </video>
  
    <span style="display: block; font-style: italic; margin-top: 0.5rem;">This is from my playthrough, some outdoor torches in Bruma and mountain lighting in the background make for a pretty good use of full Lumen</span>
  
</div>

<p>Yes, Oblivion still has some simplistic design in terms of how landscapes are laid out, but that simplicity might also be why people can run it with Nvidia Lumen set up to run at ultra. Lighting is what’s really differentiating in video game graphics now, and fully simulated lighting beats or meets nearly every AAA game releasing lately that all cost tens or hundreds of millions of dollars to make.</p>
<h2 id="not-graphics">Not Graphics</h2>
<p>Left unstated is why nothing else has improved. Walking around Oblivion, meeting characters that have some scripted voice lines, responding to them with 1 of 4 options, holding down the left stick to sprint, and hitting a single button on your controller to watch your character animate a full scripted sword swing are all the standard gameplay of action-RPGs both 19 years ago and today. The same can be said of other genres - mechanics are largely untouched for the last 20 years outside of some common quality of life changes in how menus and inventory and HUDs work.</p>
<p>Here’s my theory for why: The games industry achieved the threshold of what was comfortable and possible with the current human-computer input and output mechanisms only a few years after they were technically possible. And those inputs and outputs have not changed much since the 90s. Looking at a flat screen in front of you and pressing buttons on a controller or keyboard/mouse are incredibly limiting for gaming. Increasing the amount of pixels on the screen and making that controller wireless don&rsquo;t change the core gameplay. This is not really true for any other media than games. Reading, watching films, and all kinds of media in between essentially max out on one screen and limited input, but games immediately ran into forms of input and output as a barrier.</p>
<p>I wrote a little about this topic in <a href="/social-media-is-antisocial">my first post about VR and why I think it’s the future</a>. The only true innovation in gameplay that is happening in two places:</p>
<ul>
<li>Indie, where one-man teams can come up with strange mashups and mechanics can execute on a vision spending very little. These are almost never totally genre defining, but they are inventive.</li>
<li>VR, where we have only scratched the surface of mechanics that work and what is possible when the player’s entire hands and 3D field of view are in the game.</li>
</ul>
<p>AAA game developers are excluded from the first by definition and are excluded from the second by their own financial decision makers.</p>
<h2 id="so-what-my-predictions">So What? My predictions</h2>
<p>Maybe people want to keep buying the same games with better graphics forever. I don’t think they do. I think those decision makers that aren’t investing in truly new AAA gameplay are short sighted. As long as we’ve been hearing that VR is the future and not seeing it totally come to fruition, at some point these companies that are milking the same game franchises for years will face the reality that people will only buy the same games reskinned with better graphics for so long. They’ll either be replaced by companies that innovate or individuals using AI that will be plenty good at recreating the same game over and over again.</p>
<p>I am hoping that the current hypestorm around AI re-kindles the ideas in the hearts of AAA game studio CFOs that they might need to invest in innovation and new ideas again. Some of the things that make Oblivion just like any RPG of today (Think about the four-option scripted discourse and stuffy voice lines as two) would be meaningfully different if AI was applied in the right way. And I think they’d be far more powerful experiences, just like I think VR games can be - so much higher fidelity and responsive to player input.</p>
<p>Here are my predictions of things we’ll see in the next decade or so, regardless of the AAA studios pioneer them or not:</p>
<p>A decade or two from now, we will look at the period of 2005 to 2025 and see that it was a period of a great stagnation in game innovation. This will be driven by a few key technological advances and the downstream game mechanics that will flow from those, stemming mainly from the areas of AI and what is now called mixed reality.</p>
<p>I’m going to make a few specific predictions of mechanics we will see in the future that will make the stagnation of the last 20 years totally transparent:</p>
<h4 id="ai">AI:</h4>
<ul>
<li>
<p>In open world games, it will become standard for non-player characters to have fully generated dialogues based on motivations and incentives rather than scripts</p>
</li>
<li>
<p>In open world games, players will speak directly or engage directly in some way with their own words that NPCs will understand and react to intelligently</p>
</li>
<li>
<p>NPCs won&rsquo;t have recorded voice lines, but will instead have voices generated in real time, and AAA games will start to make contracts with celebrity talent in order to generate their voice in games</p>
</li>
<li>
<p>We will see a transition from the current minorly procedural elements of gameplay to full procedural generated worlds, especially where player actions and environmental events change landscape, buildings, etc dynamically</p>
</li>
<li>
<p>Game art will go through an incredible revolution, and we will stop having massive teams of people creating 3D meshes and textures to drop around the world, with many of these meshes and textures being generated from prompts in development, but also being generated on the fly in games</p>
</li>
</ul>
<h4 id="mixed-reality">Mixed Reality:</h4>
<p>The first reasonable augmented reality glasses that consumers are willing to wear en masse will generate entirely new genres:</p>
<ul>
<li>
<p>A new genre of video board games, where gameplay happens in view of a group of people in the same physical space, on a table, on the ground, in a park, etc, that will incorporate video game elements such as computations that are too complicated for typical board games, with the advantages that board games have today where players can interact with the same physical pieces, point and gesture, and socialize in person</p>
</li>
<li>
<p>Another new genre of exercise programs combined with games. This will go far beyond current treadmills that have built-in virtual run routes or virtual exercise classes, this will be exercise incorporated as a leveling or other mechanic that will incentivize players to get some exercise. I&rsquo;ve had a lot of hope for this category for a long time, and the one major threat to it is GLP-1 drugs that may cause a serious decline in the need and desire for people to exercise daily for calorie loss</p>
</li>
</ul>
<p>When will these happen? My specific prediction is that the next 3 years will be seen as the end of the period of stagnation we are in right now, a period that will be much more apparent looking backwards than it is from within. The combo of a decline in AAA game spending and AI hype feel like ripe conditions for innovation to return to gameplay - and my hope is that all of the other potential gameplay innovations ride the same AI wave.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-05-12:/the-purgatory-of-aaa-gaming/</guid>

                
                    <link>http://localhost:1313/the-purgatory-of-aaa-gaming/</link>
                

                
                    <pubDate>Mon, 12 May 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Purgatory of AAA Gaming</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="this is a robot"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-software-dev.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>Lots of chatter right now about AI replacing software developers.</p>
<p>I agree - AI will take over software development. The question is: what work will be left when this happens?</p>
<p>Some considerations:</p>
<ul>
<li>Benchmarks for the best LLMs still put them solidly in the &ldquo;bad at programming&rdquo; category, scoring in the 5th percentile of human programmers on common tests. Meanwhile, LLMs score in the 80th-95th percentile for law exams and 85th–100th for psychology, statistics, and many other less technical fields. More scores available in the &ldquo;simulated exams&rdquo; section of <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>.</li>
<li>Engineers have been using language models like tabnine and copilot as &ldquo;super-stackoverflow&rdquo; style code assistance years before chatGPT released. This means much of the velocity increase we might expect from current LLMs&rsquo; ability to write code has already been &ldquo;priced in&rdquo; to the market.</li>
<li>Many of the trends making software development more costly are growing, not shrinking: Systems are becoming more distributed. The cloud lowered infrastructure costs but made applications more complex. We&rsquo;re making more and deeper integrations among disparate systems. Auth is becoming more secure and thus complex (managed identity, MFA, etc).</li>
</ul>
<p>Github copilot chat and other LLM dev tools are speeding up the rote stuff. I’ve seen it in my own work.</p>
<p>And I really do believe new AI models will do more than just the basics, maybe in the next couple of years. Even precluding &ldquo;AGI&rdquo;, the trend we are on is that more and more work is automatable, and engineers, especially more junior ones - are going to have to shift focus away from algorithmic work that AI can do.</p>
<p>But by the time our neural nets are &ldquo;good enough&rdquo; at building software to make it significantly cheaper to build, I doubt this trend will make the news. Everything else gets automated too.</p>
<p>These are my thoughts at what seems to be the beginning of the next AI revolution in early 2024. I plan to revisit this topic and see if I&rsquo;m right in future posts.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-01-24:/on-ai-software-development/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development/</link>
                

                
                    <pubDate>Wed, 24 Jan 2024 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<blockquote>
<p>Editor&rsquo;s note from 2025:</p>
<p>This article was written as part of the launch of <a href="/treekeepers-vr">Treekeepers VR</a> and the sole proprietorship Together Again Studios, and represents some of my core beliefs of the value of VR and where it&rsquo;s taking us socially. Though I&rsquo;m no longer actively working on Treekeepers, I do hold that VR and AR are truly the &ldquo;endgame&rdquo; of interface and one that could save us from some of the social attitudes caused by social media of today. Enjoy!</p>
</blockquote>
<div style="text-align: center; margin-bottom: 1rem;">
    




























    



    



    





    



    





    



    






















<img  alt="proofreader GPT"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/together-again-studios.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
</div>

<p>With Together Again Studios and Treekeepers VR, we&rsquo;re setting out to solve an insidious problem we see all around us:</p>
<p><strong>Social Media Is Anti-Social</strong></p>
<p>Though Facebook, Instagram, Twitter, and Tiktok all let us share more with each other than ever before, what we are sharing is surprisingly hostile and dismissive of opinions other than our own.</p>
<p>Though Zoom, Hangouts and Teams let us finally see each other from a distance, we still can&rsquo;t speak naturally. We depend on tools like &ldquo;mute&rdquo;. We create meeting upon meeting with different sets of the same group of people. And we don&rsquo;t form the depth of relationships we could in-person.</p>
<p>We as humans are all-too-capable of forming us-versus-them &ldquo;tribes&rdquo; and dehumanizing those who appear too different, and this problem is becoming ever more apparent behind the curtain of the graphical user interface.</p>
<p><strong>Virtual and augmented reality are a way out</strong></p>
<p>In 2016, thanks to pioneers like Palmer Luckey, Michael Abrash, and John Carmack, we suddenly gained access to a technology that removes the curtain and forces us to see eachother. And in 2020, an event that has permanently limited our in-person interaction arose and gave new meaning to this technology.</p>
<p>In VR/AR, voices are no longer text on a screen, taken out of context by our social media bubbles. They&rsquo;re voices again.</p>
<p>In VR/AR, people are no longer user profiles with one image and a tag-line. They&rsquo;re really people, with bodies, faces, and hands that can point and gesture.</p>
<p>In VR/AR, messages are not just &ldquo;public&rdquo;, or &ldquo;direct&rdquo;. Conversations are dynamic, with people physically approaching one another to talk, with people moving in and out of physical groups, and with people attending public conversations together again while still able to have &ldquo;sidebar&rdquo; conversations.</p>
<p>All these abilities we used to have in-person, we have gained again in virtual reality.</p>
<p>Soon, we&rsquo;ll go even further with this technology. We&rsquo;ll be able to make real eye contact with eachother in VR. We&rsquo;ll use AR to invite distant friends and family over to our homes.</p>
<p>And at Together Again, we plan on using these new tools to let people like eachother again.</p>
<p><strong>Treekeepers: Only Possible in VR</strong></p>
<p>Why is Treekeepers a VR Game?</p>
<p><strong>Multiplayer of this depth only works in VR</strong></p>
<p>The challenges in Treekeepers VR hinge on player coordination and quick group decisions - Which weapon should we upgrade? Who&rsquo;s doing which job? Where are we going?</p>
<p>In VR, you gain the ability to gesture and point to enemies and obstacles naturally.</p>
<p>No more &ldquo;Who is the green player?&rdquo; - spend zero mental energy figuring out who you are talking to. Just turn towards them and speak.</p>
<p><strong>Scale is the most fun when you&rsquo;re in the world</strong></p>
<p>We&rsquo;re going to be exploring oversized objects around a gigantic tree. Only VR can get the full benefit of this experience.</p>
<p><strong>VR can be uncomfortable</strong></p>
<p>BUT, experiencing it via a static vehicle which acts as a persistent frame of reference reduces motion sickness.</p>
<p>No need to rotate - gameplay is based on the hot air balloon always facing one direction, and players navigating within it.</p>
<hr>
<p>Check out Treekeepers VR <a href="/treekeepers-vr">here</a></p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2022-08-01:/social-media-is-antisocial/</guid>

                
                    <link>http://localhost:1313/social-media-is-antisocial/</link>
                

                
                    <pubDate>Mon, 01 Aug 2022 00:00:00 UTC</pubDate>
                

                
                    <title>Social Media Is Anti-Social</title>
                
            </item>
        
    </channel>
</rss>
