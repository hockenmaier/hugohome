













    
        
    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/rapid-prototyping/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/tags/rapid-prototyping/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-08-11 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>http://localhost:1313/tags/rapid-prototyping/</link>

        

        <title>Rapid Prototyping · Tags · hockenworks</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Two years and four months have gone by since I ran my original Interactive Solar System experiment with the original version of GPT-4. You can see that one at hockenworks.com/gpt-4-solar-system.</p>
<p>I have a far more impressive result today, which uses ChatGPT Agent Mode. Take a test run:</p>

<div style="width:100%; max-width:1000px; margin:1em auto; position:relative; padding-top:70%;">
  <iframe
    src="/html/solar-system-self-contained.html"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">Works on desktop or mobile</span>
    
</div>

<p><strong>Controls</strong></p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>I again used this initial prompt:</p>
<blockquote>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.</p>
</blockquote>
<p>Quite an improvement, eh?</p>
<p>Like my original experiment, I didn’t one-shot this result. This is the final result after 7 prompts, but unlike in the original, those prompts were not primarily fixes - they were primarily new features that I thought would be good additions after playing with the previous version.</p>
<p>The most striking part of this experiment for me is that, despite the plateau that has largely been observed in base LLMs, this result is clearly far above and beyond what GPT-4 was able to do when it first released, and the reasons for that have to do with what the labs have been building <em>around</em> the base models.</p>
<p>In the case of Agent mode, as far as we know, we have a GPT-4 class model at the root. o3, which is what Agent Mode uses, is a GPT-4 class model with chain of thought and lots of self-play training, as far as we know.</p>
<p>It also has the ability to use tools, like the web search tool it used to find the planet facts.</p>
<p>But then the thing that most sets it apart from its base model is the deep research and computer use elements of Agent mode. As it was building this model, I saw Agent mode do things such as perusing the internet with its text browser to find scale information and browse for textures it could use as planet skins. Then, the most important new ability that allowed it to make this: It ran this code on a local browser and <em>visually tested it</em>, clicking on the planets to make sure the information panels were coming up, zooming and panning around, and using its own toolbars.</p>
<p>This to me is the start of a true general capability for AI software testing. Not just little unit and automated tests - but actual visual look and feel testing. This is a huge market and will be a huge shock to the software industry when it’s fully realized.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-08-11:/agent-mode/</guid>

                
                    <link>http://localhost:1313/agent-mode/</link>
                

                
                    <pubDate>Mon, 11 Aug 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Agent Mode is the Revelation, not GPT-5</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I built a nice little tool to help AI write code for you.</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/R5wztMBfh0w?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>Well, really, o3-mini and o3-mini-high worked together to write this and I corrected a few things here and there. I started using this tool to write itself about 30 mins into development!</p>
<p>Download on github (above) or the VScode marketplace:</p>
<p><a href="https://marketplace.visualstudio.com/items?itemName=Hockenmaier.context-caddy">https://marketplace.visualstudio.com/items?itemName=Hockenmaier.context-caddy</a></p>
<hr>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-02-13:/context-caddy/</guid>

                
                    <link>http://localhost:1313/context-caddy/</link>
                

                
                    <pubDate>Thu, 13 Feb 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Context Caddy</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I&rsquo;m writing this post retrospectively as I never published it at the time of creation. It will live here as a &ldquo;stake in the ground&rdquo; of AI software capabilities as of March 2023. Note- if you&rsquo;re reading on substack, this post won&rsquo;t work. Go to <a href="hockenworks.com/gpt-4-solar-system/">hockenworks.com/gpt-4-solar-system</a>.</p>
<p>The interactive solar system below was created with minimal help from me, by the very first version of GPT-4, before even function calling was a feature. It was the first of an ongoing series of experiments to see what frontier models could do by themselves - and I&rsquo;m posting it here because it was the earliest example I saved.</p>
<p>Here&rsquo;s a link to the chat where it was created, though it&rsquo;s not possible to continue this conversation directly since the model involved has long since been deprecated: <a href="https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387">https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387</a></p>

<div style="width:100%; max-width:1000px; margin:1em auto; position:relative; padding-top:70%;">
  <iframe
    src="/html/solar-system-self-contained.html"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">GPT-4 only wrote this for desktop, sorry phone users</span>
    
</div>

<p><strong>Controls</strong></p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>If you get lost, reload the page. That&rsquo;s an edge case GPT-4 didn&rsquo;t account for :)</p>
<p>Here was the initial prompt:</p>
<blockquote>
<p>This might be a long output, so if you need to break and I&rsquo;ll ask you to continue in another message feel free to do that. But please limit any non-code text prose to only essential statements to help mitigate this</p>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.</p>
</blockquote>
<p>Then I had to give it 4 more short prompts, one for a technical hint (to use html-5 since it was going a strange direction) and 3 for visual and mouse control misses.</p>
<p>It works - but missed some of the relatively simple directions, like the planet stats and rendering/controls for mobile. Still, I think it&rsquo;s cool to see the true scale of the planets on a zoomable canvas. And, it only goes to Neptune, the last true planet&hellip; don&rsquo;t go looking for Pluto.</p>
<p>For March 2023, this result was revolutionary - I was truly impressed. In 2025, it&rsquo;s not very impressive at all. How quickly we get used to progress!</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2023-03-18:/gpt-4-solar-system/</guid>

                
                    <link>http://localhost:1313/gpt-4-solar-system/</link>
                

                
                    <pubDate>Sat, 18 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>GPT-4 Solar System</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>The Answering Machine is a proof-of-concept system that I built using <strong>pre-LLM</strong> natural language processing (NLP), specifically NLTK, to produce answers to questions asked about data in plain English.</p>
<p>Looking back, this project was a great insight into what LLMs immediately allowed that was incredibly difficult before. This project was several months of work that the openAI sdk would probably have allowed in a few weeks - and that few weeks would have been mostly frontend design and a bit of prompting.</p>
<p><strong>Try it here:</strong> <a href="http://voicequery-dev.s3-website-us-west-2.amazonaws.com/">http://voicequery-dev.s3-website-us-west-2.amazonaws.com/</a>
<strong>Github:</strong> <a href="https://github.com/hockenmaier/voicequery">https://github.com/hockenmaier/voicequery</a></p>
<p>The system uses natural language processing to produce answers to questions asked about data in plain English.</p>
<p>It is designed with simplicity in mind—upload any columnar dataset and start asking questions and getting answers. It uses advanced NLP algorithms to make assumptions about what data you&rsquo;re asking about and lets you correct those assumptions for follow-up questions if they&rsquo;re wrong.</p>
<p>It is built entirely out of serverless components, which means there is no cost to maintain or run it other than the traffic the system receives.</p>
<h2 id="how-to">How-to</h2>
<p>On a desktop or tablet, click the link in the header to navigate to the Answering Machine. For now, it isn&rsquo;t optimized for smartphone-sized screens.</p>
<p>In order to use the Answering Machine, you can either select one of the existing datasets, such as &ldquo;HR Activity Sample,&rdquo; or upload one of your own using the homepage of the site:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine homepage"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_uploads.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>To upload your own, click in the upload area or just drag a file straight from your desktop. For now, use CSV data files. Excel and other spreadsheet programs can easily save data in the CSV format using the &ldquo;File &gt; Save As&rdquo; or similar option in the menu. Each file needs a unique name.</p>
<p>When you hit the upload button, the site may not appear to change until the file is uploaded, at which point you&rsquo;ll see it appear in the box labeled &ldquo;Ask Your Data Anything&rdquo; below. Click on your file to start using it with the Answering Machine, or click the red trash can icon to delete it.</p>
<p>There are no user accounts in this system yet, so the data you upload might be seen by other users using the system. Try not to use sensitive data for now.</p>
<h3 id="asking-questions">Asking questions</h3>
<p>When you enter a dataset, you&rsquo;ll see a view that presents you with quite a bit of information:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    







    





    



    























<img  alt="Answering Machine main view"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_hr.png"   style="height: auto; width: 100%"   >


    
</div>

<p>The only part you need to focus on right now is the information panel. This panel lists out all the fields (columns), data types of those fields, and some sample data from a few records in your dataset:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine info panel"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_info.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>You can use this panel to start to formulate questions you might have about the data. If you see number values, you might ask about averages, maximums, or other math that might otherwise take some time to calculate. If you see a date, you can ask questions about the data in certain time periods.</p>
<p>Many datasets also contain fields that only have a few specific allowed values. When the Answering Machine sees fewer than 15 unique values in any field, the data type will be a &ldquo;List&rdquo; and it lists them right out under the sample values table. You can use this type of value to ask questions about records containing those specific values. For example, in the HR dataset, you might only be interested in data where the &ldquo;Education&rdquo; field&rsquo;s value is &ldquo;High School.&rdquo;</p>
<p>Now look to the query bar to start asking your data questions:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine query bar"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_query.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The types of questions that will currently be automatically detected and answered are:</p>
<ul>
<li>Counts of records where certain conditions are true</li>
<li>Math questions such as averages, medians, maximums, and minimums</li>
</ul>
<p>These types of questions can be made specific by using qualifying statements with prepositional phrases like &ldquo;in 2019&rdquo; or adjective phrases like &ldquo;male&rdquo; or &ldquo;entry-level.&rdquo;</p>
<p>Combining these two ideas, you can ask specific questions with any number of qualifiers, such as:<br>
<em>&ldquo;What was the median salary of male employees in the engineering department 5 years ago?&rdquo;</em></p>
<p>Upon hitting the &ldquo;Ask&rdquo; button (or hitting Enter), the Answering Machine will do its best to answer your question and will show you all of its work in this format:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine response"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_answer.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The last line in the response is the Answering Machine&rsquo;s answer. In this case, it is telling you the metric you asked for with all your stipulations is <strong>6871.6 dollars.</strong></p>
<p>Moving up, you see a series of assessments that the Answering Machine has made in order to filter and identify the data you are asking about. Statements like &ldquo;Best auto-detected Numeric Subject Found: salary with column: Compensation (Monthly)&rdquo; provide a glimpse into one of the Answering Machine&rsquo;s most advanced features, which uses a selection of NLP techniques to compare words and phrases that are similar in meaning, ultimately matching things you are asking about to fields and values that actually exist in your database.</p>
<p>At the very top of the response is how the Answering Machine&rsquo;s nested grammar parsing logic actually parsed your question, with some specific pieces color-coded:</p>
<ul>
<li><strong>Green</strong> chunks indicate &ldquo;subjects&rdquo; that were detected. Subjects are what the Answering Machine thinks you&rsquo;re asking &ldquo;about.&rdquo; These should represent both the main subject and other supporting subjects in your question.</li>
<li><strong>Purple</strong> chunks are conditions. These are the things that the Answering Machine thinks you are trying to use to &ldquo;specify&rdquo; or filter data.</li>
</ul>
<p>Now that your question is answered, you might notice that some new green and purple colored bubbles have appeared in the sections of your screen labeled &ldquo;New Subjects&rdquo; and &ldquo;New Conditions.&rdquo; We&rsquo;ll call these &ldquo;lexicon&rdquo;:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine subjects and conditions"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_lexicon.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
</div>

<h3 id="forming-concepts">Forming Concepts</h3>
<p>If the Answering Machine already understood what you were asking and successfully matched it to fields and values in your data, you don&rsquo;t have to do anything with these. But often you will be using domain-specific lexicon, or the auto-matching algorithm simply won&rsquo;t pick the correct value. These situations are what concepts are for.</p>
<p>To create a concept, click and drag on the green or purple &ldquo;lexicon&rdquo; bubble and move it out into the blank middle area of the screen. Then click and drag the field or field value from the info-panel at the top of the screen and drop it right on top of that bubble. You&rsquo;ll see both the data bubble and the lexicon bubble included in a larger gray bubble, which represents the concept:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine concept"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_concept.png"   style="height: auto; max-width: 200px; width: 100%"   >


    
</div>

<p>You can add more lexicon bubbles to this concept if they mean the same thing, but you can only use one data bubble.</p>
<p>Concepts override the Answering Machine&rsquo;s auto-matching logic. If you ask another question containing a subject or condition that is now matched by a user to a data value, that data value will be used instead of the auto-match. If the concept isn&rsquo;t working well, you can delete it by dragging all of the nested bubbles out of it either into the blank middle area or into the colored panel they originally came from.</p>
<p>Feel free to play around with new datasets and questions, and use the contact section of this site if you have comments or questions. When you ask questions or create/modify concepts, that data will automatically be saved to the server in real-time. You can close the page anytime and come back to your dataset to keep asking questions.</p>
<p>Remember that there are no user accounts, meaning you can share your dataset and work in tandem with others! But again, please do not upload sensitive data to this proof-of-concept tool as it will be available for other users to see and query.</p>
<h2 id="architecture">Architecture</h2>
<p>The Answering Machine is a purely &ldquo;serverless&rdquo; application, meaning that there is no server hosting the various components of the application until those components are needed by a user. This is true for the database, the file storage, the static website, the backend compute, and the API orchestration.</p>
<p>For the cloud nerds out there, <a href="https://martinfowler.com/articles/serverless.html">here is a great article</a> by Martin Fowler on what exactly &ldquo;serverless&rdquo; means, especially in terms of the backend compute portion, which is arguably the most valuable part of the application to be serverless. For reference, I am using Martin&rsquo;s 2nd definition of &ldquo;serverless&rdquo; here.</p>
<p>This is a high-level map of all of the components that make the Answering Machine work in its current (June 2020) state. The API gateways, CloudWatch events, and some triggers &ldquo;given for free&rdquo; by AWS are left out of this for readability:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Serverless Architecture of the Answering Machine app"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/answering_machine_architecture.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The full suite of lambdas and persistent storage services that make up the Answering Machine.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2019-07-03:/answering-machine/</guid>

                
                    <link>http://localhost:1313/answering-machine/</link>
                

                
                    <pubDate>Wed, 03 Jul 2019 00:00:00 UTC</pubDate>
                

                
                    <title>The Answering Machine</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>First Ten is an educational app containing information about the U.S. Bill of Rights, accessible on Google devices and smart speakers. It uses a VUI (voice user interface) only, meaning there is no visual way to interact with the app.</p>
<p><strong>Try it here:</strong> <a href="https://assistant.google.com/services/a/uid/00000036f6a580ed">https://assistant.google.com/services/a/uid/00000036f6a580ed</a>
<strong>Github:</strong> <a href="https://github.com/hockenmaier/billofrights">https://github.com/hockenmaier/billofrights</a></p>
<p>Like Alexa skills, Google actions can be accessed through search or by simply asking for their names in Google Home smart speakers. Ask your Google Home or Android device, &ldquo;Can I speak to First Ten?&rdquo; in order to try it.</p>
<h3 id="architecture">Architecture</h3>
<p>First Ten&rsquo;s backend is built in the serverless AWS services Lambda and DynamoDB, and its frontend—the engine that parses your voice into different &ldquo;intents&rdquo; and parameters—is built on Google&rsquo;s Dialogflow.
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Serverless Architecture of the First Ten app"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/first_ten_architecture.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>
</p>
<hr>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2018-05-19:/first-ten/</guid>

                
                    <link>http://localhost:1313/first-ten/</link>
                

                
                    <pubDate>Sat, 19 May 2018 00:00:00 UTC</pubDate>
                

                
                    <title>First Ten</title>
                
            </item>
        
    </channel>
</rss>
