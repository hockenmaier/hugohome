













    
        
    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/ai/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/tags/ai/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2020-01-01 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>http://localhost:1313/tags/ai/</link>

        

        <title>AI · Tags · Home</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/q_1itpdiPb4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>The &ldquo;Human Joystick&rdquo; is an experimental VR movement system in which the player moves through the virtual environment by changing their physical location within their VR &ldquo;playspace&rdquo;.</p>
<p>A demo of the human joystick movement system, showing how the system can work on flat surfaces or terrain.</p>
<p>This was my first barebones VR project. Though I knew Unity going in, VR and 3D games in general have a lot of unique aspects that I wanted to learn about while trying to solve an actual problem, rather than following tutorials or demos online.</p>
<p>VR has some adoption problems in its current state. We all know of some of the main problems- the clunky headset, the nausea issues, and of course the pricetag. But one major problem that you don&rsquo;t really notice until you get into it, is the lack of a good solution for virtual movement.</p>
<p>I had been wondering about &ldquo;the human joystick&rdquo; as a potential a solution to this particular problem ever since getting into consumer VR in 2016.</p>
<p>In most modern VR systems, the player can move physically around the room if they choose. Some applications and games depend on this - they put you in a small space and rely on your physical movement in order to reach different areas and interact with things. But games that provide a more traditional sense of scale and allow players to move through large worlds cannot rely on physical motion, because their users are constrained by physical space. Because of this, you see all kinds of &ldquo;artificial&rdquo; locomotion systems in order to let people move around - some just like traditional 2D games that let users &ldquo;slide&rdquo; their playspaces around the world using a joystick, and others that adopt teleportation mechanics. Neither feel very natural as compared to actually walking, and some can be downright sickening.</p>
<p>My goal with this project was to solve this problem with a mixture of physical and artificial movement.</p>
<p>It works like this: When the player is standing near the center of their playspace, physical VR movement applies. The player can move around and interact with things with their actual bodies. But once the player moves further from the center, the plaspace starts to move with them in the same direction as the vector from the center of the player&rsquo;s space to their current position. This allows for some of the benefits that physical movement experiences have, while allowing the players to more naturally move through an infinite amount of space.</p>
<p>I experimented with several speeds, both static and scaling with the distance between the center and the player. I also experimented with the size of the physical movement &ldquo;deadzone&rdquo; and with vertical and constrained movement across hills, valleys, and buildings.</p>
<hr>
<table>
  <thead>
      <tr>
          <th style="text-align: center">


















<div class="paige-image">
    




























    



    

    
        

        

        
    
        

        

        
    
        

        
            

    



    







    





    



    






















<img   class="img-fluid "  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/human_joystick_centered.jpg"   style="display:block; height: auto; margin:0 auto; width: 60%"   >


</div>
</th>
          <th style="text-align: left"><em>View from the player&rsquo;s perspective looking at the guides at his feet. With the white dot in the red deadzone, the player isn&rsquo;t moving.</em></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">


















<div class="paige-image">
    




























    



    

    
        

        

        
    
        

        

        
    
        

        
            

    



    







    





    



    






















<img   class="img-fluid "  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/human_joystick_moving.jpg"   style="display:block; height: auto; margin:0 auto; width: 60%"   >


</div>
</td>
          <td style="text-align: left"><strong><em>When the white dot is in the green area, the player moves in that direction. Here I am moving forward and left at about half of max speed.</em></strong></td>
      </tr>
  </tbody>
</table>
<hr>
<p>Eventually I found some good default values and the system worked, but there were some unforeseen problems: First, it was more difficult to center yourself within the playspace without looking at the visible guides I put at the player&rsquo;s feet than I expected. Second and more importantly, when you were already moving in one direction, it was not as simple as I thought to start moving in another direction accurately without fully returning to center, which was an immersion breaker.</p>
<p>Ultimately I put the project up for others to view but have not expanded it into a full experience or released it on any marketplaces. Feel free to download the Unity project and try it on your own VR setup if you&rsquo;re curious.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2020-01-01:/human-joystick/</guid>

                
                    <link>http://localhost:1313/human-joystick/</link>
                

                
                    <pubDate>Wed, 01 Jan 2020 00:00:00 UTC</pubDate>
                

                
                    <title>Human Joystick VR</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Note from 2025: The Answering Machine is a proof-of-concept system that I built using pre-LLM natural language processing (NLP), specifically NLTK, to produce answers to questions asked about data in plain English.</p>
<p>Looking back, this project was a great insight into what LLMs immediately allowed that was incredibly difficult before.  This project was several months of work that the openAI sdk would probably have allowed in a few weeks - and that few weeks would have been mostly frontend design and a bit of prompting.</p>
<p><strong>Try it here:</strong> <a href="http://voicequery-dev.s3-website-us-west-2.amazonaws.com/">http://voicequery-dev.s3-website-us-west-2.amazonaws.com/</a>
<strong>Github:</strong> <a href="https://github.com/hockenmaier/voicequery">https://github.com/hockenmaier/voicequery</a></p>
<p><img src="/images/answering_machine_uploads.png" alt="Answering Machine homepage"></p>
<p>This proof-of-concept system uses natural language processing (NLP) to produce answers to questions asked about data in plain english.</p>
<p>It is designed with simplicity in mind - upload any columnar dataset and start asking questions and getting answers. It uses some advanced NLP algorithms to make assumptions about what data you&rsquo;re asking about, and will let you correct those assumptions for follow-up questions if they&rsquo;re wrong.</p>
<p>It is built entirely out of serverless components, which means there is no cost to maintain or run it other than the traffic the system receives.</p>
<hr>
<h3 id="how-to">How-to</h3>
<p>On a desktop or tablet, click the link in the header to navigate to the answering machine. For now it isn&rsquo;t optimized for smartphone-sized screens.</p>
<p>In order to use the answering machine, you can either select one of the existing data sets such as &ldquo;HR Activity Sample&rdquo; or upload one of your own using the homepage of the site:</p>
<p><img src="/images/answering_machine_uploads.png" alt="Answering Machine homepage"></p>
<p>To upload your own, click in the upload area or just drag a file straight from your desktop. For now, use csv data files. Excel and other spreadsheet programs can easily save data in the csv format using the &ldquo;File &gt; Save As&rdquo; or similar option in the menu. Each file needs a unique name.</p>
<p>When you hit the upload button, the site may not appear to change until the file is uploaded, at which point you&rsquo;ll see it appear in the box labeled &ldquo;Ask Your Data Anything&rdquo; below. Click on your file to start using it with the answering machine, or click the red trash can icon to delete it.</p>
<p>There are no user accounts in this system yet, so the data you upload might be seen by other users using the system. Try not to use sensitive data for now.</p>
<p>When you enter a dataset, you&rsquo;ll see a view that presents you with a quite a bit of information:</p>
<p><img src="/images/answering_machine_hr.png" alt="Main view of Answering Machine"></p>
<p>The only part you need to focus on right now is the information panel. This panel lists out all of the fields (columns), data types of those fields, and some sample data from a few records in your data set:</p>
<p><img src="/images/answering_machine_info.png" alt="Answering Machine info panel"></p>
<p>You can use this panel to start to formulate questions you might have about the data. If you see number values, you might ask about averages, maximums, or other math that might otherwise take some time to calculate. If you see a date, you can ask questions about the data in certain time periods.</p>
<p>Many datasets also contain fields that only have a few specific allowed values. When the answering machine sees less than 15 unique values in any field, the data type will be a &ldquo;List&rdquo; and it lists them right out under the sample values table. You can use this type of value to ask questions about records containing those specific values. For example in the HR data set, you might only be interested in data where the &ldquo;Education&rdquo; field&rsquo;s value is &ldquo;High School&rdquo;.</p>
<p>Now look to the query bar to start asking your data questions:</p>
<p><img src="/images/answering_machine_query.png" alt="Answering Machine query bar"></p>
<p>The types of questions that will currently be automatically detected and answered are:</p>
<ul>
<li>Counts of records where certain conditions are true</li>
<li>Math questions such as averages, medians, maximums, and minimums</li>
</ul>
<p>These types of questions can be made specific by using qualifying statements with prepositional phrases like &ldquo;in 2019&rdquo; or adjective phrases like &ldquo;male&rdquo; or &ldquo;entry-level.&rdquo;</p>
<p>Combining these two ideas, you can ask specific questions with any number of qualifiers, such as: &ldquo;What was the median salary of male employees in the engineering department 5 years ago?&rdquo;</p>
<p>Upon hitting the &ldquo;ask&rdquo; button (or hitting Enter), the answering machine will do its best to answer your question, and will show you all of its work in this format:</p>
<p><img src="/images/answering_machine_answer.png" alt="Answering Machine response"></p>
<hr>
<h3 id="architecture">Architecture</h3>
<p>The Answering Machine is a purely &ldquo;serverless&rdquo; application, meaning that there is no server hosting the various components of the application until those components are needed by a user. This is true for the database, the file storage, the static website, the backend compute, and the API orchestration.</p>
<p>For the cloud nerds out there, <a href="https://martinfowler.com/articles/serverless.html">here is a great article</a> by Martin Fowler on what exactly &ldquo;serverless&rdquo; means, especially in terms of the backend compute portion which is arguably the most valuable part of the application to be serverless. For reference, I am using Martin&rsquo;s 2nd definition of &ldquo;serverless&rdquo; here.</p>
<p>This is a high-level map of all of the components that make the answering machine work in its current (June 2020) state. The API gateways, cloudwatch events, and some triggers &ldquo;given for free&rdquo; by AWS are left out of this for readability:</p>
<p><img src="/images/answering_machine_architecture.png" alt="Serverless Architecture of the Answering Machine app"></p>
<p>The full suite of lambdas and persistent storage services that make up the answering machine.</p>
<hr>
<h2 id="features">Features</h2>
<ul>
<li><strong>Upload any dataset</strong>: Users can upload columnar datasets (CSV format) and start querying.</li>
<li><strong>Real-time responses</strong>: Questions are answered instantly, with NLP driving the interpretation and response generation.</li>
<li><strong>Serverless architecture</strong>: The system incurs no hosting costs apart from traffic-based expenses.</li>
</ul>
<hr>
<h2 id="how-to-1">How-to</h2>
<p>On a desktop or tablet, click the link above to navigate to <strong>The Answering Machine</strong>. Currently, it is not optimized for smartphones.</p>
<p>To use the system:</p>
<ol>
<li>
<p><strong>Upload a dataset</strong>: Drag and drop a CSV file or use the upload button.</p>
<ul>
<li>Example files like &ldquo;HR Activity Sample&rdquo; are available for testing.</li>
</ul>
</li>
<li>
<p><strong>Start querying</strong>: Select a dataset and type your question in the query bar.</p>
</li>
</ol>
<p><img src="/images/answering_machine_hr.png" alt="HR dataset view"></p>
<ol start="3">
<li>Use the <strong>information panel</strong> to understand dataset fields, data types, and sample values. This helps formulate more specific questions.</li>
</ol>
<p><img src="/images/answering_machine_info.png" alt="Info panel"></p>
<hr>
<h2 id="query-types">Query Types</h2>
<p>The system currently supports:</p>
<ul>
<li><strong>Counts</strong>: “How many employees joined in 2020?”</li>
<li><strong>Mathematical questions</strong>: Averages, medians, maximums, and minimums.</li>
<li><strong>Filters</strong>: Combine conditions like “What’s the average salary of engineers in 2019?”</li>
</ul>
<hr>
<h2 id="concept-matching">Concept Matching</h2>
<p>If the NLP system misinterprets a query, users can create <strong>concepts</strong> to align query terms with dataset fields.</p>
<p><img src="/images/answering_machine_query.png" alt="Answering Machine query bar"></p>
<p>Concepts override the system’s auto-matching logic, ensuring accurate data interpretation.</p>
<p><img src="/images/answering_machine_concept.png" alt="Concept creation"></p>
<hr>
<h2 id="architecture-1">Architecture</h2>
<p><strong>The Answering Machine</strong> is a purely serverless application. Its backend consists of AWS Lambda functions, API Gateway, and other serverless components.</p>
<p><img src="/images/answering_machine_architecture.png" alt="Serverless architecture"></p>
<p>For more details, check out <a href="https://martinfowler.com/articles/serverless.html">Martin Fowler&rsquo;s article on serverless computing</a>.</p>
<hr>
<p>Feel free to explore the tool and test various datasets. Remember, this is a proof-of-concept system with no user accounts, so avoid uploading sensitive data.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2019-07-03:/answering-machine/</guid>

                
                    <link>http://localhost:1313/answering-machine/</link>
                

                
                    <pubDate>Wed, 03 Jul 2019 00:00:00 UTC</pubDate>
                

                
                    <title>The Answering Machine</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>First Ten is an educational app containing information about the U.S. Bill of Rights, accessible on Google devices and smart speakers. It uses a VUI (voice user interface) only, meaning there is no visual way to interact with the app.</p>
<p><strong>Try it here:</strong> <a href="https://assistant.google.com/services/a/uid/00000036f6a580ed">https://assistant.google.com/services/a/uid/00000036f6a580ed</a>
<strong>Github:</strong> <a href="https://github.com/hockenmaier/billofrights">https://github.com/hockenmaier/billofrights</a></p>
<p>Like Alexa skills, Google actions can be accessed through search or by simply asking for their names in Google Home smart speakers. Ask your Google Home or Android device, &ldquo;Can I speak to First Ten?&rdquo; in order to try it.</p>
<h3 id="architecture">Architecture</h3>
<p>First Ten&rsquo;s backend is built in the serverless AWS services Lambda and DynamoDB, and its frontend—the engine that parses your voice into different &ldquo;intents&rdquo; and parameters—is built on Google&rsquo;s Dialogflow.</p>
<p><img src="/images/first_ten_architecture.png" alt="Serverless Architecture of the First Ten app"></p>
<hr>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2018-05-19:/first-ten/</guid>

                
                    <link>http://localhost:1313/first-ten/</link>
                

                
                    <pubDate>Sat, 19 May 2018 00:00:00 UTC</pubDate>
                

                
                    <title>First Ten</title>
                
            </item>
        
    </channel>
</rss>
