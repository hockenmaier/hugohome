













    
        
    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="http://localhost:1313/tags/software/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="http://localhost:1313/tags/software/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-08-24 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>http://localhost:1313/tags/software/</link>

        

        <title>Software · Tags · hockenworks</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<blockquote>
<p>Warning: This article has a lot of embedded code, so <a href="/this-website/#ball-machine---the-game">the ball machine</a> is slow unless you have a REALLY fast computer. Play at your own risk.</p>
</blockquote>
<p>Seriously, though.</p>
<p>GPT-5, as a simple text completion model, is not a revelation.</p>
<p>This isn&rsquo;t so surprising. It was becoming clearer with every new raw LLM release that the fundamental improvements from scaling solely the performance of the core text predictor were starting to show diminishing returns. But I&rsquo;m going to make an argument today that, although the LLM itself is not nearly as much of a leap from GPT-4 as GPT-4 was from GPT-3, we have still seen at least a whole-version-number of real improvement between the release of GPT-4 and 5 as we did between 3 and 4. The reasons for that are mostly what exists around that LLM core.</p>
<p>And I&rsquo;m going to make this argument by showing you things the original GPT-4 could never have done.</p>
<h1 id="exhibit-a-a-new-solar-system">Exhibit A: A New Solar System</h1>
<p>GPT-5 came at a great time for me, because when OpenAI set the announcement to 8/7/25, I was in the middle of stress testing Agent Mode with the prior models. So, let me start by just illustrating the progress we had made between the first GPT-4 release in early 2023 and the capabilities of GPT-4 level models the week before the release of GPT-5.</p>
<p>Two years and four months have gone by since I ran my original Interactive Solar System experiment with the original version of GPT-4. You can play with it at <a href="/gpt-4-solar-system">hockenworks.com/gpt-4-solar-system</a>.</p>
<p>Now have a look at the same test performed a week before the release of GPT-5, which uses ChatGPT Agent Mode:</p>
<p>
<style>
  .agent-mode-solar-wrapper {
    position: relative;
    left: 50%;
    transform: translateX(-50%);
    width: 150%;
    max-width: 1500px;
    margin: 1em 0;
    padding-top: 70%;
  }

  @media (max-width: 768px) {
    .agent-mode-solar-wrapper {
      width: 100%;
      left: 0;
      transform: none;
      padding-top: 180%;
    }
  }
</style>

<div class="agent-mode-solar-wrapper">
  <iframe
    class="lazy-iframe"
    data-src="/html/agent-mode-solar-system-self-contained.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
  
    <span style="display: block; font-style: italic; margin-top: 0.5rem;">Works on desktop or mobile</span>
  
</div>

<div style="text-align:center;">
  <a href="/html/agent-mode-solar-system-self-contained.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>Here are Controls, again:</p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>And I again used this initial prompt:</p>
<blockquote>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.
 </p>
</blockquote>
<p>Quite an improvement, right?</p>
<p>Like my original experiment, I didn’t one-shot this result. This is the final result after 7 prompts, but unlike in the original, those follow-up prompts were not primarily fixes - they were new features that I thought would be good additions after playing with each previous version - all of which worked right away.</p>
<p>The most striking part of this experiment for me is that, despite the plateau that has largely been observed in base LLMs, this result is clearly far above and beyond what GPT-4 was able to do when it first released, and the reasons for that have to do with what the labs have been building <em>around</em> the base models.</p>
<p>In the case of Agent mode, as far as we know, we have a GPT-4 class model at the root. o3, which is what Agent Mode uses, is a GPT-4 class model with chain-of-thought and lots of self-play training.</p>
<p>It also has the ability to use tools, like the web search tool it used to find the planet facts.</p>
<p>But then the thing that most sets it apart from its base model is the deep research and computer use elements of Agent mode. As it was building this model, I saw Agent mode do things such as perusing the internet with its text browser to find scale information and browse for textures it could use as planet skins.</p>
<p>It wrote and ran test code using its Code Interpreter tool.</p>
<p>It used its chain-of-thought &ldquo;thinking&rdquo; to respond to errors, rethink controls and scale decisions, and decide to go back and search the web or use other tools some more.</p>
<p>Then, the most impressive new ability that allowed it to make this: It ran this code on a local browser and <em>visually tested it</em>, clicking on the planets to make sure the information panels were coming up, zooming and panning around, and using its own toolbars.</p>
<p>This last one to me, when paired with all of the other things that GPT-4 class models are not instrumented to do, is the start of a true general capability for AI software testing, almost as a side note to all of the other things it will unlock in the coming few years. Not just the little unit and automated tests that LLMs have been writing into our codebases for a long time - but actual visual look and feel testing. This is a huge market and will be a huge shock to the software industry when it’s fully realized.</p>
<p>These are all net-new abilities that GPT-4 class models have gained since GPT-4 came out in March of 2023. It&rsquo;s hard to see how much progress these capabilities add up to without just running the direct experiment like this solar system generation.</p>
<h1 id="gpt-5-without-agent-mode">GPT-5 Without Agent Mode</h1>
<p>I see GPT-5 as a formalization of all of these capabilities that lock in the step change in capability we&rsquo;ve seen over the last 2 years. There is a new model underneath there somewhere (some are saying it is o4) and that underlying model has certainly been reinforcement-trained to choose its tools wisely. It is the first model I have seen that can do all of these simultaneously, without user choice:</p>
<ul>
<li>Choose when to think and when to just answer</li>
<li>Run iterative web search calls</li>
<li>Write and run code to do highly logical-symbolic tasks like data analysis</li>
<li>Create, edit, and analyze images</li>
<li>Create and read nearly any kind of document a user might be working with</li>
</ul>
<p>And there are still a couple of things locked behind user choice, which is probably because these both result in much longer running and expensive tasks than simple thinking:</p>
<ul>
<li>Deep map-reduce style web research</li>
<li>Computer use (mouse and keyboard style, with Agent Mode)</li>
</ul>
<p>Using only the first set of &ldquo;default&rdquo; behaviors, GPT-5 can do things that the original GPT-4 could never have dreamed of. I have had the following prompt sitting around in my &ldquo;intelligence tests&rdquo; document for more than a year now under <em>&ldquo;Cool picross app idea, def solvable by competent AI (which doesn&rsquo;t exist yet at the end of 2024)&rdquo;</em>, waiting for a single model that can one-shot it. GPT-5 is the first one that does:</p>
<h2 id="exhibit-b-picross">Exhibit B: Picross</h2>
<blockquote>
<p>Take an image, increase contrast, and turn it into a 15x15 image. Create a black and white picross puzzle out of the image, including a UI that lets a player solve the puzzle.</p>
</blockquote>
<p>It&rsquo;s a simple prompt that implies a lot of underlying complexity. Here is the first one-shot result from GPT-5 I got (normal mode, I didn&rsquo;t select &ldquo;Thinking&rdquo; in advance):</p>
<p><style>
  .picross-wrapper { display:flex; justify-content:center; }
  .picross-wrapper .frame {
    position: relative;
    width: 100%;
    max-width: 900px;
    aspect-ratio: 3.35/4;
    border: 2px solid #fff; border-radius: 4px; overflow: hidden;
  }
  .picross-wrapper .frame > iframe {
    position:absolute; inset:0; width:100%; height:100%;
    transform-origin: top left;
  }
   
  @media (max-width: 480px){
    .picross-wrapper .frame { --s: .65; }
    .picross-wrapper .frame > iframe {
      transform: scale(var(--s,1));
      width: calc(100% / var(--s,1));
      height: calc(100% / var(--s,1));
    }
  }
</style>

<div class="picross-wrapper">
  <div class="frame">
    <iframe
      class="lazy-iframe"
      data-src="/html/picross-generator.html"
      src="about:blank"
      loading="lazy"
    ></iframe>
  </div>
</div>

<div style="text-align:center;">
  <a href="/html/picross-generator.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>It didn&rsquo;t make all the choices I would have, but it worked in one shot, the first time I tried it. All the way from uploading an image and transforming that to a puzzle, to a whole UI that lets you solve it. I purposely left this here after its one-shot result just to demonstrate the progress. You could take this idea much further.</p>
<h2 id="exhibit-c-the-baby-mesmerizer">Exhibit C: The Baby Mesmerizer:</h2>
<p>Now for the coolest thing I&rsquo;ve made with GPT-5 so far. I call this one the baby mesmerizer because baby Alice is absolutely stunned every time she sees it. I got this idea from another entertaining little physics simulation I saw somewhere.</p>
<p>I had GPT-5 make this one, then I tested it using the built-in runner in ChatGPT, changed it, and iterated on it 16 times. But I didn&rsquo;t write any of it myself - nor did I even open the code other than to tweak some variables to make it &ldquo;feel right&rdquo; here and there.</p>
<p>
<style>
  .agent-mode-solar-wrapper {
    position: relative;
    left: 50%;
    transform: translateX(-50%);
    width: 150%;
    max-width: 1500px;
    margin: 1em 0;
    padding-top: 85%;
  }

  @media (max-width: 768px) {
    .agent-mode-solar-wrapper {
      width: 100%;
      left: 0;
      transform: none;
      padding-top: 180%;
    }
  }
</style>

<div class="agent-mode-solar-wrapper">
  <iframe
    class="lazy-iframe"
    data-src="/html/exponential_bounce.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
  
</div>

<div style="text-align:center;">
  <a href="/html/exponential_bounce.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>How cool is that?</p>
<p>After you’re done messing around with this, take a detailed look at what has been built here. It is a tech demo, yes, but it has:</p>
<ul>
<li>A nice-looking UI and color scheme</li>
<li>A side menu that dynamically operates as a fly-out menu based on screen size</li>
<li>A bunch of tunable variables, including niceties like minimums and maximums affecting each other</li>
<li>Dynamic generated sound effects</li>
</ul>
<p>I also thought it was interesting that GPT-5, when I said that it needed to run as a single HTML file with no external dependencies, chose to write its own physics for this. There is no prebuilt physics engine at all here.</p>
<p>This result is about 1400 lines of code. This isn’t a huge project, but it is far more than we could get a GPT to reliably produce just a year ago. The typical loop before was that you’d get past 200 lines of code or so, and then every new feature or bugfix requested would break two other things around the codebase, effectively enforcing a tiny complexity cap.</p>
<p>Let&rsquo;s be real: It&rsquo;s absolutely amazing that I could make something as complicated as this, exactly how I imagined it, just by describing it in English and a collective hour or two of testing. And though I like to code and have been bummed for a while that most straightforward coding like this is going the way of the dodo, this experience of not coding and instead just setting requirements and playing with the result was also a lot of fun. More fun than tearing through <a href="/on-ai-software-development-2/#agentic-ides">codespam in a tool like Cursor</a>. And frankly, I would never spend time making something like this if I had to code it all from scratch.</p>
<h1 id="a-plateau">A Plateau?</h1>
<p>I&rsquo;ve already read more than enough takes that GPT-5 signals the end of the current wave of AI, as a sort of intelligence plateau somewhere just below humans.</p>
<p>We should observe that we did not foresee the advancements that would get us from GPT-4 to GPT-5. Yet here we are: GPT-4 was <a href="/gpt-4-solar-system/">barely able to write 200 lines of buggy solar system code</a>, and GPT-5 one-shots it. The core model under the hood is likely a bit better, but it was proper tooling and reinforcement training on that tooling that really made the difference. And from what I hear, there are many other avenues of work that researchers say are still in early stages, such as reinforcement training on long-running agentic tasks and building the synthetic datasets that will allow for that.</p>
<p>So, even though GPT-5 doesn&rsquo;t seem like a huge advancement from models like o3 and Sonnet 4, hindsight makes the upward trajectory clear.</p>
<p>No matter how fast we see progress, there is clearly a ton of fun to have along the way!</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-08-24:/gpt5/</guid>

                
                    <link>http://localhost:1313/gpt5/</link>
                

                
                    <pubDate>Sun, 24 Aug 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Real GPT-5 Was The Friends We Made Along The Way</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Recently I read the <a href="https://ai-2027.com/scenario.pdf">AI 2027 paper</a>. I was surprised to see Scott Alexander&rsquo;s name on this paper, and I was doubly surprised to see him do his <a href="https://www.dwarkesh.com/p/scott-daniel">first face reveal podcast about it with Dwarkesh</a>.</p>
<p>On its face, this is one of the most aggressive predictions for when we will have AGI (at least the new definition of AGI, which is something that is comparable to or better than humans at all non-bodily tasks) that I have read. Even as someone who has been a long believer in <a href="https://en.wikipedia.org/wiki/The_Singularity_Is_Near">Ray Kurzweil&rsquo;s Singularity predictions</a>, 2027 strikes me as very early. I realize that Kurzweil&rsquo;s AGI date was also late 2020s, which puts his prediction in line with AI 2027, while 2045 was his singularity prediction. But 2027 still feels early to me.</p>
<div class="paige-row-wide">
  <div style="display:grid; grid-template-columns:repeat(2, minmax(0,1fr)); gap:15px;">
      































    













    



    























<img  alt="AI 2027"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-2027.png"    >


      































    













    



    























<img  alt="Singularity Is Near"   class="rounded-3 w-100"  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/The-Singularity-Is-Near.jpg"    >

</div>
</div>

<p>I won&rsquo;t get into my full take on AI 2027 here, but the core argument comes down to the same one I was making in my <a href="/on-ai-software-development">original post on AI software development</a> - which is that, once AI agents are able to replace software engineers, instead of just assisting them, their current performance in non-software domains doesn&rsquo;t matter. They will simply improve on their own software and training procedures at such a rate that the time difference between the moment of automating software engineering tasks and the moment of automating all other tasks is negligible.</p>
<p>In light of AI 2027, I figured it was a good time to update where AI is actually in terms of software engineering. I&rsquo;ve had the chance to test many of the latest AI software development tools and models, and they have come a long way since my original post. This post is primarily a story of my personal experience with the latest AI coding and &ldquo;vibe coding&rdquo; tools on a game and web development project.</p>
<h1 id="what-i-built">What I Built</h1>
<p>I&rsquo;ve been developing with AI since 2021, but I got a special chance to really put the latest models and tools to use the last few months while on parental leave. And I have a more valuable type of project with which to accurately evaluate these models on their creative capacity for programming, which is really the hard stuff.</p>
<p>This project is absolutely not in the training data, not even a little bit. That&rsquo;s because it&rsquo;s a game built into a website. AI is already much worse at gamedev than other types of development, but it&rsquo;s made extra strange by the fact that the game is built into a CMS website. <a href="/this-website/#ball-machine---the-game">This website!</a></p>
<p>It&rsquo;s a very strange project. I won&rsquo;t get too much into the weeds here, but essentially we have a <a href="https://gohugo.io/">Hugo CMS</a> underneath everything, and that Hugo CMS has a bunch of short code that interacts with a bunch of other custom JavaScript that loads dynamically when the player decides to start playing any given page. Just try it on the top right corner of this page and you might start to imagine how weird of an application this would be to work on.</p>
<h1 id="three-ways-for-ai-to-build-your-project">Three Ways for AI to Build Your Project</h1>
<p>Over the course of 5 months, I worked on this game via 3 main methods, which happen to be the main 3 ways that anyone is writing code with AI these days:</p>
<ol>
<li><strong>Chat Assistants</strong> - particularly the o4-mini and o3 models</li>
<li><strong>Automated IDEs</strong> - particularly Cursor</li>
<li><strong>Autonomous Coding Agents</strong> - particularly OpenAI&rsquo;s &ldquo;Codex&rdquo;, which launched about 2 months before I finished the project.</li>
</ol>
<p>I&rsquo;m going to cover what they are and how they performed on my game.</p>
<h2 id="chat-assistants">Chat Assistants</h2>
<blockquote>
<p>Chat Assistants are models running in a client such as <a href="https://chatgpt.com/">ChatGPT</a>, <a href="https://claude.ai/login?returnTo=%2F%3F">Claude</a>, or <a href="https://gemini.google.com/app">Gemini</a>. This often happens on a website, but can happen in other software too - like Github copilot&rsquo;s chat mode in <a href="https://code.visualstudio.com/">VScode</a>. The key is that the human is doing all of the input into the AI model and output from the AI response to the codebase. Getting these assistants to write code for you entails passing in instructions and sometimes code context from your project.</p>
</blockquote>
<p>This section is on chat assistants, but it&rsquo;s also a general update on the status of AI code quality today, absent of the wrappers like Cursor and Codex which I will get to later.</p>
<p>Chat assistants are great and have been helping developers for a long time! These are the closest most people get to running LLMs &ldquo;raw&rdquo; rather than through specialized tools designed to limit their errors or make them more agentic. So, you can think of the performance of Chat Assistants for coding as sort of the &ldquo;default performance&rdquo; of AI models to today at coding tasks. The other methods of having AI writing code still have the general performance dynamics I&rsquo;m going to talk about here at their cores, even if they mitigate some types of failures by running their own tests and dealing with console errors.</p>
<p>I&rsquo;ll start with what might be an obvious statement: LLMs are great at producing simple, low context code. Like - so great that most humans won&rsquo;t need to write much algorithmic logic from here on out. I certainly didn&rsquo;t when making this game, or really anything else I&rsquo;ve made in the last two years.</p>
<p>But basic coding is not all of software engineering, and LLMs still have major issues:</p>
<p><strong>1. They can&rsquo;t deal well with contexts over 30K tokens or so (even the best models with supposedly millions of token context windows).</strong></p>
<p>This means the actual developer (me and you) are typically the ones picking specific files and functions to send into the context window, lest we confuse the model. This is arduous unless the codebase is small enough to fit entirely into the context window. That&rsquo;s exactly what I think is going on with most of the new &ldquo;vibe coded&rdquo; projects we see showing impressive results - these are just tiny POC apps that haven&rsquo;t hit more than a few thousand lines of code yet. For context, most serious enterprise apps containing the detailed logic and edge cases that real use cases require are in the millions or hundreds of millions of lines of code.</p>
<p>This issue is compounded by the fact that the largest labs seem to recognize it, and have modified their chat clients in ways kept secret from the user, to save themselves money. OpenAI in particular implements an invisible, silently failing ~40K token limit on text pasted into the chat window. I know this is the case after many tests where I directly ask models about context that came after the 40K token mark. I have run into many strange issues caused by this, and it’s an insane policy on OpenAI’s part because they have non-invisible text limits for some models like 4o that tell you when your message is too long. My theory on why is pretty insidious if true: They want paying customers to think their very large thinking models will accept their full token limits, and they don’t think many customers will find out they are truncating them for cost reasons anyway, because these models become stupid after 30K tokens.</p>
<p><strong>2. They are biased to be &ldquo;advisors&rdquo; rather than &ldquo;doers&rdquo;</strong></p>
<p>This is just annoying, and I hope it gets trained out soon, though the second two categories of AI coding tools might obviate fixing this in chat assistants themselves. When they are not specifically trained out of this mindset, most AI models just really <em>want</em> you, the human, to be doing everything, and to act themselves as an advisor. This makes sense with one of the main sources of code training data being from Stackoverflow and other blogs, where developers can never seem to rid themselves of a pseudo-condescending &ldquo;you should have been able to read the docs and learn this yourself&rdquo; tone. It&rsquo;s also just a pattern exhibited by people in general - more often than not, especially in the corporate world, people are incentivized to be the &ldquo;coaches&rdquo; rather than the &ldquo;worker bees&rdquo;. One reason why things can get done so slowly in big political companies.</p>
<p><strong>3. They are still wrong at the basics sometimes, but they&rsquo;re wrong MUCH more confidently than human developers.</strong></p>
<p>This is probably the biggest issue for all code produced by AI, regardless of interface. But I&rsquo;m including it in the &ldquo;Chat Assistants&rdquo; section because there are no guardrails on chat assistants other than the human user: they won&rsquo;t run tests and they won&rsquo;t encounter errors. Here&rsquo;s a recent example of a true algorithmic mistake that cost me days of confusion developing my game.</p>
<p>It&rsquo;s a function that sets the time interval between physics steps in my game.</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>setInterval(() =&gt; {
</span></span><span style="display:flex;"><span>  <span style="color:#080;background-color:#0f140f;font-style:italic">/* --- recompute every X engine ticks (~0.5 s) ---------------------- */</span>
</span></span><span style="display:flex;"><span>  <span style="color:#fb660a;font-weight:bold">if</span> (engine.timing.timestamp % <span style="color:#0086f7;font-weight:bold">20</span> === <span style="color:#0086f7;font-weight:bold">0</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#fb660a;font-weight:bold">let</span> maxV = <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>    Matter.Composite.allBodies(engine.world).forEach((b) =&gt; {
</span></span><span style="display:flex;"><span>      <span style="color:#fb660a;font-weight:bold">if</span> (b.label === <span style="color:#0086d2">&#34;BallFallBall&#34;</span>) {
</span></span><span style="display:flex;"><span>        <span style="color:#fb660a;font-weight:bold">const</span> v = Math.hypot(b.velocity.x, b.velocity.y);
</span></span><span style="display:flex;"><span>        <span style="color:#fb660a;font-weight:bold">if</span> (v &gt; maxV) maxV = v;
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>    });
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#080;background-color:#0f140f;font-style:italic">/* map speed → substeps (1‒5) */</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fb660a;font-weight:bold">let</span> target =
</span></span><span style="display:flex;"><span>      maxV &gt; <span style="color:#0086f7;font-weight:bold">40</span> ? <span style="color:#0086f7;font-weight:bold">8</span> : maxV &gt; <span style="color:#0086f7;font-weight:bold">25</span> ? <span style="color:#0086f7;font-weight:bold">8</span> : maxV &gt; <span style="color:#0086f7;font-weight:bold">12</span> ? <span style="color:#0086f7;font-weight:bold">8</span> : maxV &gt; <span style="color:#0086f7;font-weight:bold">3</span> ? <span style="color:#0086f7;font-weight:bold">8</span> : <span style="color:#0086f7;font-weight:bold">1</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#080;background-color:#0f140f;font-style:italic">/* mobile caps at 2 */</span>
</span></span><span style="display:flex;"><span>    <span style="color:#fb660a;font-weight:bold">if</span> (isMobileLike &amp;&amp; target &gt; <span style="color:#0086f7;font-weight:bold">2</span>) target = <span style="color:#0086f7;font-weight:bold">2</span>;
</span></span><span style="display:flex;"><span>    substeps = target;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#fb660a;font-weight:bold">const</span> dt = baseDt / substeps;
</span></span><span style="display:flex;"><span>  <span style="color:#fb660a;font-weight:bold">for</span> (<span style="color:#fb660a;font-weight:bold">let</span> i = <span style="color:#0086f7;font-weight:bold">0</span>; i &lt; substeps; i++) {
</span></span><span style="display:flex;"><span>    Matter.Engine.update(engine, dt * engine.timing.timeScale);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}, baseDt);
</span></span></code></pre></div><p>This code was confidently written by o3, the current best model for coding, after lots of correct &amp; thoughtful discussion about how often we should run physics simulation on my game. 60 hz is a common simulation time, which is Matter.js’s default, but when you have small objects moving around quickly, you need to increase it so that they don’t tunnel through each other.</p>
<blockquote>
<p>Tunneling in physics simulations is when one of two objects on a collision course is moving fast enough that there is no single frame where they are colliding, and so they simply pass through each other</p>
</blockquote>
<p>This is all good theory, and o3 also came up with a really thorough chart for this little line:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>maxV &gt; 40 ? 8 : maxV &gt; 25 ? 8 : maxV &gt; 12 ? 8 : maxV &gt; 3 ? 8 : 1;
</span></span></code></pre></div><p>which tried to determine dynamically how many physics steps to run based on how fast objects are moving on the screen. It&rsquo;s a great idea, it is simple algorithmic work, and I was happy not to have to write or think about it.</p>
<p><em>There was just one problem.</em></p>
<p>The condition at the start of this snippet, <em>if (engine.timing.timestamp % 20 === 0)</em>, would never be true, because we are always dividing time up into little chunks in a way that they wouldn’t be whole numbers. The entire code snippet above never runs due to this oversight.</p>
<p>Here&rsquo;s where it gets really hairy, and the confidence issue really compounds in a negative way. I asked o3 and many other AI chatbots and agents about this code, and all “assumed” it was correct and would fire when I told them it wasn’t working. They would dig into all the little minutia of this function, the thresholds, how we were getting the max velocity, etc, and none figured out that why it wasn’t working was something more basic like this.. that it was never firing at all because of the faulty condition that o3 wrote at the top.</p>
<p>To be fair, I, the human, should have caught this before asking yet more AI agents why it wasn’t working. But the AI has taught me to be a little bit lazy, and what does it say about the state of AI independently-written software if multiple passes by state-of-the-art agents didn’t see the issue with a snippet of less than 20 lines of code?</p>
<h2 id="agentic-ides">Agentic IDEs</h2>
<blockquote>
<p>Automated IDEs are &ldquo;development environments&rdquo;, the software that humans use to write code, with LLM integrations added in to auto-complete or produce new code somewhat agentically. They can do things like read terminal output and attempt to fix their mistakes before you see them. The main examples right now are <a href="https://cursor.com/en">Cursor</a> and <a href="https://windsurf.com/editor">Windsurf</a>. Amazon just launched <a href="https://kiro.dev/blog/introducing-kiro/">Kiro</a> in this category.</p>
</blockquote>
<p>After I became a parent and then started having a few hours a day to come up for air, one of the first things I wanted to do was REALLY crack into one of these vibe-coding tools to see what it was all about. I picked <a href="https://cursor.com/">Cursor</a>.</p>
<p>Simple download, login, install. All going well. The very first feature I decided to throw at it (using <a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7</a> which was at the time the top recommended model) was something relatively simple: I wanted to modify the behaviour of the &ldquo;Compactor&rdquo; item in the Ball Machine so that it would not only combine the value of balls, but also remember how many balls had been combined and combine their value growth per second. This would need to change about 3 files and read from maybe 5 more.</p>
<p>The first real disappointment here happened after I entered my detailed prompt for how I wanted this to work: Cursor not only wanted an instruction file explaining how my codebase worked, it also wanted me to <em>manually pick every file</em> to include in the context. I laughed out loud when I saw this. I had been under some impression that Cursor was more than essentially copying and pasting a prompt and some code context into ChatGPT.</p>
<p>Nevertheless, I picked all the files I thought were relevant - about 10 of them - and let the agent run. I did this at least three times, testing each one. Some of these runs were many minutes of Claude 3.7 iteratively coding.</p>
<p>Abject failure. Nonsense written into the files. Near total disregard for the context I had provided, outside of some basics like naming and references to real things.</p>
<p>I pressed on for many more attempts, changing the wording of my request, including different files for reference, trying to explain the problem more clearly.</p>
<p>After the final attempt with prompt tweaks and file context tweaks, I just gave up on Cursor doing this. I used <a href="/context-caddy">Context Caddy</a>, my own VScode extension that lets me copy code context from my projects, along with o3, and I got code that worked. Code which I had to tweak and find the right place for. Apparently this was too complicated for Cursor and its top recommended AI model. I was able to achieve simple, mostly one file edits with Cursor using simple natural language prompts. It never seemed to respond well to my multi-pager prompts that I have grown accustomed to giving big thinking models like o3.</p>
<p>In the end, I cannot argue that using a raw LLM via a chat assistant is better than using an automated IDE, but I found them to be similar experiences. There is theoretically nothing that a chat-based LLM could do that Cursor could not, provided you are willing to pay for an expensive model like o3 behind it, prompt it carefully, and correct its mistakes. But I was not willing to do that - the small agentic benefits of Cursor didn&rsquo;t seem worth this cost to me, while already paying for a ChatGPT plus subscription. Perhaps I had been spoiled by Context Caddy.</p>
<p>If you were used to simply plugging in prompts to ChatGPT, then maybe being able to pick files for context combined with a loop that lets the AI model respond to terminal errors could seem like a big upgrade. For me, it was not. The whole experience felt like I was dealing with a typical chat assistant, with a few quirks. This chat assistant has the upside of being able to see errors in the terminal and correct them, but also the downside of directly injecting a bunch of potential AI spam into my codebase without me picking it ahead of time and fixing its mistakes.</p>
<p>After all this, here&rsquo;s my <strong>hottest take of the article</strong>: Automated IDEs like Cursor and Kiro are a fad that will pass us by soon.</p>
<p>They exist today because IDEs are where all code editing takes place, and putting LLMs there was obvious. The next obvious thing was to let them run code and read output from the terminal in a loop. The next most obvious thing is to give them &ldquo;specs&rdquo; instead of prompts, so they have more patterns to follow rather than duplicating code everywhere as LLMs always want to do. This is where we&rsquo;re at with these Agentic IDEs, and they are useful.</p>
<p>But the core issue with this obvious progression of features is that it ignores this fundamental reality: Once AI models are actually good enough to do a software engineering task, there is no reason for that task to happen in an IDE at all, a tool made for humans to write code.</p>
<h2 id="autonomous-coding-agents">Autonomous Coding Agents</h2>
<blockquote>
<p>Autonomous Coding Agents, unlike Agentic IDEs, approach the problem of agentic software development without the lens of the toolspace that humans rely on. Using these systems does not depend on the actual code ever living on your own computer. Instead, you give them access to the code in your remote repository such as Github, and they do all the work to clone that code, look through it, make changes, test what they can, and ultimately contribute directly back to that codebase in the form of a <a href="https://en.wikipedia.org/wiki/Distributed_version_control#Pull_requests">Pull Request</a>, or &ldquo;PR&rdquo;. The main two right now are OpenAI&rsquo;s <a href="https://openai.com/codex/">Codex</a> and <a href="https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/">Github Copilot Agents</a></p>
</blockquote>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    







    





    



    























<img  alt="Image description"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/codex-sample.webp"   style="height: auto; width: 100%"   >


    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">Codex taking a crack at the achievements feature of The Ball Machine</span>
    
</div>

<p>On the face of it, Autonomous Coding Agents aim to do the same thing which has come to be the top feature of Agentic IDEs: They let you specify a feature, change, or fix by simply speaking or typing English, and then they proceed through a loop of coding, evaluating, and refining until they &ldquo;think&rdquo; the feature is done. And both types of system can do that job.</p>
<p>But in my experience with Codex, the Autonomous Coding Agent I used, the difference was night and day.</p>
<p>Codex benefits most from its limitations. It does not sit in an IDE, so it could not possibly ask you, the human, to pick the files to add to system context. It doesn&rsquo;t run on your computer with you watching it, so it needs to handle all of its own looping, eval, and stop logic on its own. It needed to &ldquo;get good&rdquo; at these things. And that it did.</p>
<p>I will just skip straight to the punchline: OpenAI&rsquo;s Codex was writing full new features for my game via my voice-texted prompts alone (most of which I spoke while walking the baby) within days of me setting it up. In fact, the <a href="/this-website/#achievements">entire achievements feature of my game</a>, including the UI components, the logic to detect them, and save them - everything but the images I used for the achievement icon - was created in two Codex PRs generated without me touching the codebase. <a href="https://gist.github.com/hockenmaier/b28ad007f75a1e6a3efbff419ace6589">Full prompt here</a></p>
<p>I had built up to this point a little bit. I started Codex on simple tasks reminiscent of the one I started Cursor on, but I found that I was willing to lend more tasks to Codex, because I knew it wouldn&rsquo;t interrupt my work at all. I would just end up with a PR I could review 20 minutes later or so, and if it worked I would merge it. Compare that to the experience with Agentic IDEs, where most of the time you need to open your computer, type up a prompt, select all the context you think is necessary, and then &ldquo;help it along&rdquo; when it gets stuck. This is not to say Codex didn&rsquo;t get stuck - there were things it couldn&rsquo;t do, even the occasional simple thing, but for those I would pull the PR, see that it didn&rsquo;t work, and then just reject it. No longer was I in the world of trying to &ldquo;help along&rdquo; the AI ending up in strange diff states and git stashing to clean up my own workspace. It was just &ldquo;Hey this is a thing the robot could do&rdquo;, then quick voice text, and 20 minutes later 80% of the time a working PR.</p>
<p><strong>But that thought, &ldquo;this is a thing the robot could do&rdquo; is very important</strong></p>
<p>For all the advantages of Codex, it’s still just as bad and generally uncreative as any other AI. And of course that is highly dependent on how standard and boilerplate your code is. Codex is better than any agent I&rsquo;ve used at following a super detailed prompt to completion (The first prompt for that achievements feature is almost two pages long) - but you still need to know your project well enough to write that super detailed prompt. For my game, I just know there are things I could never do with AI. Those involve messing with the fundamental architecture of the code, rethinking gameplay look and feel, or anything with too many moving parts. AI has a limit, and in 2025 that limit is still quite low. It&rsquo;s a junior developer that writes super fast. But I found that when shifting from thinking of AI as an assistant I would proactively try to use vs something I could completely offload half of my tasks to &ldquo;set and forget&rdquo; - the latter is a much more meaningful time savings. It really forced me to draw the line of what could be automated and what the AI could do for me.</p>
<p>There is another nice little benefit to a system that introduces a neat little PR with comments instead of pouring a bunch of code into your active workspace: it immediately puts you into the mindset of &ldquo;I am reviewing this potentially bad code&rdquo;. It forces you to either accept or not. Typically, in &ldquo;assistant land&rdquo; whether that is using chat assistants or things like Cursor, you are going to PR the code yourself. You won&rsquo;t be distinctly &ldquo;reviewing it&rdquo; like a Codex PR, and to other humans it will appear as your work.</p>
<p>I like Codex. A lot. It&rsquo;s more fun to use than something like Cursor and lets me focus on the things I need to. I believe this type of system is where automated software development is going, long term.</p>
<h1 id="tldr-and-what-comes-next">TLDR and What Comes Next</h1>
<p>There is a common pattern when people talk about AI in their own field vs other fields: AI seems revolutionary in fields you don&rsquo;t understand deeply, and less so in your own - because you see the nuance of the errors it makes. I have seen <a href="https://arxiv.org/abs/2507.09089">this study</a> referenced by several engineers in the past few weeks as solid evidence that AI coding is just hype. It makes the core claim that AI is <em>reducing</em> engineering productivity rather than enhancing it, by 20%.</p>
<p>I don&rsquo;t totally buy this paper. Measuring productivity like this is riddled with issues arising from the basic fact that trying to make engineers use AI often results in the same kind of misuse that you might expect if you asked a bunch of artists to use Midjourney to do their jobs.</p>
<p>But that is not all that&rsquo;s going on. A big part of it, too, is that no matter the paradigm, the underlying AI of 2025 is just <em>not smart enough</em> for most difficult coding tasks. This is especially true in large existing codebases where AI models must use existing classes and refactor existing code rather than just whipping up 300 new lines of code every time.</p>
<p>There is the big stumbling block most people have in evaluating AI: They see a success or they see a failure, they aren&rsquo;t sure how much &ldquo;help&rdquo; the AI received from a human in the cases of success, and they fall back to their biases for how useful or useless they already thought AI was in this domain. Their priors are not updated by these examples.</p>
<p>But now, with the advent of Autonomous Coding Agents (keep in mind, these only started to be a thing a few months ago), software engineers have a clearer view into <em>actually how much</em> of their work can be truly automated. That&rsquo;s because systems like Codex are &ldquo;all or nothing&rdquo; in some sense. They PR working code or they don&rsquo;t - and there is much less space to get caught up in &ldquo;helping&rdquo; the AI in a task it really didn&rsquo;t have the smarts to do on its own.</p>
<p>Right now, about half of the things I typically set out to do on a project like this game are achievable via Codex. This is mostly small stuff that can be done changing only 1-3 files. But it can be some larger features too, like my achievements system, which don&rsquo;t require thinking hard about the architecture of the application or changing things really significantly. Adding achievements on top of my working game was a sort of perfect example of a high output but low context task that Codex could achieve. This all means there&rsquo;s still a long way to go: the long tail of low complexity work in software engineering is what Codex can already pick up, and the hard stuff is left.</p>
<p>But now that AI is working &ldquo;independently&rdquo;, instead of humans always subtly correcting it when they choose what to paste in from a chat assistant, or when they clean up sloppy Cursor code, that threshold of &ldquo;AI achievable&rdquo; is much clearer. If it&rsquo;s at 50% now, we&rsquo;ll see it tick up to 60%, then 70%, and so on.</p>
<p>I still hold this to be true, as do the authors of AI 2027: when something like Codex is at 100%, the world has entered the <a href="https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion">intelligence explosion</a>.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-08-01:/on-ai-software-development-2/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development-2/</link>
                

                
                    <pubDate>Fri, 01 Aug 2025 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development, Vibe Coding Edition</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>The advent of general coding AI assistants almost immediately changed how I think about hiring and interviews.</p>
<p>In the software engineering world, this mindset shift was psychologically easy for me, because I&rsquo;ve always had a bias against the types of coding questions that AI can now answer near-perfectly. And they also happen to be the kind of questions I personally do badly at - the ones requiring troves of knowledge or rote memory of specific language capabilities, libraries, and syntax. It is not so psychologically easy for everyone, especially those who have developed a core skill set of running or passing &ldquo;leetcode-style&rdquo; interviews. Even before AI, the only types of coding questions I would personally ask were things that simply evaluate whether a candidate is lying or not about whether they can code at all, which was and still is surprisingly common. I have interviewed people that list bullet points like 7 years of Java experience but can&rsquo;t pass a fizz-buzz like question, and this was a question I gave out on paper with a closed door and no significant time pressure.</p>
<p>So, when LLMs that could remember any syntax or attribute of any programming language perfectly were released, not only was I excited - I immediately saw that a huge chunk of the programming questions I and many I know have asked in interviews were essentially irrelevant now, not only because people could cheat on interviews, at least virtually, but because this knowledge simply lost much of its value overnight.</p>
<p>Over a few conversations with friends and colleagues I began to explore the idea of what this meant generally for the interview process. There are just lots of questions that we ask in every field, it turns out, that are mostly solved by LLMs today. These models have memorized most useful information that lets them ace simple interviewing questions across fields, even if the original intent of the question was to test for experience.</p>
<h2 id="the-build">The Build</h2>
<p>In the summer of 2022 my ideas and conversations on this topic had gotten to the point where I really just needed to test my hypothesis: LLMs and continuous audio transcription could let someone with no knowledge answer many interview questions correctly. My initial thought was that an app like this must already exist. But after searching for apps on the app stores that did what I was thinking of, to my surprise, I found none did.</p>
<p>I&rsquo;m still not sure if this was a legal thing at the time, or if it&rsquo;s hard to get apps that continuously transcribe audio published, but as of 2025 apps like this definitely exist. Some of them have gotten famous and one has gotten its creator expelled from an Ivy League for revealing that he used it to ace interviews with some top tech companies. Link for the curious here:</p>
<p><a href="https://cluely.com/">https://cluely.com/</a></p>
<p>But, in mid 2023, these apps were apparently not a thing, so I decided to make a prototype.</p>
<p>My basic requirements were simply something that could continuously transcribe words being spoken in a meeting or over a call, group them up into meaningfully long chunks, and then send those chunks with some overlap to two different AI passes:</p>
<ol>
<li>An AI pass that would try to make meaningful questions out of the transcribed potential gibberish</li>
<li>An AI pass that would answer those questions</li>
</ol>
<p>My tech stack for this was a little weird, but I know Unity well and I don&rsquo;t know other ways of deploying native mobile apps as well, and this definitely needed to be a mobile app if it was going to sit on the phone and continuously transcribe audio. Web has all kinds of restrictions on its APIs and I hadn&rsquo;t made a web app like this anyways.</p>
<p>This was surprisingly easy to achieve, even in 2023. I ran into a few hiccups mainly around continuous audio transcription, but for an app I wasn&rsquo;t going to publish and that I was directly putting onto my own Android device, I got around these difficulties by simply starting up a new audio transcription thread every time one closed.</p>
<div style="display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 1rem;">
    





























    



    



    





    







    



    























<img  alt="the UI"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/make-us-smarter.jpg"   style="height: auto; max-width: 200px"   >


    
        <span style="font-style: italic;">Super barebones UI just showing the continuously auto-transcribed words, questions derived from those words, and answers to those questions.  This particular screen was grabbed long after my API key had expired and is only here to show the basic output of the app, transcription building continuously in the background and detected questions and answers in the foreground.</span>
    
</div>

<p>And the results were surprisingly compelling. Of course I was using some of the very first versions of GPT-4 and AI is still not perfect, but the main result of this was that occasionally questions were picked up that were not actually implied by the meeting audio, and occasionally real questions were missed. The part that I knew was going to work did indeed work incredibly well: when I simulated some fizz-buzz style questions and there were no major audio transcription issues, the second question-answering AI nailed them and was able to put a succinct script to answer the question on screen within a few seconds.</p>
<p>There was clearly more work to be done on UI and also the flow between the AI passes, and more agentic APIs of today could definitely do this all more seamlessly.</p>
<p>But for me, my question was answered: My hunch was right and we should definitely not be asking questions about basic constructs of programming languages or simple scripts in interviews anymore.</p>
<p>I open-sourced the project which is a pretty small Unity build, and it&rsquo;s a Unity version from a couple of years ago now, but anyone is welcome to look through and modify the code any way they want:</p>
<p><a href="https://github.com/hockenmaier/make-us-smarter">https://github.com/hockenmaier/make-us-smarter</a></p>
<h2 id="interviewing-mitigations">Interviewing Mitigations</h2>
<p>This whole experience has led me to an interview approach that I think is infallible (for now). And it doesn&rsquo;t require sending someone home with a project or any of the stuff that great candidates often don&rsquo;t even consider. I heard about a version of this technique on Twitter, so can&rsquo;t take credit here:</p>
<p>First: ask candidates to bring some code they have written, regardless of language or framework. Then simply walk through it with them in the interview. asking them questions about why they made certain decisions and trying to guide the conversation to parts that are technically interesting. It only takes 15 minutes or so, and it usually gets much better conversation going than sample interview questions do. This leans on the fact that you need an interviewer who can mostly understand most programming projects, but it cannot be faked with any LLM assistance. LLM-written code is typically pretty obvious: much better commented and differently organized than most humans would write. But even if the code was very sneakily written AI code the person didn&rsquo;t actually contribute to, then having a human go through and explain the parts they thought were clever defeats the purpose of cheating with AI anyway.</p>
<p>This is just a little tidbit of a technique that works well today, if the goal is to assess coding skills. Of course, it leaves some obvious lingering questions about what we are evaluating and why. I hope no one out there that I know is using these apps to cheat on interviews, but we all need to be wise to the fact that it is trivially easy to do so in 2025, and we should shift focus to testing for the qualities that actually matter in the era of AI - or at the very least techniques that prevent the types of cheating possible today.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-06-29:/my-experiments-with-ai-cheating/</guid>

                
                    <link>http://localhost:1313/my-experiments-with-ai-cheating/</link>
                

                
                    <pubDate>Sun, 29 Jun 2025 00:00:00 UTC</pubDate>
                

                
                    <title>My Experiments with AI Cheating</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I built a nice little tool to help AI write code for you.</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/R5wztMBfh0w?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>Well, really, o3-mini and o3-mini-high worked together to write this and I corrected a few things here and there. I started using this tool to write itself about 30 mins into development!</p>
<p>Download on github (above) or the VScode marketplace:</p>
<p><a href="https://marketplace.visualstudio.com/items?itemName=Hockenmaier.context-caddy">https://marketplace.visualstudio.com/items?itemName=Hockenmaier.context-caddy</a></p>
<hr>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2025-02-13:/context-caddy/</guid>

                
                    <link>http://localhost:1313/context-caddy/</link>
                

                
                    <pubDate>Thu, 13 Feb 2025 00:00:00 UTC</pubDate>
                

                
                    <title>Context Caddy</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>Custom GPTs are free for everyone as of yesterday, so I thought I’d post some of the best ones I’ve made over the last few months for all of you:</p>
<p>Proofreader (<a href="https://chatgpt.com/g/g-hjaNCJ8PU-proofreader)">https://chatgpt.com/g/g-hjaNCJ8PU-proofreader)</a>:
This one is super simple. Give it what you’ve written and it will provide no-BS proofreads. It’s not going to hallucinate content, just point out mistakes and parts that don’t make sense.</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="proofreader GPT"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/proofreader-gpt.png"   style="height: auto; max-width: 200px; width: 100%"   >


    
</div>

<p>Make Real (<a href="https://chatgpt.com/g/g-Hw8qvqqey-make-real)">https://chatgpt.com/g/g-Hw8qvqqey-make-real)</a>:
This makes your napkin drawings into working websites. It’s got some of the same limitations other code-generating AI tools do, but it does a surprisingly good job creating simple working web frontends for your ideas!</p>
<p>Postman for PMs (<a href="https://chatgpt.com/g/g-QeNbSmirA-postman-for-pms">https://chatgpt.com/g/g-QeNbSmirA-postman-for-pms</a>)
Talk to APIs using natural language instead of downloading technical tools or writing code (only unauthenticated APIs, for now). Also a great way to learn about APIs for newbies - Postman for PMs knows about some free online APIs to get started with.</p>
<p>The Boy (<a href="https://chatgpt.com/g/g-efYNPIDrz-the-boy">https://chatgpt.com/g/g-efYNPIDrz-the-boy</a>)
An experimental “AI generated RPG” where you play as “The Boy” who realizes fantastic superpowers. It’s fun to play around and explore, but don’t expect too much consistent gameplay from the currently available AI models.</p>
<p>Exciting times. Have fun!</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-06-01:/some-custom-gpts/</guid>

                
                    <link>http://localhost:1313/some-custom-gpts/</link>
                

                
                    <pubDate>Sat, 01 Jun 2024 00:00:00 UTC</pubDate>
                

                
                    <title>Some Custom GPTs</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="this is a robot"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/ai-software-dev.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>Lots of chatter right now about AI replacing software developers.</p>
<p>I agree - AI will take over software development. The question is: what work will be left when this happens?</p>
<p>Some considerations:</p>
<ul>
<li>Benchmarks for the best LLMs still put them solidly in the &ldquo;bad at programming&rdquo; category, scoring in the 5th percentile of human programmers on common tests. Meanwhile, LLMs score in the 80th-95th percentile for law exams and 85th–100th for psychology, statistics, and many other less technical fields. More scores available in the &ldquo;simulated exams&rdquo; section of <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>.</li>
<li>Engineers have been using language models like tabnine and copilot as &ldquo;super-stackoverflow&rdquo; style code assistance years before chatGPT released. This means much of the velocity increase we might expect from current LLMs&rsquo; ability to write code has already been &ldquo;priced in&rdquo; to the market.</li>
<li>Many of the trends making software development more costly are growing, not shrinking: Systems are becoming more distributed. The cloud lowered infrastructure costs but made applications more complex. We&rsquo;re making more and deeper integrations among disparate systems. Auth is becoming more secure and thus complex (managed identity, MFA, etc).</li>
</ul>
<p>Github copilot chat and other LLM dev tools are speeding up the rote stuff. I’ve seen it in my own work.</p>
<p>And I really do believe new AI models will do more than just the basics, maybe in the next couple of years. Even precluding &ldquo;AGI&rdquo;, the trend we are on is that more and more work is automatable, and engineers, especially more junior ones - are going to have to shift focus away from algorithmic work that AI can do.</p>
<p>But by the time our neural nets are &ldquo;good enough&rdquo; at building software to make it significantly cheaper to build, I doubt this trend will make the news. Everything else gets automated too.</p>
<p>These are my thoughts at what seems to be the beginning of the next AI revolution in early 2024. I plan to revisit this topic and see if I&rsquo;m right in future posts.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-01-24:/on-ai-software-development/</guid>

                
                    <link>http://localhost:1313/on-ai-software-development/</link>
                

                
                    <pubDate>Wed, 24 Jan 2024 00:00:00 UTC</pubDate>
                

                
                    <title>On AI Software Development</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I made &ldquo;Postman for PMs,&rdquo; a tool to help non-engineers understand and use APIs!</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/3O4r_q2Ioko?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>It&rsquo;s a &ldquo;Custom GPT&rdquo; - a customized version of chatGPT. Just give it some details about the API and then tell it in English what you want to get, post, update, whatever.</p>
<p>If you&rsquo;re a PM, business analyst, or anyone that cares about APIs but doesn&rsquo;t like terminals and engineer-y tools like Postman, and you have ChatGPT plus, try it out. Here&rsquo;s a link:
<a href="https://chatgpt.com/g/g-QeNbSmirA-postman-for-pms">https://chatgpt.com/g/g-QeNbSmirA-postman-for-pms</a></p>
<p>Important disclaimer: DON&rsquo;T use ChatGPT on corporate stuff if your company doesn&rsquo;t allow it! This was a fun experiment for me and I&rsquo;m definitely not using any corporate resources on it/for it. There are plenty of free APIs to try this out on. Maybe ask ChatGPT for some suggestions</p>
<hr>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2024-01-13:/postman-for-pms/</guid>

                
                    <link>http://localhost:1313/postman-for-pms/</link>
                

                
                    <pubDate>Sat, 13 Jan 2024 00:00:00 UTC</pubDate>
                

                
                    <title>Postman for PMs</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have been occasionally challenging GPT to create models using <a href="https://openscad.org/">OpenSCAD</a>, a &ldquo;programming language for 3D models&rdquo;</p>
<p>Both struggle, but GPT-4 has been a massive improvement. Here are both models&rsquo; outputs after asking for an acorn and 3 messages of me giving feedback:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="some weird acorns"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="http://localhost:1313/images/gpt-acorn.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>For the record, it is impressive that these LLMs can get anything right with no visual input or training on shapes like these. Imagine looking at the programming reference for openSCAD and trying to do this blind. The fact that the 3.5 version has a bunch of strangely intersecting primitives and some union issues has been normal in my experience. It takes quite a bit of spatial logic to get a model not to look like that.</p>
]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2023-03-19:/3d-modeling-with-ai/</guid>

                
                    <link>http://localhost:1313/3d-modeling-with-ai/</link>
                

                
                    <pubDate>Sun, 19 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>3D Modeling With AI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I&rsquo;m writing this post retrospectively as I never published it at the time of creation. It will live here as a &ldquo;stake in the ground&rdquo; of AI software capabilities as of March 2023. Note- if you&rsquo;re reading on substack, this post won&rsquo;t work. Go to <a href="hockenworks.com/gpt-4-solar-system/">hockenworks.com/gpt-4-solar-system</a>.</p>
<p>The interactive solar system below was created with minimal help from me, by the very first version of GPT-4, before even function calling was a feature. It was the first of an ongoing series of experiments to see what frontier models could do by themselves - and I&rsquo;m posting it here because it was the earliest example I saved.</p>
<p>Here&rsquo;s a link to the chat where it was created, though it&rsquo;s not possible to continue this conversation directly since the model involved has long since been deprecated: <a href="https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387">https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387</a></p>

<div style="width:100%; max-width:1000px; margin:1em auto; position:relative; padding-top:70%;">
  <iframe
    class="lazy-iframe"
    data-src="/html/solar-system-self-contained.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">GPT-4 only wrote this for desktop, sorry phone users</span>
    
</div>

<p><strong>Controls</strong></p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>If you get lost, reload the page. That&rsquo;s an edge case GPT-4 didn&rsquo;t account for :)</p>
<p>Here was the initial prompt:</p>
<blockquote>
<p>This might be a long output, so if you need to break and I&rsquo;ll ask you to continue in another message feel free to do that. But please limit any non-code text prose to only essential statements to help mitigate this</p>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.</p>
</blockquote>
<p>Then I had to give it 4 more short prompts, one for a technical hint (to use html-5 since it was going a strange direction) and 3 for visual and mouse control misses.</p>
<p>It works - but missed some of the relatively simple directions, like the planet stats and rendering/controls for mobile. Still, I think it&rsquo;s cool to see the true scale of the planets on a zoomable canvas. And, it only goes to Neptune, the last true planet&hellip; don&rsquo;t go looking for Pluto.</p>
<p>For March 2023, this result was revolutionary - I was truly impressed. In 2025, it&rsquo;s not very impressive at all. How quickly we get used to progress!</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2023-03-18:/gpt-4-solar-system/</guid>

                
                    <link>http://localhost:1313/gpt-4-solar-system/</link>
                

                
                    <pubDate>Sat, 18 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>GPT-4 Solar System</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I’ve been playing around with <a href="https://en.wikipedia.org/wiki/Neural_radiance_field">neural radiance fields</a> (NeRFs) lately and thought a fun way to explore them would be flying through them in the Treekeepers “Puddle Jumper” in true scale.</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/QguH3aK90Ck?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>Of course, you lose a lot of the draw of NeRFs when you export the model into a 3d engine because it has to flatten all the textures and lighting, and also Luma AI cuts off 3D model exports as a jarring cube</p>
<p>But still - I was amazed at how well just applying a day/night lighting cycle and mesh colliders worked with this. Projectile and enemy physics played well too.</p>
<p>It’s still early days, but I could see 3D model generation from this tech getting a lot better and forming the basis for some really interesting user-generated content in the future!</p>
<p>Neat stuff - big thanks to Luma AI for the free toolset.</p>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2023-02-01:/nerfs/</guid>

                
                    <link>http://localhost:1313/nerfs/</link>
                

                
                    <pubDate>Wed, 01 Feb 2023 00:00:00 UTC</pubDate>
                

                
                    <title>NeRFs in VR</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p><strong>Raspberry Pi Control Panel</strong> is a hardware project I designed in 2016 to manage home automation systems. The project involved designing a custom 3D-printed case for a Raspberry Pi microcomputer with a touchscreen interface.</p>
<p>Links:</p>
<ul>
<li><a href="https://github.com/hockenmaier/RaspberryPiControlPanel">GitHub</a></li>
<li><a href="https://www.thingiverse.com/thing:2524560">Thingiverse</a></li>
</ul>
<hr>
<p>I created this panel display in 2016 to control much of the home automation I used in my Studio City apartment. Mainly a hardware project, I designed and 3D-printed a case and frame for the touchscreen and raspberry pi microcomputer in order to mount them to the wall. The software running the control panel is SaaS, but I did write a custom html wrapper to control the orientation and settings of the site, which is available on the github linked above.</p>
<p>Update in 2025: This panel is still my main view into my home automation in my new house in Sherman Oaks, almost 10 years in with no modification!</p>
<p>Here&rsquo;s a video to see the panel in action:</p>
<h2 id="hahahugoshortcode29s0hbhb">
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/iFGmm-ijJvE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>
</h2>
<p>Feel free to explore the linked repositories for schematics and source code.</p>
<h2 id="instructions">Instructions</h2>
<p>If you want to make this, all you need to do is set up a raspberry pi, download chromium (or your preferred web browser), and navigate to your action tiles panel.</p>
<p>If you want to mount the screen vertically like mine, then I have made an easier solution than going through the trouble of actually rotating the raspberry&rsquo;s display and touch device. Just use the html below and edit it to use your own panel&rsquo;s URL in the &ldquo;iframe&rdquo; element instead of mine. This will launch the panel rotated in your browser.</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>&lt;!DOCTYPE html&gt;
</span></span><span style="display:flex;"><span>&lt;html&gt;
</span></span><span style="display:flex;"><span>  &lt;head&gt;
</span></span><span style="display:flex;"><span>    &lt;title&gt;Rotated Raspberry Panel&lt;/title&gt;
</span></span><span style="display:flex;"><span>	&lt;style type=&#34;text/css&#34;&gt;
</span></span><span style="display:flex;"><span>		body {
</span></span><span style="display:flex;"><span>		   -webkit-transform: rotate(90deg);
</span></span><span style="display:flex;"><span>		   -webkit-transform-origin: bottom left;
</span></span><span style="display:flex;"><span>		   position: absolute;
</span></span><span style="display:flex;"><span>		   top: -100vw;
</span></span><span style="display:flex;"><span>		   height: 100vw;
</span></span><span style="display:flex;"><span>		   width: 100vh;
</span></span><span style="display:flex;"><span>		   background-color: #000;
</span></span><span style="display:flex;"><span>		   color: #fff;
</span></span><span style="display:flex;"><span>		   overflow: hidden;&#34;
</span></span><span style="display:flex;"><span>		}
</span></span><span style="display:flex;"><span>	   iframe{
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			-ms-transform: scale(0.97);
</span></span><span style="display:flex;"><span>			-moz-transform: scale(0.97);
</span></span><span style="display:flex;"><span>			-o-transform: scale(0.97);
</span></span><span style="display:flex;"><span>			-webkit-transform: scale(0.97);
</span></span><span style="display:flex;"><span>			transform: scale(0.97);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>			-ms-transform-origin: 0 0;
</span></span><span style="display:flex;"><span>			-moz-transform-origin: 0 0;
</span></span><span style="display:flex;"><span>			-o-transform-origin: 0 0;
</span></span><span style="display:flex;"><span>			-webkit-transform-origin: 0 0;
</span></span><span style="display:flex;"><span>			transform-origin: 0 0;
</span></span><span style="display:flex;"><span>		}
</span></span><span style="display:flex;"><span>	&lt;/style&gt;
</span></span><span style="display:flex;"><span>  &lt;/head&gt;
</span></span><span style="display:flex;"><span>  &lt;body&gt;
</span></span><span style="display:flex;"><span>	&lt;iframe src=&#34;https://app.actiontiles.com/panel/f7a7118c-236b-4144-b5b9-ccb35abeef21&#34; height=&#34;300%&#34; width=&#34;300%&#34; frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;
</span></span><span style="display:flex;"><span>  &lt;/body&gt;
</span></span><span style="display:flex;"><span>&lt;/html&gt;
</span></span></code></pre></div><p>Link to buy the screen:
<a href="https://smile.amazon.com/gp/product/B01ID5BQTC/">https://smile.amazon.com/gp/product/B01ID5BQTC/</a></p>
<p>Link to the Action Tiles web application this is running:
<a href="https://www.actiontiles.com/">https://www.actiontiles.com/</a></p>
<p>If you have issues getting your pi to use the full touchscreen width, try adding these setting to the /boot/config.txt file and reboot:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>max_usb_current=1
</span></span><span style="display:flex;"><span>hdmi_group=2
</span></span><span style="display:flex;"><span>hdmi_mode=1
</span></span><span style="display:flex;"><span>hdmi_mode=87
</span></span><span style="display:flex;"><span>hdmi_cvt 800 480 60 6 0 0 0
</span></span></code></pre></div><p>If you want to make sure your screen doesn&rsquo;t go to sleep:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo nano /etc/lightdm/lightdm.conf
</span></span></code></pre></div><p>Add the following lines to the [SeatDefaults] section:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>xserver-command=X -s 0 dpms
</span></span></code></pre></div>]]></description>
                

                <guid isPermaLink="false">tag:localhost:1313,2016-01-01:/raspberry-pi-panel/</guid>

                
                    <link>http://localhost:1313/raspberry-pi-panel/</link>
                

                
                    <pubDate>Fri, 01 Jan 2016 00:00:00 UTC</pubDate>
                

                
                    <title>Raspberry Pi Control Panel</title>
                
            </item>
        
    </channel>
</rss>
