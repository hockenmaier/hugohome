













    
        
    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    

    

    
        
    







    

    






<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"  xml:lang="en-us"  xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        
            

            
                
            

            
                <link href="https://hockenworks.com/tags/experimental/" rel="self" type="text/html"/>
            
        
            

            

            
                <link href="https://hockenworks.com/tags/experimental/rss.xml" rel="alternate" type="application/rss+xml"/>
            
        

        

        

        <description>Recent content</description>

        
            <language>en-us</language>
        

        
            <lastBuildDate>2025-08-24 00:00:00 +0000 UTC</lastBuildDate>
        

        <link>https://hockenworks.com/tags/experimental/</link>

        

        <title>Experimental · Tags · hockenworks</title>

        

        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<blockquote>
<p>Warning: This article has a lot of embedded code, so <a href="/this-website/#ball-machine---the-game">the ball machine</a> is slow unless you have a REALLY fast computer. Play at your own risk.</p>
</blockquote>
<p>Seriously, though.</p>
<p>GPT-5, as a simple text completion model, is not a revelation.</p>
<p>This isn&rsquo;t so surprising. It was becoming clearer with every new raw LLM release that the fundamental improvements from scaling solely the performance of the core text predictor were starting to show diminishing returns. But I&rsquo;m going to make an argument today that, although the LLM itself is not nearly as much of a leap from GPT-4 as GPT-4 was from GPT-3, we have still seen at least a whole-version-number of real improvement between the release of GPT-4 and 5 as we did between 3 and 4. The reasons for that are mostly what exists around that LLM core.</p>
<p>And I&rsquo;m going to make this argument by showing you things the original GPT-4 could never have done.</p>
<h1 id="exhibit-a-a-new-solar-system">Exhibit A: A New Solar System</h1>
<p>GPT-5 came at a great time for me, because when OpenAI set the announcement to 8/7/25, I was in the middle of stress testing Agent Mode with the prior models. So, let me start by just illustrating the progress we had made between the first GPT-4 release in early 2023 and the capabilities of GPT-4 level models the week before the release of GPT-5.</p>
<p>Two years and four months have gone by since I ran my original Interactive Solar System experiment with the original version of GPT-4. You can play with it at <a href="/gpt-4-solar-system">hockenworks.com/gpt-4-solar-system</a>.</p>
<p>Now have a look at the same test performed a week before the release of GPT-5, which uses ChatGPT Agent Mode:</p>
<p>
<style>
  .agent-mode-solar-wrapper {
    position: relative;
    left: 50%;
    transform: translateX(-50%);
    width: 150%;
    max-width: 1500px;
    margin: 1em 0;
    padding-top: 70%;
  }

  @media (max-width: 768px) {
    .agent-mode-solar-wrapper {
      width: 100%;
      left: 0;
      transform: none;
      padding-top: 180%;
    }
  }
</style>

<div class="agent-mode-solar-wrapper">
  <iframe
    class="lazy-iframe"
    data-src="/html/agent-mode-solar-system-self-contained.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
  
    <span style="display: block; font-style: italic; margin-top: 0.5rem;">Works on desktop or mobile</span>
  
</div>

<div style="text-align:center;">
  <a href="/html/agent-mode-solar-system-self-contained.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>Here are controls, again:</p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>And I again used this initial prompt:</p>
<blockquote>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.
 </p>
</blockquote>
<p>Quite an improvement, right?</p>
<p>Like my original experiment, I didn’t one-shot this result. This is the final result after 7 prompts, but unlike in the original, those follow-up prompts were not primarily fixes - they were new features that I thought would be good additions after playing with each previous version - all of which worked right away.</p>
<p>The most striking part of this experiment for me is that, despite the plateau that has largely been observed in base LLMs, this result is clearly far above and beyond what GPT-4 was able to do when it first released, and the reasons for that have to do with what the labs have been building <em>around</em> the base models.</p>
<p>In the case of Agent mode, as far as we know, we have a GPT-4 class model at the root. o3, which is what Agent Mode uses, is a GPT-4 class model with chain-of-thought and lots of self-play training.</p>
<p>It also has the ability to use tools, like the web search tool it used to find the planet facts.</p>
<p>But then the thing that most sets it apart from its base model is the deep research and computer use elements of Agent mode. As it was building this model, I saw Agent mode do things such as perusing the internet with its text browser to find scale information and browse for textures it could use as planet skins.</p>
<p>It wrote and ran test code using its Code Interpreter tool.</p>
<p>It used its chain-of-thought &ldquo;thinking&rdquo; to respond to errors, rethink controls and scale decisions, and decide to go back and search the web or use other tools some more.</p>
<p>Then, the most impressive new ability that allowed it to make this: It ran this code on a local browser and <em>visually tested it</em>, clicking on the planets to make sure the information panels were coming up, zooming and panning around, and using its own toolbars.</p>
<p>This last one to me, when paired with all of the other things that GPT-4 class models are not instrumented to do, is the start of a true general capability for AI software testing, almost as a side note to all of the other things it will unlock in the coming few years. Not just the little unit and automated tests that LLMs have been writing into our codebases for a long time - but actual visual look and feel testing. This is a huge market and will be a huge shock to the software industry when it’s fully realized.</p>
<p>These are all net-new abilities that GPT-4 class models have gained since GPT-4 came out in March of 2023. It&rsquo;s hard to see how much progress these capabilities add up to without just running the direct experiment like this solar system generation.</p>
<h1 id="gpt-5-without-agent-mode">GPT-5 Without Agent Mode</h1>
<p>I see GPT-5 as a formalization of all of these capabilities that lock in the step change in capability we&rsquo;ve seen over the last 2 years. There is a new model underneath there somewhere (some are saying it is o4) and that underlying model has certainly been reinforcement-trained to choose its tools wisely. It is the first model I have seen that can do all of these simultaneously, without user choice:</p>
<ul>
<li>Choose when to think and when to just answer</li>
<li>Run iterative web search calls</li>
<li>Write and run code to do highly logical-symbolic tasks like data analysis</li>
<li>Create, edit, and analyze images</li>
<li>Create and read nearly any kind of document a user might be working with</li>
</ul>
<p>And there are still a couple of things locked behind user choice, which is probably because these both result in much longer running and expensive tasks than simple thinking:</p>
<ul>
<li>Deep map-reduce style web research</li>
<li>Computer use (mouse and keyboard style, with Agent Mode)</li>
</ul>
<p>Using only the first set of &ldquo;default&rdquo; behaviors, GPT-5 can do things that the original GPT-4 could never have dreamed of. I have had the following prompt sitting around in my &ldquo;intelligence tests&rdquo; document for more than a year now under <em>&ldquo;Cool picross app idea, def solvable by competent AI (which doesn&rsquo;t exist yet at the end of 2024)&rdquo;</em>, waiting for a single model that can one-shot it. GPT-5 is the first one that does:</p>
<h2 id="exhibit-b-picross">Exhibit B: Picross</h2>
<blockquote>
<p>Take an image, increase contrast, and turn it into a 15x15 image. Create a black and white picross puzzle out of the image, including a UI that lets a player solve the puzzle.</p>
</blockquote>
<p>It&rsquo;s a simple prompt that implies a lot of underlying complexity. Here is the first one-shot result from GPT-5 I got (normal mode, I didn&rsquo;t select &ldquo;Thinking&rdquo; in advance):</p>
<p><style>
  .picross-wrapper { display:flex; justify-content:center; }
  .picross-wrapper .frame {
    position: relative;
    width: 100%;
    max-width: 900px;
    aspect-ratio: 3.35/4;
    border: 2px solid #fff; border-radius: 4px; overflow: hidden;
  }
  .picross-wrapper .frame > iframe {
    position:absolute; inset:0; width:100%; height:100%;
    transform-origin: top left;
  }
   
  @media (max-width: 480px){
    .picross-wrapper .frame { --s: .65; }
    .picross-wrapper .frame > iframe {
      transform: scale(var(--s,1));
      width: calc(100% / var(--s,1));
      height: calc(100% / var(--s,1));
    }
  }
</style>

<div class="picross-wrapper">
  <div class="frame">
    <iframe
      class="lazy-iframe"
      data-src="/html/picross-generator.html"
      src="about:blank"
      loading="lazy"
    ></iframe>
  </div>
</div>

<div style="text-align:center;">
  <a href="/html/picross-generator.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>It didn&rsquo;t make all the choices I would have, but it worked in one shot, the first time I tried it. All the way from uploading an image and transforming that to a puzzle, to a whole UI that lets you solve it. I purposely left this here after its one-shot result just to demonstrate the progress. You could take this idea much further.</p>
<h2 id="exhibit-c-the-baby-mesmerizer">Exhibit C: The Baby Mesmerizer</h2>
<p>Now for the coolest thing I&rsquo;ve made with GPT-5 so far. I call this one the baby mesmerizer because baby Alice is absolutely stunned every time she sees it. I got this idea from another entertaining little physics simulation I saw somewhere.</p>
<p>I had GPT-5 make this one, then I tested it using the built-in runner in ChatGPT, changed it, and iterated on it 16 times. But I didn&rsquo;t write any of it myself - nor did I even open the code other than to tweak some variables to make it &ldquo;feel right&rdquo; here and there.</p>
<p>
<style>
  .agent-mode-solar-wrapper {
    position: relative;
    left: 50%;
    transform: translateX(-50%);
    width: 150%;
    max-width: 1500px;
    margin: 1em 0;
    padding-top: 85%;
  }

  @media (max-width: 768px) {
    .agent-mode-solar-wrapper {
      width: 100%;
      left: 0;
      transform: none;
      padding-top: 180%;
    }
  }
</style>

<div class="agent-mode-solar-wrapper">
  <iframe
    class="lazy-iframe"
    data-src="/html/exponential_bounce.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
  
</div>

<div style="text-align:center;">
  <a href="/html/exponential_bounce.html" target="_blank" rel="noopener noreferrer"
     style="display:inline-block;padding:.5rem 1rem;border:1px solid #ccc;border-radius:.5rem;text-decoration:none">
    Open In Standalone Tab
  </a>
</div>
</p>
<p>How cool is that?</p>
<p>After you’re done messing around with this, take a detailed look at what has been built here. It is a tech demo, yes, but it has:</p>
<ul>
<li>A nice-looking UI and color scheme</li>
<li>A side menu that dynamically operates as a fly-out menu based on screen size</li>
<li>A bunch of tunable variables, including niceties like minimums and maximums affecting each other</li>
<li>Dynamic generated sound effects</li>
</ul>
<p>I also thought it was interesting that GPT-5, when I said that it needed to run as a single HTML file with no external dependencies, chose to write its own physics for this. There is no prebuilt physics engine at all here.</p>
<p>This result is about 1400 lines of code. This isn’t a huge project, but it is far more than we could get a GPT to reliably produce just a year ago. The typical loop before was that you’d get past 200 lines of code or so, and then every new feature or bugfix requested would break two other things around the codebase, effectively enforcing a tiny complexity cap.</p>
<p>Let&rsquo;s be real: It&rsquo;s absolutely amazing that I could make something as complicated as this, exactly how I imagined it, just by describing it in English and a collective hour or two of testing. And though I like to code and have been bummed for a while that most straightforward coding like this is going the way of the dodo, this experience of not coding and instead just setting requirements and playing with the result was also a lot of fun. More fun than tearing through <a href="/on-ai-software-development-2/#agentic-ides">codespam in a tool like Cursor</a>. And frankly, I would never spend time making something like this if I had to code it all from scratch.</p>
<h1 id="a-plateau">A Plateau?</h1>
<p>I&rsquo;ve already read more than enough takes that GPT-5 signals the end of the current wave of AI, as a sort of intelligence plateau somewhere just below humans.</p>
<p>We should observe that we did not foresee the advancements that would get us from GPT-4 to GPT-5. Yet here we are: GPT-4 was <a href="/gpt-4-solar-system/">barely able to write 200 lines of buggy solar system code</a>, and GPT-5 one-shots it. The core model under the hood is likely a bit better, but it was proper tooling and reinforcement training on that tooling that really made the difference. And from what I hear, there are many other avenues of work that researchers say are still in early stages, such as reinforcement training on long-running agentic tasks and building the synthetic datasets that will allow for that.</p>
<p>So, even though GPT-5 doesn&rsquo;t seem like a huge advancement from models like o3 and Sonnet 4, hindsight makes the upward trajectory clear.</p>
<p>No matter how fast we see progress, there is clearly a ton of fun to have along the way!</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2025-08-24:/gpt5/</guid>

                
                    <link>https://hockenworks.com/gpt5/</link>
                

                
                    <pubDate>Sun, 24 Aug 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Real GPT-5 Was The Friends We Made Along The Way</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>The advent of general coding AI assistants almost immediately changed how I think about hiring and interviews.</p>
<p>In the software engineering world, this mindset shift was psychologically easy for me, because I&rsquo;ve always had a bias against the types of coding questions that AI can now answer near-perfectly. And they also happen to be the kind of questions I personally do badly at - the ones requiring troves of knowledge or rote memory of specific language capabilities, libraries, and syntax. It is not so psychologically easy for everyone, especially those who have developed a core skill set of running or passing &ldquo;leetcode-style&rdquo; interviews. Even before AI, the only types of coding questions I would personally ask were things that simply evaluate whether a candidate is lying or not about whether they can code at all, which was and still is surprisingly common. I have interviewed people that list bullet points like 7 years of Java experience but can&rsquo;t pass a fizz-buzz like question, and this was a question I gave out on paper with a closed door and no significant time pressure.</p>
<p>So, when LLMs that could remember any syntax or attribute of any programming language perfectly were released, not only was I excited - I immediately saw that a huge chunk of the programming questions I and many I know have asked in interviews were essentially irrelevant now, not only because people could cheat on interviews, at least virtually, but because this knowledge simply lost much of its value overnight.</p>
<p>Over a few conversations with friends and colleagues I began to explore the idea of what this meant generally for the interview process. There are just lots of questions that we ask in every field, it turns out, that are mostly solved by LLMs today. These models have memorized most useful information that lets them ace simple interviewing questions across fields, even if the original intent of the question was to test for experience.</p>
<h2 id="the-build">The Build</h2>
<p>In the summer of 2022 my ideas and conversations on this topic had gotten to the point where I really just needed to test my hypothesis: LLMs and continuous audio transcription could let someone with no knowledge answer many interview questions correctly. My initial thought was that an app like this must already exist. But after searching for apps on the app stores that did what I was thinking of, to my surprise, I found none did.</p>
<p>I&rsquo;m still not sure if this was a legal thing at the time, or if it&rsquo;s hard to get apps that continuously transcribe audio published, but as of 2025 apps like this definitely exist. Some of them have gotten famous and one has gotten its creator expelled from an Ivy League for revealing that he used it to ace interviews with some top tech companies. Link for the curious here:</p>
<p><a href="https://cluely.com/">https://cluely.com/</a></p>
<p>But, in mid 2023, these apps were apparently not a thing, so I decided to make a prototype.</p>
<p>My basic requirements were simply something that could continuously transcribe words being spoken in a meeting or over a call, group them up into meaningfully long chunks, and then send those chunks with some overlap to two different AI passes:</p>
<ol>
<li>An AI pass that would try to make meaningful questions out of the transcribed potential gibberish</li>
<li>An AI pass that would answer those questions</li>
</ol>
<p>My tech stack for this was a little weird, but I know Unity well and I don&rsquo;t know other ways of deploying native mobile apps as well, and this definitely needed to be a mobile app if it was going to sit on the phone and continuously transcribe audio. Web has all kinds of restrictions on its APIs and I hadn&rsquo;t made a web app like this anyways.</p>
<p>This was surprisingly easy to achieve, even in 2023. I ran into a few hiccups mainly around continuous audio transcription, but for an app I wasn&rsquo;t going to publish and that I was directly putting onto my own Android device, I got around these difficulties by simply starting up a new audio transcription thread every time one closed.</p>
<div style="display: flex; align-items: center; justify-content: center; gap: 10px; margin-bottom: 1rem;">
    





























    



    



    





    







    



    























<img  alt="the UI"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/make-us-smarter.jpg"   style="height: auto; max-width: 200px"   >


    
        <span style="font-style: italic;">Super barebones UI just showing the continuously auto-transcribed words, questions derived from those words, and answers to those questions.  This particular screen was grabbed long after my API key had expired and is only here to show the basic output of the app, transcription building continuously in the background and detected questions and answers in the foreground.</span>
    
</div>

<p>And the results were surprisingly compelling. Of course I was using some of the very first versions of GPT-4 and AI is still not perfect, but the main result of this was that occasionally questions were picked up that were not actually implied by the meeting audio, and occasionally real questions were missed. The part that I knew was going to work did indeed work incredibly well: when I simulated some fizz-buzz style questions and there were no major audio transcription issues, the second question-answering AI nailed them and was able to put a succinct script to answer the question on screen within a few seconds.</p>
<p>There was clearly more work to be done on UI and also the flow between the AI passes, and more agentic APIs of today could definitely do this all more seamlessly.</p>
<p>But for me, my question was answered: My hunch was right and we should definitely not be asking questions about basic constructs of programming languages or simple scripts in interviews anymore.</p>
<p>I open-sourced the project which is a pretty small Unity build, and it&rsquo;s a Unity version from a couple of years ago now, but anyone is welcome to look through and modify the code any way they want:</p>
<p><a href="https://github.com/hockenmaier/make-us-smarter">https://github.com/hockenmaier/make-us-smarter</a></p>
<h2 id="interviewing-mitigations">Interviewing Mitigations</h2>
<p>This whole experience has led me to an interview approach that I think is infallible (for now). And it doesn&rsquo;t require sending someone home with a project or any of the stuff that great candidates often don&rsquo;t even consider. I heard about a version of this technique on Twitter, so can&rsquo;t take credit here:</p>
<p>First: ask candidates to bring some code they have written, regardless of language or framework. Then simply walk through it with them in the interview. asking them questions about why they made certain decisions and trying to guide the conversation to parts that are technically interesting. It only takes 15 minutes or so, and it usually gets much better conversation going than sample interview questions do. This leans on the fact that you need an interviewer who can mostly understand most programming projects, but it cannot be faked with any LLM assistance. LLM-written code is typically pretty obvious: much better commented and differently organized than most humans would write. But even if the code was very sneakily written AI code the person didn&rsquo;t actually contribute to, then having a human go through and explain the parts they thought were clever defeats the purpose of cheating with AI anyway.</p>
<p>This is just a little tidbit of a technique that works well today, if the goal is to assess coding skills. Of course, it leaves some obvious lingering questions about what we are evaluating and why. I hope no one out there that I know is using these apps to cheat on interviews, but we all need to be wise to the fact that it is trivially easy to do so in 2025, and we should shift focus to testing for the qualities that actually matter in the era of AI - or at the very least techniques that prevent the types of cheating possible today.</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2025-06-29:/my-experiments-with-ai-cheating/</guid>

                
                    <link>https://hockenworks.com/my-experiments-with-ai-cheating/</link>
                

                
                    <pubDate>Sun, 29 Jun 2025 00:00:00 UTC</pubDate>
                

                
                    <title>My Experiments with AI Cheating</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>&ldquo;The Flapper&rdquo; is a project that spawned out of a simple VR movement mechanic test that I had in my head for a while, which turned out to be surprisingly fun! The idea is to flap your arms to fly - wrapped up as a multiplayer battle to really get people moving.</p>
<p>In order to start working on this game, because there was so much standard VR code that I had to write for <a href="/treekeepers-vr">Treekeepers</a>, I decided to make a sort of engine out of the Treekeepers codebase and work off of that rather than start from scratch. That let me tie in some of the nice associated graphics, music and sound effects I had made, and a bunch of other helper functions and tools I use for things like the camera following around the character, how I deal with collisions, a bunch of netcode, etc.</p>
<p>You can see my more detailed post about that engine here: <a href="/treekeepers-engine">Treekeepers Engine</a></p>
<h2 id="core-mechanic--gameplay">Core Mechanic &amp; Gameplay</h2>
<p>Most of the start of this game was just tuning the movement mechanic, which borrowed from some physics realities and some elements I made up to make flapping feel good. But, the essential idea was that each arm generates unique thrust in the direction it moves with an exponential applied to its speed. It&rsquo;s hard to describe any native VR mechanic with words and videos only, but to me and the folks I demoed it to, it felt &ldquo;right&rdquo; for how flying should work if you did it by flapping your arms - and that feeling is based in the physical reality of wing-powered flight. I had a ton of fun just jetting around the obstacle courses I made for myself.</p>
<p>My idea for this other than just the mechanic was to make a sort of gorilla tag-esque multiplayer game where players would fly around and try to pop each other&rsquo;s balloons in an NES balloon fight {link} style. Ideally something like 15 to 20 people would be in a lobby flying around and trying to pop each other.</p>
<p>Like gorilla tag I didn&rsquo;t want anyone to have to &ldquo;sit out&rdquo; of the game, so it&rsquo;s essentially a deathmatch where the player who pops the most balloons wins, and is also visible who&rsquo;s winning, because they also gain the balloons that they pop. In some playtests players would have 20 or 30 balloons on their heads. This was my clever idea of adding a built-in rubber-band effect to the gameplay as well, since having more balloons over your head made you a bigger target to pop. The gameplay worked well - but I never quite got the game to a place with netcode and networking engine where multiplayer felt seamless enough.</p>
<p>Here’s a video of one of the later states of the game, where I have it fully networked and am testing with friends, though it still has a few bugs here:</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/Vxn8rDOZ7dU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<h2 id="today">Today</h2>
<p>I stopped working on this project after about 3 months. It turned out that flying alone with this mechanic was very compelling (and also a great workout!) but the networking engine I had used for Treekeepers, Photon, was not up to the task for how low latency a competitive game needed to be. Treekeepers was four-player co-op so Photon was just fine.</p>
<p>In the future I might pick this one back up (or maybe have an AI agent pick it up for me depending on how that goes) using <a href="https://spacetimedb.com/">space-time DB</a> which looks like a great solution for this type of game that doesn&rsquo;t require a whole ton of cloud programming and setup</p>
<p>I haven&rsquo;t made a build of this game public yet due to its unfinished and multiplayer nature - it&rsquo;s not set up with a usable server other than for testing. If I receive interest in playing it from enough people, I&rsquo;ll go back in and package up the single player parts as a tech demo and put a download here.</p>
<p>I learned a lot from this project, both in terms of game code organization and game mechanic design, and I still think it&rsquo;s a great concept. I hope this movement mechanic becomes the basis for a full game in the future and with any luck with the direction software development AI is going I might get that opportunity sooner vs later. About 3 weeks from now, I will be releasing an article on the modern state of AI software development, including a deep dive on some of the latest tools for web and game development, so stay tuned!</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2025-06-23:/the-flapper/</guid>

                
                    <link>https://hockenworks.com/the-flapper/</link>
                

                
                    <pubDate>Mon, 23 Jun 2025 00:00:00 UTC</pubDate>
                

                
                    <title>The Flapper - A Physical VR Multiplayer Game</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I have been occasionally challenging GPT to create models using <a href="https://openscad.org/">OpenSCAD</a>, a &ldquo;programming language for 3D models&rdquo;</p>
<p>Both struggle, but GPT-4 has been a massive improvement. Here are both models&rsquo; outputs after asking for an acorn and 3 messages of me giving feedback:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="some weird acorns"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/gpt-acorn.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>For the record, it is impressive that these LLMs can get anything right with no visual input or training on shapes like these. Imagine looking at the programming reference for openSCAD and trying to do this blind. The fact that the 3.5 version has a bunch of strangely intersecting primitives and some union issues has been normal in my experience. It takes quite a bit of spatial logic to get a model not to look like that.</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2023-03-19:/3d-modeling-with-ai/</guid>

                
                    <link>https://hockenworks.com/3d-modeling-with-ai/</link>
                

                
                    <pubDate>Sun, 19 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>3D Modeling With AI</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I&rsquo;m writing this post retrospectively as I never published it at the time of creation. It will live here as a &ldquo;stake in the ground&rdquo; of AI software capabilities as of March 2023. Note- if you&rsquo;re reading on substack, this post won&rsquo;t work. Go to <a href="hockenworks.com/gpt-4-solar-system/">hockenworks.com/gpt-4-solar-system</a>.</p>
<p>The interactive solar system below was created with minimal help from me, by the very first version of GPT-4, before even function calling was a feature. It was the first of an ongoing series of experiments to see what frontier models could do by themselves - and I&rsquo;m posting it here because it was the earliest example I saved.</p>
<p>Here&rsquo;s a link to the chat where it was created, though it&rsquo;s not possible to continue this conversation directly since the model involved has long since been deprecated: <a href="https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387">https://chatgpt.com/share/683b5680-8ac8-8006-9493-37add8749387</a></p>

<div style="width:100%; max-width:1000px; margin:1em auto; position:relative; padding-top:70%;">
  <iframe
    class="lazy-iframe"
    data-src="/html/solar-system-self-contained.html"
    src="about:blank"
    loading="lazy"
    style="position:absolute; top:0; left:0; width:100%; height:100%; border:1px solid #ccc;"
  ></iframe>
</div>

<div style="text-align: center; margin-bottom: 1rem;">
    
        <span style="display: block; font-style: italic; margin-top: 0.5rem;">GPT-4 only wrote this for desktop, sorry phone users</span>
    
</div>

<p><strong>Controls</strong></p>
<p><kbd>Mouse Click + Drag</kbd> to move the solar system around</p>
<p><kbd>Mouse Wheel Up</kbd> to zoom in from the cursor location</p>
<p><kbd>Mouse Wheel Down</kbd> to zoom out</p>
<p>If you get lost, reload the page. That&rsquo;s an edge case GPT-4 didn&rsquo;t account for :)</p>
<p>Here was the initial prompt:</p>
<blockquote>
<p>This might be a long output, so if you need to break and I&rsquo;ll ask you to continue in another message feel free to do that. But please limit any non-code text prose to only essential statements to help mitigate this</p>
<p>I want you to make me a dynamic website. It should look good on mobile or on desktop, and I would like you to pick a nice dark background color and an interesting font to use across the page.</p>
<p>The page is intended to show the scale of the solar system in an interactive way, primarily designed for children to zoom in and out of different parts of the solar system and see planets and the Sun in relative scale. Mouse controls should also include panning around the model solar system, and should include text around planets with some statistics about their size, gravity, atmospheres, and any other fun facts you think would be educational and fun for 10-12 year olds.</p>
</blockquote>
<p>Then I had to give it 4 more short prompts, one for a technical hint (to use html-5 since it was going a strange direction) and 3 for visual and mouse control misses.</p>
<p>It works - but missed some of the relatively simple directions, like the planet stats and rendering/controls for mobile. Still, I think it&rsquo;s cool to see the true scale of the planets on a zoomable canvas. And, it only goes to Neptune, the last true planet&hellip; don&rsquo;t go looking for Pluto.</p>
<p>For March 2023, this result was revolutionary - I was truly impressed. In 2025, it&rsquo;s not very impressive at all. How quickly we get used to progress!</p>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2023-03-18:/gpt-4-solar-system/</guid>

                
                    <link>https://hockenworks.com/gpt-4-solar-system/</link>
                

                
                    <pubDate>Sat, 18 Mar 2023 00:00:00 UTC</pubDate>
                

                
                    <title>GPT-4 Solar System</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I’ve been playing around with <a href="https://en.wikipedia.org/wiki/Neural_radiance_field">neural radiance fields</a> (NeRFs) lately and thought a fun way to explore them would be flying through them in the Treekeepers “Puddle Jumper” in true scale.</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/QguH3aK90Ck?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>Of course, you lose a lot of the draw of NeRFs when you export the model into a 3d engine because it has to flatten all the textures and lighting, and also Luma AI cuts off 3D model exports as a jarring cube</p>
<p>But still - I was amazed at how well just applying a day/night lighting cycle and mesh colliders worked with this. Projectile and enemy physics played well too.</p>
<p>It’s still early days, but I could see 3D model generation from this tech getting a lot better and forming the basis for some really interesting user-generated content in the future!</p>
<p>Neat stuff - big thanks to Luma AI for the free toolset.</p>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2023-02-01:/nerfs/</guid>

                
                    <link>https://hockenworks.com/nerfs/</link>
                

                
                    <pubDate>Wed, 01 Feb 2023 00:00:00 UTC</pubDate>
                

                
                    <title>NeRFs in VR</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>The &ldquo;Human Joystick&rdquo; is an experimental VR movement system in which the player moves through the virtual environment by changing their physical location within their VR &ldquo;playspace&rdquo;.</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/q_1itpdiPb4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<p>A demo of the human joystick movement system, showing how the system can work on flat surfaces or terrain.</p>
<p>This was my first barebones VR project. Though I knew Unity going in, VR and 3D games in general have a lot of unique aspects that I wanted to learn about while trying to solve an actual problem, rather than following tutorials or demos online.</p>
<p>VR has some adoption problems in its current state. We all know of some of the main problems- the clunky headset, the nausea issues, and of course the pricetag. But one major problem that you don&rsquo;t really notice until you get into it, is the lack of a good solution for virtual movement.</p>
<p>I had been wondering about &ldquo;the human joystick&rdquo; as a potential a solution to this particular problem ever since getting into consumer VR in 2016.</p>
<p>In most modern VR systems, the player can move physically around the room if they choose. Some applications and games depend on this - they put you in a small space and rely on your physical movement in order to reach different areas and interact with things. But games that provide a more traditional sense of scale and allow players to move through large worlds cannot rely on physical motion, because their users are constrained by physical space. Because of this, you see all kinds of &ldquo;artificial&rdquo; locomotion systems in order to let people move around - some just like traditional 2D games that let users &ldquo;slide&rdquo; their playspaces around the world using a joystick, and others that adopt teleportation mechanics. Neither feel very natural as compared to actually walking, and some can be downright sickening.</p>
<p>My goal with this project was to solve this problem with a mixture of physical and artificial movement.</p>
<p>It works like this: When the player is standing near the center of their playspace, physical VR movement applies. The player can move around and interact with things with their actual bodies. But once the player moves further from the center, the plaspace starts to move with them in the same direction as the vector from the center of the player&rsquo;s space to their current position. This allows for some of the benefits that physical movement experiences have, while allowing the players to more naturally move through an infinite amount of space.</p>
<p>I experimented with several speeds, both static and scaling with the distance between the center and the player. I also experimented with the size of the physical movement &ldquo;deadzone&rdquo; and with vertical and constrained movement across hills, valleys, and buildings.</p>
<hr>
<table>
  <thead>
      <tr>
          <th style="text-align: center">


















<div class="paige-image">
    





























    



    

    
        

        

        
    
        

        

        
    
        

        
            

    



    







    





    



    























<img   class="img-fluid "  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/human_joystick_centered.jpg"   style="display:block; height: auto; margin:0 auto; width: 60%"   >


</div>
</th>
          <th style="text-align: left"><em>View from the player&rsquo;s perspective looking at the guides at his feet. With the white dot in the red deadzone, the player isn&rsquo;t moving.</em></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">


















<div class="paige-image">
    





























    



    

    
        

        

        
    
        

        

        
    
        

        
            

    



    







    





    



    























<img   class="img-fluid "  crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/human_joystick_moving.jpg"   style="display:block; height: auto; margin:0 auto; width: 60%"   >


</div>
</td>
          <td style="text-align: left"><strong><em>When the white dot is in the green area, the player moves in that direction. Here I am moving forward and left at about half of max speed.</em></strong></td>
      </tr>
  </tbody>
</table>
<hr>
<p>Eventually I found some good default values and the system worked, but there were some unforeseen problems: First, it was more difficult to center yourself within the playspace without looking at the visible guides I put at the player&rsquo;s feet than I expected. Second and more importantly, when you were already moving in one direction, it was not as simple as I thought to start moving in another direction accurately without fully returning to center, which was an immersion breaker.</p>
<p>Ultimately I put the project up for others to view but have not expanded it into a full experience or released it on any marketplaces. Feel free to download the Unity project and try it on your own VR setup if you&rsquo;re curious.</p>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2020-01-01:/human-joystick/</guid>

                
                    <link>https://hockenworks.com/human-joystick/</link>
                

                
                    <pubDate>Wed, 01 Jan 2020 00:00:00 UTC</pubDate>
                

                
                    <title>Human Joystick VR</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>The Answering Machine is a proof-of-concept system that I built using <strong>pre-LLM</strong> natural language processing (NLP), specifically NLTK, to produce answers to questions asked about data in plain English.</p>
<p>Looking back, this project was a great insight into what LLMs immediately allowed that was incredibly difficult before. This project was several months of work that the openAI sdk would probably have allowed in a few weeks - and that few weeks would have been mostly frontend design and a bit of prompting.</p>
<p><strong>Try it here:</strong> <a href="http://voicequery-dev.s3-website-us-west-2.amazonaws.com/">http://voicequery-dev.s3-website-us-west-2.amazonaws.com/</a>
<strong>Github:</strong> <a href="https://github.com/hockenmaier/voicequery">https://github.com/hockenmaier/voicequery</a></p>
<p>The system uses natural language processing to produce answers to questions asked about data in plain English.</p>
<p>It is designed with simplicity in mind—upload any columnar dataset and start asking questions and getting answers. It uses advanced NLP algorithms to make assumptions about what data you&rsquo;re asking about and lets you correct those assumptions for follow-up questions if they&rsquo;re wrong.</p>
<p>It is built entirely out of serverless components, which means there is no cost to maintain or run it other than the traffic the system receives.</p>
<h2 id="how-to">How-to</h2>
<p>On a desktop or tablet, click the link in the header to navigate to the Answering Machine. For now, it isn&rsquo;t optimized for smartphone-sized screens.</p>
<p>In order to use the Answering Machine, you can either select one of the existing datasets, such as &ldquo;HR Activity Sample,&rdquo; or upload one of your own using the homepage of the site:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine homepage"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_uploads.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>To upload your own, click in the upload area or just drag a file straight from your desktop. For now, use CSV data files. Excel and other spreadsheet programs can easily save data in the CSV format using the &ldquo;File &gt; Save As&rdquo; or similar option in the menu. Each file needs a unique name.</p>
<p>When you hit the upload button, the site may not appear to change until the file is uploaded, at which point you&rsquo;ll see it appear in the box labeled &ldquo;Ask Your Data Anything&rdquo; below. Click on your file to start using it with the Answering Machine, or click the red trash can icon to delete it.</p>
<p>There are no user accounts in this system yet, so the data you upload might be seen by other users using the system. Try not to use sensitive data for now.</p>
<h3 id="asking-questions">Asking questions</h3>
<p>When you enter a dataset, you&rsquo;ll see a view that presents you with quite a bit of information:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    







    





    



    























<img  alt="Answering Machine main view"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_hr.png"   style="height: auto; width: 100%"   >


    
</div>

<p>The only part you need to focus on right now is the information panel. This panel lists out all the fields (columns), data types of those fields, and some sample data from a few records in your dataset:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine info panel"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_info.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>You can use this panel to start to formulate questions you might have about the data. If you see number values, you might ask about averages, maximums, or other math that might otherwise take some time to calculate. If you see a date, you can ask questions about the data in certain time periods.</p>
<p>Many datasets also contain fields that only have a few specific allowed values. When the Answering Machine sees fewer than 15 unique values in any field, the data type will be a &ldquo;List&rdquo; and it lists them right out under the sample values table. You can use this type of value to ask questions about records containing those specific values. For example, in the HR dataset, you might only be interested in data where the &ldquo;Education&rdquo; field&rsquo;s value is &ldquo;High School.&rdquo;</p>
<p>Now look to the query bar to start asking your data questions:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine query bar"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_query.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The types of questions that will currently be automatically detected and answered are:</p>
<ul>
<li>Counts of records where certain conditions are true</li>
<li>Math questions such as averages, medians, maximums, and minimums</li>
</ul>
<p>These types of questions can be made specific by using qualifying statements with prepositional phrases like &ldquo;in 2019&rdquo; or adjective phrases like &ldquo;male&rdquo; or &ldquo;entry-level.&rdquo;</p>
<p>Combining these two ideas, you can ask specific questions with any number of qualifiers, such as:<br>
<em>&ldquo;What was the median salary of male employees in the engineering department 5 years ago?&rdquo;</em></p>
<p>Upon hitting the &ldquo;Ask&rdquo; button (or hitting Enter), the Answering Machine will do its best to answer your question and will show you all of its work in this format:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine response"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_answer.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The last line in the response is the Answering Machine&rsquo;s answer. In this case, it is telling you the metric you asked for with all your stipulations is <strong>6871.6 dollars.</strong></p>
<p>Moving up, you see a series of assessments that the Answering Machine has made in order to filter and identify the data you are asking about. Statements like &ldquo;Best auto-detected Numeric Subject Found: salary with column: Compensation (Monthly)&rdquo; provide a glimpse into one of the Answering Machine&rsquo;s most advanced features, which uses a selection of NLP techniques to compare words and phrases that are similar in meaning, ultimately matching things you are asking about to fields and values that actually exist in your database.</p>
<p>At the very top of the response is how the Answering Machine&rsquo;s nested grammar parsing logic actually parsed your question, with some specific pieces color-coded:</p>
<ul>
<li><strong>Green</strong> chunks indicate &ldquo;subjects&rdquo; that were detected. Subjects are what the Answering Machine thinks you&rsquo;re asking &ldquo;about.&rdquo; These should represent both the main subject and other supporting subjects in your question.</li>
<li><strong>Purple</strong> chunks are conditions. These are the things that the Answering Machine thinks you are trying to use to &ldquo;specify&rdquo; or filter data.</li>
</ul>
<p>Now that your question is answered, you might notice that some new green and purple colored bubbles have appeared in the sections of your screen labeled &ldquo;New Subjects&rdquo; and &ldquo;New Conditions.&rdquo; We&rsquo;ll call these &ldquo;lexicon&rdquo;:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine subjects and conditions"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_lexicon.png"   style="height: auto; max-width: 400px; width: 100%"   >


    
</div>

<h3 id="forming-concepts">Forming Concepts</h3>
<p>If the Answering Machine already understood what you were asking and successfully matched it to fields and values in your data, you don&rsquo;t have to do anything with these. But often you will be using domain-specific lexicon, or the auto-matching algorithm simply won&rsquo;t pick the correct value. These situations are what concepts are for.</p>
<p>To create a concept, click and drag on the green or purple &ldquo;lexicon&rdquo; bubble and move it out into the blank middle area of the screen. Then click and drag the field or field value from the info-panel at the top of the screen and drop it right on top of that bubble. You&rsquo;ll see both the data bubble and the lexicon bubble included in a larger gray bubble, which represents the concept:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Answering Machine concept"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_concept.png"   style="height: auto; max-width: 200px; width: 100%"   >


    
</div>

<p>You can add more lexicon bubbles to this concept if they mean the same thing, but you can only use one data bubble.</p>
<p>Concepts override the Answering Machine&rsquo;s auto-matching logic. If you ask another question containing a subject or condition that is now matched by a user to a data value, that data value will be used instead of the auto-match. If the concept isn&rsquo;t working well, you can delete it by dragging all of the nested bubbles out of it either into the blank middle area or into the colored panel they originally came from.</p>
<p>Feel free to play around with new datasets and questions, and use the contact section of this site if you have comments or questions. When you ask questions or create/modify concepts, that data will automatically be saved to the server in real-time. You can close the page anytime and come back to your dataset to keep asking questions.</p>
<p>Remember that there are no user accounts, meaning you can share your dataset and work in tandem with others! But again, please do not upload sensitive data to this proof-of-concept tool as it will be available for other users to see and query.</p>
<h2 id="architecture">Architecture</h2>
<p>The Answering Machine is a purely &ldquo;serverless&rdquo; application, meaning that there is no server hosting the various components of the application until those components are needed by a user. This is true for the database, the file storage, the static website, the backend compute, and the API orchestration.</p>
<p>For the cloud nerds out there, <a href="https://martinfowler.com/articles/serverless.html">here is a great article</a> by Martin Fowler on what exactly &ldquo;serverless&rdquo; means, especially in terms of the backend compute portion, which is arguably the most valuable part of the application to be serverless. For reference, I am using Martin&rsquo;s 2nd definition of &ldquo;serverless&rdquo; here.</p>
<p>This is a high-level map of all of the components that make the Answering Machine work in its current (June 2020) state. The API gateways, CloudWatch events, and some triggers &ldquo;given for free&rdquo; by AWS are left out of this for readability:</p>
<div style="text-align: center; margin-bottom: 1rem;">
    





























    



    



    





    



    





    



    























<img  alt="Serverless Architecture of the Answering Machine app"   crossorigin="anonymous"    referrerpolicy="no-referrer"  src="https://hockenworks.com/images/answering_machine_architecture.png"   style="height: auto; max-width: 720px; width: 100%"   >


    
</div>

<p>The full suite of lambdas and persistent storage services that make up the Answering Machine.</p>
]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2019-07-03:/answering-machine/</guid>

                
                    <link>https://hockenworks.com/answering-machine/</link>
                

                
                    <pubDate>Wed, 03 Jul 2019 00:00:00 UTC</pubDate>
                

                
                    <title>The Answering Machine</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p><strong>Raspberry Pi Control Panel</strong> is a hardware project I designed in 2016 to manage home automation systems. The project involved designing a custom 3D-printed case for a Raspberry Pi microcomputer with a touchscreen interface.</p>
<p>Links:</p>
<ul>
<li><a href="https://github.com/hockenmaier/RaspberryPiControlPanel">GitHub</a></li>
<li><a href="https://www.thingiverse.com/thing:2524560">Thingiverse</a></li>
</ul>
<hr>
<p>I created this panel display in 2016 to control much of the home automation I used in my Studio City apartment. Mainly a hardware project, I designed and 3D-printed a case and frame for the touchscreen and raspberry pi microcomputer in order to mount them to the wall. The software running the control panel is SaaS, but I did write a custom html wrapper to control the orientation and settings of the site, which is available on the github linked above.</p>
<p>Update in 2025: This panel is still my main view into my home automation in my new house in Sherman Oaks, almost 10 years in with no modification to the software or Pi itself! However, the LED behind the screen died this year and I had to replace it (link below)</p>
<p>Here&rsquo;s a video to see the panel in action:</p>
<h2 id="hahahugoshortcode28s0hbhb">
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/iFGmm-ijJvE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>
</h2>
<p>Feel free to explore the linked repositories for schematics and source code.</p>
<h2 id="instructions">Instructions</h2>
<p>If you want to make this, all you need to do is set up a raspberry pi, download chromium (or your preferred web browser), and navigate to your action tiles panel.</p>
<p>If you want to mount the screen vertically like mine, then I have made an easier solution than going through the trouble of actually rotating the raspberry&rsquo;s display and touch device. Just use the html below and edit it to use your own panel&rsquo;s URL in the &ldquo;iframe&rdquo; element instead of mine. This will launch the panel rotated in your browser.</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-html" data-lang="html"><span style="display:flex;"><span><span style="color:#ff0007;background-color:#0f140f;font-weight:bold;font-style:italic">&lt;!DOCTYPE html&gt;</span>
</span></span><span style="display:flex;"><span>&lt;<span style="color:#fb660a;font-weight:bold">html</span>&gt;
</span></span><span style="display:flex;"><span>  &lt;<span style="color:#fb660a;font-weight:bold">head</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#fb660a;font-weight:bold">title</span>&gt;Rotated Raspberry Panel&lt;/<span style="color:#fb660a;font-weight:bold">title</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#fb660a;font-weight:bold">style</span> <span style="color:#ff0086;font-weight:bold">type</span>=<span style="color:#0086d2">&#34;text/css&#34;</span>&gt;
</span></span><span style="display:flex;"><span>      <span style="color:#fb660a;font-weight:bold">body</span> {
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a">-webkit-</span><span style="color:#fb660a;font-weight:bold">transform</span>: rotate(<span style="color:#0086f7;font-weight:bold">90</span><span style="color:#cdcaa9;font-weight:bold">deg</span>);
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a">-webkit-</span><span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#fb660a;font-weight:bold">bottom</span> <span style="color:#fb660a;font-weight:bold">left</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">position</span>: <span style="color:#fb660a;font-weight:bold">absolute</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">top</span>: <span style="color:#0086f7;font-weight:bold">-100</span><span style="color:#cdcaa9;font-weight:bold">vw</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">height</span>: <span style="color:#0086f7;font-weight:bold">100</span><span style="color:#cdcaa9;font-weight:bold">vw</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">width</span>: <span style="color:#0086f7;font-weight:bold">100</span><span style="color:#cdcaa9;font-weight:bold">vh</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">background-color</span>: <span style="color:#0086f7;font-weight:bold">#000</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">color</span>: <span style="color:#0086f7;font-weight:bold">#fff</span>;
</span></span><span style="display:flex;"><span>         <span style="color:#fb660a;font-weight:bold">overflow</span>: <span style="color:#fb660a;font-weight:bold">hidden</span>;&#34;
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>        <span style="color:#fb660a;font-weight:bold">iframe</span>{
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-ms-</span><span style="color:#fb660a;font-weight:bold">transform</span>: scale(<span style="color:#0086f7;font-weight:bold">0.97</span>);
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-moz-</span><span style="color:#fb660a;font-weight:bold">transform</span>: scale(<span style="color:#0086f7;font-weight:bold">0.97</span>);
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-o-</span><span style="color:#fb660a;font-weight:bold">transform</span>: scale(<span style="color:#0086f7;font-weight:bold">0.97</span>);
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-webkit-</span><span style="color:#fb660a;font-weight:bold">transform</span>: scale(<span style="color:#0086f7;font-weight:bold">0.97</span>);
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a;font-weight:bold">transform</span>: scale(<span style="color:#0086f7;font-weight:bold">0.97</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-ms-</span><span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-moz-</span><span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-o-</span><span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a">-webkit-</span><span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>      	<span style="color:#fb660a;font-weight:bold">transform-origin</span>: <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>;
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>    &lt;/<span style="color:#fb660a;font-weight:bold">style</span>&gt;
</span></span><span style="display:flex;"><span>  &lt;/<span style="color:#fb660a;font-weight:bold">head</span>&gt;
</span></span><span style="display:flex;"><span>  &lt;<span style="color:#fb660a;font-weight:bold">body</span>&gt;
</span></span><span style="display:flex;"><span>    &lt;<span style="color:#fb660a;font-weight:bold">iframe</span>
</span></span><span style="display:flex;"><span>      <span style="color:#ff0086;font-weight:bold">src</span>=<span style="color:#0086d2">&#34;https://app.actiontiles.com/panel/f7a7118c-236b-4144-b5b9-ccb35abeef21&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#ff0086;font-weight:bold">height</span>=<span style="color:#0086d2">&#34;300%&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#ff0086;font-weight:bold">width</span>=<span style="color:#0086d2">&#34;300%&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#ff0086;font-weight:bold">frameborder</span>=<span style="color:#0086d2">&#34;0&#34;</span>
</span></span><span style="display:flex;"><span>    &gt;&lt;/<span style="color:#fb660a;font-weight:bold">iframe</span>&gt;
</span></span><span style="display:flex;"><span>  &lt;/<span style="color:#fb660a;font-weight:bold">body</span>&gt;
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#fb660a;font-weight:bold">html</span>&gt;
</span></span></code></pre></div><p>Link to buy the screen:
Modern:
<a href="https://www.amazon.com/dp/B07P8P3X6M">https://www.amazon.com/dp/B07P8P3X6M</a>
Original:
<a href="https://smile.amazon.com/gp/product/B01ID5BQTC/">https://smile.amazon.com/gp/product/B01ID5BQTC/</a></p>
<p>Link to the Action Tiles web application this is running:
<a href="https://www.actiontiles.com/">https://www.actiontiles.com/</a></p>
<p>If you have issues getting your pi to use the full touchscreen width, try adding these setting to the /boot/config.txt file and reboot:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#fb660a">max_usb_current</span>=<span style="color:#0086f7;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span><span style="color:#fb660a">hdmi_group</span>=<span style="color:#0086f7;font-weight:bold">2</span>
</span></span><span style="display:flex;"><span><span style="color:#fb660a">hdmi_mode</span>=<span style="color:#0086f7;font-weight:bold">1</span>
</span></span><span style="display:flex;"><span><span style="color:#fb660a">hdmi_mode</span>=<span style="color:#0086f7;font-weight:bold">87</span>
</span></span><span style="display:flex;"><span>hdmi_cvt <span style="color:#0086f7;font-weight:bold">800</span> <span style="color:#0086f7;font-weight:bold">480</span> <span style="color:#0086f7;font-weight:bold">60</span> <span style="color:#0086f7;font-weight:bold">6</span> <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span> <span style="color:#0086f7;font-weight:bold">0</span>
</span></span></code></pre></div><p>If you want to make sure your screen doesn&rsquo;t go to sleep:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo nano /etc/lightdm/lightdm.conf
</span></span></code></pre></div><p>Add the following lines to the [SeatDefaults] section:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#111;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>xserver-command=X -s <span style="color:#0086f7;font-weight:bold">0</span> dpms
</span></span></code></pre></div>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2016-01-01:/raspberry-pi-panel/</guid>

                
                    <link>https://hockenworks.com/raspberry-pi-panel/</link>
                

                
                    <pubDate>Fri, 01 Jan 2016 00:00:00 UTC</pubDate>
                

                
                    <title>Raspberry Pi Control Panel</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>This switch closes a circuit when a strong magnetic field is nearby. The magnet used in the video is a rare earth magnet which is stronger than your typical refrigerator magnet.</p>
<p>Thingiverse Download: <a href="https://www.thingiverse.com/thing:190218">https://www.thingiverse.com/thing:190218</a></p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/b4piw_LMiRg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>

<h2 id="instructions">Instructions</h2>
<p>You will need two 4mm-wide wall hooks (the kind used to mount pictures). Make sure they conduct and are attracted to magnets. You will also need a hot glue gun.
Print two of the attached STL files (one of the mountable variety if you plan to mount it). Flatten the wall hooks with a hammer to get them completely flat, and lay the first one on one of your printed pieces leaving about a half an inch to spare from one end, and hanging out the other. Place a dab of hot glue on the end where it is hanging off. Do the same with the other half, and then place the halves together so the metal pieces are not quite touching in the middle. Seal the metal in place with hot glue.</p>
<p>The hot glue lets the metal move slightly, so that one metal piece bends to touch the other when a magnet is near either side of the switch, completing the circuit between the two metal ends still sticking out.</p>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2013-11-25:/3d-reed-switch/</guid>

                
                    <link>https://hockenworks.com/3d-reed-switch/</link>
                

                
                    <pubDate>Mon, 25 Nov 2013 00:00:00 UTC</pubDate>
                

                
                    <title>3D Printed Reed Switch</title>
                
            </item>
        
            <item>
                
                
                
                
                
                
                

                

                

                

                

                
                

                

                

                
                    <description><![CDATA[<p>I 3D Modeled and printed my apartment building&rsquo;s key with the Makergear M2. I won&rsquo;t be posting the model because it IS in fact a key to my apartment. Thanks for watching!</p>

    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/_H2W8qXUJtg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
    </div>]]></description>
                

                <guid isPermaLink="false">tag:hockenworks.com,2013-05-13:/3d-key/</guid>

                
                    <link>https://hockenworks.com/3d-key/</link>
                

                
                    <pubDate>Mon, 13 May 2013 00:00:00 UTC</pubDate>
                

                
                    <title>3D Printed Key</title>
                
            </item>
        
    </channel>
</rss>
